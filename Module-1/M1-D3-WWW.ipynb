{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a61f0b2-f334-4005-a88e-866f46521cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KGzB/CAS-Applied-Data-Science/blob/master/Module-1/M1-D3-WWW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee45e9ca-30eb-4a1e-8fbc-c4269a7d125d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dNrx7rgk7rM1"
   },
   "source": [
    "Notebook 1, Module 1, Data and Data Management, CAS Applied Data Science, 2024-08-23, A. M√ºhlemann, University of Bern. (based on the template by S. Haug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36290e3a-c305-43ed-a787-bf0aff3e18ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "X5Hcfb-p7rM5"
   },
   "source": [
    "# 3. Data acquisition on the world wide web\n",
    "\n",
    "**Learning outcomes:**\n",
    "\n",
    "Participants will be able to collect data from www sources. Examples are provided and exercised. We have about 1.5h hours for this tutorial.\n",
    "\n",
    "**Table of Contents**\n",
    "- 3.1 Read json from the web\n",
    "- 3.2 Retrieve and display pictures and files from the web\n",
    "- 3.3 Scraping webpages (html scraping)\n",
    "- 3.4 Cron jobs and Scheduled tasks\n",
    "- Some notes and links concerning social media\n",
    "\n",
    "**Further sources**\n",
    "- Examples all over internet\n",
    "- A book: https://www.packtpub.com/big-data-and-business-intelligence/mastering-social-media-mining-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb81508b-7b0c-4c84-8ce3-cb7ad3f25558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PhdDr0Mw7rM5"
   },
   "source": [
    "## 3.1 Analyse Aare with data from https://aareguru.existenz.ch/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd25481a-7c2d-4e27-a517-de55d15bd140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gUIRlVJy7rM6"
   },
   "source": [
    "Get the data from website, bring it into a format which can be imported into a dataframe, plot the time series and the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1a6ec1-488a-471a-9967-adb3671f120c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ww9S_bsn1JZ1",
    "outputId": "a11eaf56-02ee-4337-bdc0-4425cfac0694"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# URL of the Aare Guru API only getting infos from bern\n",
    "url = 'https://aareguru.existenz.ch/v2018/current?city=bern'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(url)\n",
    "data = response.json()  # Parse the JSON response\n",
    "df = pd.DataFrame(data['aarepast']) # Get the past flow and water temperature and save it in a data frame\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s') # change datetime\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818e2d63-e068-4011-aa07-30be3fe587c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zDbfzOLz5hDE"
   },
   "source": [
    "Get a rough overview of the data set we obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9a06f8-bb3f-4415-afe9-45fecfbd539c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "UsUo15Mr7rM7",
    "outputId": "f2257edc-a5ca-4a40-9b15-5e469cc6cc43"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510e70f9-03ab-4bd1-a035-08898c90b3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "W3geyWxR5uOK"
   },
   "source": [
    "Plot water temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77800416-5279-4a00-b805-545e825f6ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "VMzC8pdt7rM8",
    "outputId": "144a2687-6a75-4864-e40f-a33655d2564c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.plot(x='timestamp', y='temperature', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6f945f-fc8b-4f1d-9959-de9a11257926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jzsbBrbB6WtI"
   },
   "source": [
    "Let us now compare the aare temperature in Bern with the air temperature and percipitation. You can find the documentation of this API here: https://open-meteo.com/en/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1863563b-a7d9-4e12-85d3-5467267a4310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "wwqMjqrI7rM9",
    "outputId": "603f6681-ccf6-4a44-942c-9dc51632a75d"
   },
   "outputs": [],
   "source": [
    "# URL of weather api\n",
    "url = 'https://api.open-meteo.com/v1/forecast?latitude=52.52&longitude=13.41&past_days=10&hourly=temperature_2m,precipitation'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(url)\n",
    "data = response.json()  # Parse the JSON response\n",
    "df2 = pd.DataFrame(data['hourly']) # Get the past flow and water temperature and save it in a data frame\n",
    "df2['time'] = pd.to_datetime(df2['time']) # change datetime\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82310d3e-aa87-48fc-961c-5ee3e4fab325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_b5EyqmP_f4g"
   },
   "source": [
    "Let's also plot the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d45a85-ba42-426c-bb37-e66c2fd98cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "Vc3UnZgU_oXE",
    "outputId": "0258226f-652b-4f68-d39f-ea0e3491c624"
   },
   "outputs": [],
   "source": [
    "df2.plot(x='time', y='temperature_2m', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a54b80-de7c-4fc0-b741-d63bcb80536e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QQ7gBlV9_v7p"
   },
   "source": [
    "We can see that the two timeseries have a different scope. Let us thus only take the timepoints that occur in both time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f891f4-a8f9-4b95-aefd-7ead9341633a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "54zv0fCK_voS",
    "outputId": "7725d329-f5ee-491a-9da9-a715e172d2a8"
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'time': 'timestamp'}, inplace=True)\n",
    "df_new =pd.merge(df, df2, on='timestamp', how='left') # we use a left join to have the fine timegrid from the aare data\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a73dc5-02c2-495c-9b15-de6f92e7fa4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GMhZKY_gBTsf"
   },
   "source": [
    "Now let us plot the air and water temperature to see whether there is a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef89de0-76cf-4e3d-b0fb-0e6a08024efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "_PUnTb5CBylV",
    "outputId": "89355ac4-020f-496f-c663-612fc6a60cfe"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Optional: Adjust the figure size\n",
    "plt.plot(df_new['timestamp'], df_new['temperature'], label=\"water temperature\")\n",
    "plt.plot(df_new['timestamp'], df_new['temperature_2m'], label=\"air temperature\", marker='o' )\n",
    "# Customize the plot\n",
    "plt.xlabel('datetime')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Aare and Air Temperature in Bern')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74e17ebb-4188-44e8-badf-0007896570ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZH50SfUx7rM9"
   },
   "source": [
    "**Possible further exercise or project for Module 1 and 2**\n",
    "\n",
    "Find some colleague who can get the historical data (knows how to use the API) out of https://aareguru.existenz.ch/. Bring all data into one data frame. Look for correlations, averages (per month, per year ...). Combine the data with weather data, e.g. the wind on the Thun lake. For the Model 2 project, try to make a linear regression model predicting the Aare temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c93691-f0b0-46a0-a900-b98a63733d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "IGj-HaUB7rM9"
   },
   "source": [
    "## 3.2 Get pictures (or files) from webpages\n",
    "\n",
    "Get 3 pictures from a webserver with the Image module and show it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c05dd35-0bf8-43a6-9d58-05520aba4ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yF9T9Bcv7rM-",
    "outputId": "65d9ed07-486a-497e-84d5-912260eabdf3"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# URL of weather api\n",
    "url = ' https://dog.ceo/api/breed/hound/images'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "dog_images = pd.DataFrame(data)\n",
    "for i in range(1,4):\n",
    "    url = dog_images.iat[i,0]\n",
    "    img = Image(url)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c722c4-1097-4ac2-a3bb-cf40e6d4bb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wYpmokNe7rM-"
   },
   "source": [
    "### Exercise\n",
    "There is an API giving some cat pictures (https://api.thecatapi.com/v1/images/search?limit=10). Can you adapt the above code to obtain pictures of cats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf40463-1493-42e2-b41c-633b93e2f09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# URL of weather api\n",
    "url = 'https://api.thecatapi.com/v1/images/search?limit=10'\n",
    "\n",
    "# Send GET request to the API\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "cat_images = pd.DataFrame([item['url'] for item in data])\n",
    "for i in range(1,4):\n",
    "    url = cat_images.iat[i,0]\n",
    "    img = Image(url)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d2ab14-bca8-4b05-80da-51a812eb392b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ipSDJg4M7rM_"
   },
   "source": [
    "## 3.3 Scrape Webpages (html scraping)\n",
    "\n",
    "There are several billion online websites. With python you can easily read and parse this data if you have the links. Since pages are linked, one can in principle unnest probably all internet for webpages.\n",
    "\n",
    "In Python there is a library https://www.crummy.com/software/BeautifulSoup/bs4/doc/ for pulling data out of html and xml pages. We don't practise that library here, however, if you at some point deal with a lot of html, you may want to use it.\n",
    "\n",
    "Here we get the email adresses from a contact page of a website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f5305c-f6e7-4df0-9041-5e33777e4572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "jSxqLWuq7rM_",
    "outputId": "9c8484ae-1567-4a6f-c91e-607d2e665f31"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "startlink = \"https://www.zeilenwerk.ch/agentur\"\n",
    "f = urlopen(startlink)\n",
    "myfile = f.read()\n",
    "str(myfile)\n",
    "lines = str(myfile).split(' ')\n",
    "addresses = []\n",
    "for line in lines:\n",
    "    if 'mailto' in line:\n",
    "        tmp = np.array(line.split('\"'))\n",
    "        if len(tmp)== 3:\n",
    "          tmp2 = np.array(tmp[1].split(':'))\n",
    "          addresses.append(tmp2[1])\n",
    "df_addrs = pd.DataFrame(addresses,columns=['Adresses'])\n",
    "df_addrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d387f529-62bc-4d7b-8d05-d4d5cef47b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Prv2-jof7rM_"
   },
   "source": [
    "The above code is not optimal as you have probably seen. Lets use regular expressions instead (from StackOverflow). Regular expressions are a bit geeky, but very powerful and great fun. If you don't wan't to learn them, you mostly find the expression you want by googling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8537cccf-6478-4153-8518-fc7a3de4c95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EhEV4lY77rM_",
    "outputId": "e3537b63-9ac4-45a1-e018-03e9bc7f6833"
   },
   "outputs": [],
   "source": [
    "import re # the regular expression module\n",
    "startlink = \"https://www.zeilenwerk.ch/agentur\"\n",
    "f = urlopen(startlink)\n",
    "html = f.read()\n",
    "# Extract email addresses\n",
    "reobj = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b\", re.IGNORECASE)\n",
    "print(re.findall(reobj, html.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479b4cb1-ee13-4394-9d6d-4b222556e67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7A3APECh7rNB"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Hier a nice little challenge for you. Use the code above (together with a for loop or two) to scrape all webpages of your employer company for public available email addresses and put them into a dataframe :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4069e73d-9002-47ad-b790-8eb03ca762dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INDEX_URL = \"https://www.swissinfo.ch/eng/sitemap-2023.xml\"\n",
    "\n",
    "def read_sitemap(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Liest eine (Teil-)Sitemap mit <url><loc>‚Ä¶</loc><lastmod>‚Ä¶</lastmod> ein.\"\"\"\n",
    "    df = pd.read_xml(url)\n",
    "    # Manche Sitemaps liefern direkt <urlset>, manche sind wieder Indizes.\n",
    "    # Wir normalisieren auf Spaltennamen 'loc' und 'lastmod', wenn vorhanden.\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    # Falls es ein weiterer Index ist (nur 'loc' mit weiteren ?mm&dd-Links), geben wir ihn einfach zur√ºck.\n",
    "    return df\n",
    "\n",
    "# 1) Index einlesen -> Liste der verlinkten (Tages-)Sitemaps\n",
    "index_df = pd.read_xml(INDEX_URL)\n",
    "if \"loc\" not in index_df.columns:\n",
    "    raise ValueError(\"Im Index wurden keine 'loc'-Eintr√§ge gefunden.\")\n",
    "\n",
    "sitemap_links = index_df[\"loc\"].dropna().unique().tolist()\n",
    "\n",
    "# 2) Alle verlinkten Sitemaps einlesen und zusammenf√ºhren\n",
    "all_urls = []\n",
    "for sm in sitemap_links:\n",
    "    try:\n",
    "        part = pd.read_xml(sm)\n",
    "        # Nur Zeilen behalten, die tats√§chliche Seiten-URLs enthalten (also <urlset> mit <loc>)\n",
    "        if \"loc\" in part.columns:\n",
    "            # Einige Sitemaps enthalten mehr Felder (z.B. lastmod). Wir behalten das Wesentliche.\n",
    "            keep = [c for c in part.columns if c in {\"loc\", \"lastmod\", \"changefreq\", \"priority\"}]\n",
    "            if not keep:\n",
    "                keep = [\"loc\"]\n",
    "            all_urls.append(part[keep])\n",
    "    except Exception as e:\n",
    "        print(f\"√úbersprungen wegen Fehler bei {sm}: {e}\")\n",
    "\n",
    "urls_df = pd.concat(all_urls, ignore_index=True).drop_duplicates(subset=[\"loc\"])\n",
    "# Optional: Nur die eigene Domain behalten (hier ohnehin schon der Fall)\n",
    "# urls_df = urls_df[urls_df[\"loc\"].str.startswith(\"https://www.swissinfo.ch/\")]\n",
    "\n",
    "print(urls_df.shape)\n",
    "print(urls_df.head())\n",
    "\n",
    "# Optional speichern\n",
    "# urls_df.to_csv(\"swissinfo_sitemap_2023_urls.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908483d3-dbac-469d-b13d-a86273f1b57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, pandas as pd\n",
    "from lxml import etree\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "INDEX_URL = \"https://www.watson.ch/sitemap.xml\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; sitemap-loader/1.0)\"}\n",
    "TIMEOUT = 15\n",
    "MAX_SITEMAPS = 10   # üëâ hier steuern\n",
    "MAX_WORKERS = 8     # parallel\n",
    "\n",
    "def fetch(url):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def parse_urlset(xml_bytes):\n",
    "    # Extrahiert <loc> und optional <lastmod> aus einem <urlset>\n",
    "    root = etree.fromstring(xml_bytes)\n",
    "    ns = root.nsmap.get(None, \"\")  # Standard-NS\n",
    "    nsmap = {\"ns\": ns} if ns else {}\n",
    "    locs = root.xpath(\"//ns:url/ns:loc/text()\" if ns else \"//url/loc/text()\", namespaces=nsmap)\n",
    "    lastmods = root.xpath(\"//ns:url/ns:lastmod/text()\" if ns else \"//url/lastmod/text()\", namespaces=nsmap)\n",
    "    if lastmods and len(locs) == len(lastmods):\n",
    "        return pd.DataFrame({\"loc\": locs, \"lastmod\": lastmods})\n",
    "    return pd.DataFrame({\"loc\": locs})\n",
    "\n",
    "# 1) Index holen & auf erste N beschr√§nken\n",
    "index_xml = fetch(INDEX_URL)\n",
    "root = etree.fromstring(index_xml)\n",
    "ns = root.nsmap.get(None, \"\")\n",
    "nsmap = {\"ns\": ns} if ns else {}\n",
    "sitemap_links = root.xpath(\"//ns:sitemap/ns:loc/text()\" if ns else \"//sitemap/loc/text()\", namespaces=nsmap)\n",
    "sitemap_links = sitemap_links[:MAX_SITEMAPS]\n",
    "\n",
    "# 2) Parallel alle Sitemaps laden & parsen\n",
    "parts = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futs = {ex.submit(fetch, sm): sm for sm in sitemap_links}\n",
    "    for fut in as_completed(futs):\n",
    "        sm = futs[fut]\n",
    "        try:\n",
    "            xml_bytes = fut.result()\n",
    "            df = parse_urlset(xml_bytes)\n",
    "            if not df.empty:\n",
    "                parts.append(df[[\"loc\"] + ([c for c in df.columns if c == \"lastmod\"])])\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei {sm}: {e}\")\n",
    "\n",
    "urls_df = pd.concat(parts, ignore_index=True).drop_duplicates(\"loc\")\n",
    "print(urls_df.shape)\n",
    "print(urls_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672a4218-5358-4b98-b9d4-09074a0cd458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(urls_df[\"loc\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09ed51a-ccc5-4cd2-bef6-ee3c17044266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Sjr1SvUp7rNB"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "from tqdm import tqdm   # ‚úÖ Fortschrittsbalken\n",
    "\n",
    "# Regex f√ºr E-Mails\n",
    "reobj = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,6}\\b\", re.IGNORECASE)\n",
    "\n",
    "all_emails = []\n",
    "\n",
    "# Schleife √ºber die ersten 30 URLs mit Fortschrittsanzeige\n",
    "for link in tqdm(urls_df[\"loc\"].head(2), desc=\"Scraping\", unit=\"url\"):\n",
    "    try:\n",
    "        f = urlopen(link)\n",
    "        html = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        emails = re.findall(reobj, html)\n",
    "        if emails:\n",
    "            for e in emails:\n",
    "                all_emails.append({\"url\": link, \"email\": e})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei {link}: {e}\")\n",
    "\n",
    "# Ergebnis in DataFrame\n",
    "emails_df = pd.DataFrame(all_emails)\n",
    "\n",
    "print(pd.unique(emails_df['email']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2847bf6-09eb-450b-8a59-899903a1efbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UdoXpSlK7rNA"
   },
   "source": [
    "### Tables from webpages\n",
    "\n",
    "If you or someone else pubslishes data in html tables, it can be collected with pandas quite easily, actually directly without using the urllib module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0190c1c0-b7a1-4e25-b504-9226e4bce958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pLIK_9-l7rNA",
    "outputId": "ddc7e161-10f9-4be2-b14e-0e37dbed17a3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\n",
    "tables = pd.read_html(link)\n",
    "df = tables[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223e1cf9-19d2-4f30-bff5-bbb914e32588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GeLE8zYJ7rNA"
   },
   "source": [
    "Which countries have the largest population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c73e4d-88c9-47e8-ad22-5ebef0e19e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgVy8-jh7rNA",
    "outputId": "75479fd8-687d-4d88-ed64-8fb9693e140a"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1267a2-3d56-4591-8ff4-c7bf5941d21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8poi5RRH7rNA",
    "outputId": "a34dd0d1-df2f-4e9f-cc0f-9d6f80723017"
   },
   "outputs": [],
   "source": [
    "s_df = df.iloc[1:,1:3]\n",
    "s_df.sort_values('Population', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc8c96f-a82a-4bc4-be3d-922cfe94595c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "09v1o8gCQJfL"
   },
   "source": [
    "### Exercise\n",
    "Find another interesting table to scrape from Wikipedia and look at it more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3fbb91-a189-4f23-b2c0-aee3ac57586d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "pT1yim6uQS__"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link = \"https://de.wikipedia.org/wiki/Liste_der_meistverkauften_Rapalben_in_Deutschland\"\n",
    "tables = pd.read_html(link)\n",
    "df = tables[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05343e0b-e5b5-4bb0-b21b-287f6474a038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YkM2urT87rNB"
   },
   "source": [
    "## Some notes and links concerning social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7e5be4-5846-4173-bdef-9e7bd562017a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dLJew5sZ7rNB"
   },
   "source": [
    "### 1. Google search\n",
    "\n",
    "There is are APIs for doing google searches from Python. Hier is one explained.\n",
    "\n",
    "https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1656545-d0e9-47f0-bf7f-f4f3f796fa95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lk8Uf-137rNC"
   },
   "source": [
    "### 2. Twitter\n",
    "\n",
    "Twitter generates about 500M tweets per day. Thus, data mining on twitter can be interesting.\n",
    "\n",
    "Note: there are rate limits in the use of the Twitter API, as well as limitations in case you want to provide a downloadable data-set, see:\n",
    "\n",
    "https://dev.twitter.com/overview/terms/agreement-and-policy\n",
    "\n",
    "https://dev.twitter.com/rest/public/rate-limiting\n",
    "\n",
    "Tweepy is one python module with clients for thwe Twitter API.\n",
    "\n",
    "- https://www.tweepy.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dafab6d-01f2-4020-b6b4-2e89c64bd841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NHHCIXY17rNL"
   },
   "source": [
    "### 3. Instagram\n",
    "\n",
    "Largest photo sharing social media platform with 500 million monthly active users, and 95 million pictures and videos uploaded on Instagram daily in 2018 (?).\n",
    "\n",
    "https://stackoverflow.com/questions/61010431/how-to-start-with-the-instagramapi-in-python"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "M1-D3-WWW",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
