{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018fe2be-cf97-4674-abe6-e007ac8cfe2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Context and Starting Point\n",
    "\n",
    "The purpose of this work is to build a scalable pipeline that simplifies multilingual publishing across SRG SSR entities.\n",
    "\n",
    "Within SRG SSR, several initiatives already address related use cases. This work builds on these existing foundations.\n",
    "\n",
    "One key initiative is the **Publication Data Platform (PDP)**.  \n",
    "The PDP aggregates all published articles from SRG SSR and its business units:\n",
    "\n",
    "- Swissinfo  \n",
    "- SRF  \n",
    "- RTS  \n",
    "- RSI  \n",
    "- RTR  \n",
    "\n",
    "All published content is consolidated into a centralized Kafka feed.\n",
    "\n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "---\n",
    "\n",
    "# Infrastructure\n",
    "\n",
    "A Databricks infrastructure is in place and operated within SRG SSR.\n",
    "\n",
    "Workspace:  \n",
    "https://adb-4119964566130471.11.azuredatabricks.net/\n",
    "\n",
    "The Kafka feed is ingested into Databricks and available as the Delta table:\n",
    "\n",
    "`udp_prd_modeled.pdp.articles_v2`\n",
    "\n",
    "This table represents the **Bronze layer**, meaning the raw modeled Kafka data.\n",
    "\n",
    "However, the structure still reflects the original Kafka schema and is therefore:\n",
    "\n",
    "- Nested (JSON structure)  \n",
    "- Not normalized  \n",
    "- Not directly suited for analytics or downstream automation  \n",
    "\n",
    "The data represents raw publication events and requires structural transformation before it can be used in analytical or AI-driven workflows.\n",
    "\n",
    "Access to this infrastructure and its datasets is available only to entitled SRG staff.\n",
    "\n",
    "---\n",
    "\n",
    "# Purpose of this Notebook (Silver Layer)\n",
    "\n",
    "This notebook represents the **Silver layer** of the pipeline.\n",
    "\n",
    "Its goal is to:\n",
    "\n",
    "1. Read the raw PDP article data from `udp_prd_modeled.pdp.articles_v2`.  \n",
    "2. Explode nested JSON structures.  \n",
    "3. Separate complex hierarchical structures into structured tables.  \n",
    "4. Persist relationally accessible Delta tables for downstream processing.  \n",
    "\n",
    "At this stage:\n",
    "\n",
    "- No AI logic is implemented.  \n",
    "- No content generation is performed.  \n",
    "- Only structural normalization is applied.  \n",
    "\n",
    "The result is a clean relational foundation for further transformations in Gold or AI application layers.\n",
    "\n",
    "---\n",
    "\n",
    "# Processing Steps\n",
    "\n",
    "1. Read article data from `udp_prd_modeled.pdp.articles_v2`.  \n",
    "2. Flatten and explode nested fields (e.g., titles, resources, contributors).  \n",
    "3. Create structured Spark DataFrames.  \n",
    "4. Persist the transformed data as Delta tables.  \n",
    "5. Prepare relational access patterns for downstream usage.  \n",
    "\n",
    "---\n",
    "\n",
    "# Internal Infrastructure Notice\n",
    "\n",
    "This notebook is developed and executed within the secured SRG SSR Databricks environment.\n",
    "\n",
    "- The dataset may contain confidential publication data.  \n",
    "- Access is restricted to entitled SRG staff.  \n",
    "- Confidential datasets cannot be exported.  \n",
    "- Code execution is limited to the Databricks (Spark) environment.  \n",
    "- The infrastructure is not accessible to external users.  \n",
    "\n",
    "This ensures compliance with SRG governance, data protection, and infrastructure policies.\n",
    "\n",
    "---\n",
    "\n",
    "# Note on AI Assistance\n",
    "\n",
    "Parts of the transformation logic in this notebook were developed with AI assistance to accelerate structural exploration and flattening of complex nested JSON schemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9718adba-4f18-4657-b4ae-7efe497d71b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”§ 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d490ac65-7334-478d-a937-05ac1ad978e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks | PySpark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "CATALOG = \"swi_audience_prd\"\n",
    "SCHEMA  = \"pdp_articles_v2_silver\"\n",
    "SRC     = \"udp_prd_atomic.pdp.articles_v2\"   # Bronze/Atomic-Quelle\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA}\")\n",
    "\n",
    "df = spark.table(SRC)\n",
    "\n",
    "def save(df, table_name, mode=\"overwrite\"):\n",
    "    full = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "    (df.write\n",
    "       .option(\"overwriteSchema\", \"true\")\n",
    "       .mode(mode)\n",
    "       .saveAsTable(full))\n",
    "    print(f\"âœ… wrote {full} ({df.count()} rows)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c2ce742-24a8-4b05-943c-6309a37ce024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ“˜ 2) Core-Tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dea1899-ba8f-4ffa-9400-46e15a658ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "core = (\n",
    "    df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.col(\"value.publisher\").alias(\"publisher\"),\n",
    "        F.col(\"value.provenance\").alias(\"provenance\"),\n",
    "        F.col(\"value.releaseDate\").alias(\"releaseDate\"),\n",
    "        F.col(\"value.modificationDate\").alias(\"modificationDate\"),\n",
    "        F.col(\"topic\").alias(\"topic\"),  # Top-Level\n",
    "        # âš ï¸ Ohne .items, da Struct:\n",
    "        F.col(\"value.kicker.content\").alias(\"kicker_default\"),\n",
    "        F.col(\"value.lead.content\").alias(\"lead_default\"),\n",
    "        F.col(\"partition\").alias(\"source_partition\"),\n",
    "        F.col(\"offset\").alias(\"source_offset\"),\n",
    "        F.col(\"timestamp\").alias(\"source_timestamp\"),\n",
    "        F.col(\"atomic_ts\").alias(\"atomic_ts\")\n",
    "    )\n",
    "    .dropDuplicates([\"article_id\"])\n",
    ")\n",
    "save(core, \"core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b086e68b-1a13-4535-9f3e-bc941a148b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2) Helper: â€žitemsâ€œ-Erkennung + normalisierte Auswahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26d0bec-b402-49bb-ab36-b49f3c21071c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType\n",
    "\n",
    "def has_items(value_schema: StructType, field_name: str) -> bool:\n",
    "    f = next((x for x in value_schema.fields if x.name == field_name), None)\n",
    "    if not f or not isinstance(f.dataType, StructType):\n",
    "        return False\n",
    "    return any(ch.name == \"items\" and isinstance(ch.dataType, ArrayType) for ch in f.dataType.fields)\n",
    "\n",
    "value_schema = next(f for f in df.schema.fields if f.name == \"value\").dataType\n",
    "TITLE_HAS_ITEMS  = has_items(value_schema, \"title\")\n",
    "LEAD_HAS_ITEMS   = has_items(value_schema, \"lead\")\n",
    "KICKER_HAS_ITEMS = has_items(value_schema, \"kicker\")\n",
    "\n",
    "def write_lang_items(field_name: str, out_table: str, HAS_ITEMS: bool):\n",
    "    if HAS_ITEMS:\n",
    "        out = (\n",
    "            df.select(\n",
    "                F.col(\"value.id\").alias(\"article_id\"),\n",
    "                F.posexplode_outer(F.col(f\"value.{field_name}.items\")).alias(\"pos\",\"it\")\n",
    "            )\n",
    "            .select(\n",
    "                \"article_id\",\"pos\",\n",
    "                F.col(\"it.language\").alias(\"language\"),\n",
    "                F.col(\"it.content\").alias(\"content\"),\n",
    "                F.col(\"it.formatted.content\").alias(\"formatted_content\"),\n",
    "                F.col(\"it.formatted.contentType\").alias(\"contentType\")\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # Einzelner Struct -> als ein â€žItemâ€œ mit pos=0 ausgeben\n",
    "        out = (\n",
    "            df.where(F.col(f\"value.{field_name}\").isNotNull())\n",
    "              .select(\n",
    "                  F.col(\"value.id\").alias(\"article_id\"),\n",
    "                  F.lit(0).alias(\"pos\"),\n",
    "                  F.col(f\"value.{field_name}.language\").alias(\"language\"),\n",
    "                  F.col(f\"value.{field_name}.content\").alias(\"content\"),\n",
    "                  F.col(f\"value.{field_name}.formatted.content\").alias(\"formatted_content\"),\n",
    "                  F.col(f\"value.{field_name}.formatted.contentType\").alias(\"contentType\"),\n",
    "              )\n",
    "        )\n",
    "    save(out, out_table)\n",
    "\n",
    "write_lang_items(\"title\",  \"title_items\",  TITLE_HAS_ITEMS)\n",
    "write_lang_items(\"lead\",   \"lead_items\",   LEAD_HAS_ITEMS)\n",
    "write_lang_items(\"kicker\", \"kicker_items\", KICKER_HAS_ITEMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9622c92f-23ce-4a0c-b595-aeff5ad02fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ”§ Helper-Funktionen (robuste Array-Handhabung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaae989-ebc7-40ae-96d1-205e8cc432c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType, StringType\n",
    "\n",
    "value_schema = next(f for f in df.schema.fields if f.name == \"value\").dataType\n",
    "\n",
    "def is_array(path: str) -> bool:\n",
    "    \"\"\"Check if a given path (relative to 'value') is an array.\"\"\"\n",
    "    # simple path walker: value.a.b.c\n",
    "    cur = value_schema\n",
    "    for name in path.split(\".\"):\n",
    "        if isinstance(cur, StructType):\n",
    "            f = next((x for x in cur.fields if x.name == name), None)\n",
    "            if not f: \n",
    "                return False\n",
    "            cur = f.dataType\n",
    "        elif isinstance(cur, ArrayType):\n",
    "            cur = cur.elementType\n",
    "        else:\n",
    "            return False\n",
    "    return isinstance(cur, ArrayType)\n",
    "\n",
    "def has_items_struct(parent: str) -> bool:\n",
    "    \"\"\"Check if a struct has an 'items' Array child (e.g. title.items).\"\"\"\n",
    "    f = next((x for x in value_schema.fields if x.name == parent), None)\n",
    "    if not f or not isinstance(f.dataType, StructType):\n",
    "        return False\n",
    "    items = next((x for x in f.dataType.fields if x.name == \"items\"), None)\n",
    "    return items is not None and isinstance(items.dataType, ArrayType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbfd0e8-db3c-47f6-8c04-e7c9ae9e6653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ§© A) Einfache Arrays: keywords, relatedArticles, existsAs, relatedEditorialObjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbe367c-50ef-49c2-bff8-c712bc2457ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def array_simple_strings(field: str, out_table: str, out_col: str):\n",
    "    # FÃ¤lle:\n",
    "    # 1) value.field.items[] existiert (Schema-Doku-Variante)\n",
    "    # 2) value.field[] ist direkt ein Array\n",
    "    # 3) value.field ist einzelner String\n",
    "    if has_items_struct(field):\n",
    "        out = (\n",
    "            df.select(\n",
    "                F.col(\"value.id\").alias(\"article_id\"),\n",
    "                F.posexplode_outer(F.col(f\"value.{field}.items\")).alias(\"pos\", out_col)\n",
    "            )\n",
    "        )\n",
    "    elif is_array(field):\n",
    "        out = (\n",
    "            df.select(\n",
    "                F.col(\"value.id\").alias(\"article_id\"),\n",
    "                F.posexplode_outer(F.col(f\"value.{field}\")).alias(\"pos\", out_col)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # Einzelwert â†’ pos=0\n",
    "        out = (\n",
    "            df.where(F.col(f\"value.{field}\").isNotNull())\n",
    "              .select(\n",
    "                  F.col(\"value.id\").alias(\"article_id\"),\n",
    "                  F.lit(0).alias(\"pos\"),\n",
    "                  F.col(f\"value.{field}\").alias(out_col)\n",
    "              )\n",
    "        )\n",
    "    save(out, out_table)\n",
    "\n",
    "array_simple_strings(\"keywords\",                \"keywords\",                    \"keyword\")\n",
    "array_simple_strings(\"relatedArticles\",         \"related_articles\",            \"related_id\")\n",
    "array_simple_strings(\"existsAs\",                \"exists_as\",                   \"exists_as\")\n",
    "array_simple_strings(\"relatedEditorialObjects\", \"related_editorial_objects\",   \"editorial_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6b529b-87cf-4449-9da4-a457165d01a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ§© B) content.text (Array oder einzelner String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613719a6-c439-4f07-88ba-46b682917d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def content_text_table():\n",
    "    # Doku sagt: content.text.items; real kann auch string / array sein\n",
    "    base_path = \"content.text\"\n",
    "    if has_items_struct(\"content\"):\n",
    "        # Sonderfall: content hat items? (unwahrscheinlich hier) â€“ fallback:\n",
    "        out = (\n",
    "            df.select(\n",
    "                F.col(\"value.id\").alias(\"article_id\"),\n",
    "                F.posexplode_outer(F.col(\"value.content.text.items\")).alias(\"pos\",\"text\")\n",
    "            )\n",
    "        )\n",
    "    elif is_array(base_path):\n",
    "        out = (\n",
    "            df.select(\n",
    "                F.col(\"value.id\").alias(\"article_id\"),\n",
    "                F.posexplode_outer(F.col(\"value.content.text\")).alias(\"pos\",\"text\")\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        out = (\n",
    "            df.where(F.col(\"value.content.text\").isNotNull())\n",
    "              .select(\n",
    "                  F.col(\"value.id\").alias(\"article_id\"),\n",
    "                  F.lit(0).alias(\"pos\"),\n",
    "                  F.col(\"value.content.text\").alias(\"text\")\n",
    "              )\n",
    "        )\n",
    "    save(out, \"content_text\")\n",
    "\n",
    "content_text_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6870df91-9c7f-420b-87be-189c4f62d6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ§© C) identifiers (Array von Structs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967ce73d-7638-4026-aeed-ae1d61012003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IDENTIFIERS â€“ robust (items[] | direktes Array | einzelner Struct)\n",
    "if has_items_struct(\"identifiers\"):\n",
    "    identifiers = (\n",
    "        df.select(\n",
    "            F.col(\"value.id\").alias(\"article_id\"),\n",
    "            F.posexplode_outer(\"value.identifiers.items\").alias(\"pos\",\"i\")\n",
    "        ).select(\"article_id\",\"pos\",\n",
    "                 F.col(\"i.type\").alias(\"type\"),\n",
    "                 F.col(\"i.value\").alias(\"value\"))\n",
    "    )\n",
    "elif is_array(\"identifiers\"):\n",
    "    identifiers = (\n",
    "        df.select(\n",
    "            F.col(\"value.id\").alias(\"article_id\"),\n",
    "            F.posexplode_outer(\"value.identifiers\").alias(\"pos\",\"i\")\n",
    "        ).select(\"article_id\",\"pos\",\n",
    "                 F.col(\"i.type\").alias(\"type\"),\n",
    "                 F.col(\"i.value\").alias(\"value\"))\n",
    "    )\n",
    "else:\n",
    "    # einzelner Struct (selten, aber sicher)\n",
    "    identifiers = (\n",
    "        df.where(F.col(\"value.identifiers\").isNotNull())\n",
    "          .select(\n",
    "              F.col(\"value.id\").alias(\"article_id\"),\n",
    "              F.lit(0).alias(\"pos\"),\n",
    "              F.col(\"value.identifiers.type\").alias(\"type\"),\n",
    "              F.col(\"value.identifiers.value\").alias(\"value\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "save(identifiers, \"identifiers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1709c7e8-f843-4c9f-ac36-3c239ff8da4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "d) genres (Array von Structs â€“ ohne .items unter name/description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9fb17aa7-871c-4a21-92fd-2b5f279d4866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GENRES â€“ robust: genres.items  ODER  genres[]  ODER einzelner Struct\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if has_items_struct(\"genres\"):\n",
    "    genres_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.genres.items\").alias(\"pos\",\"g\")\n",
    "    )\n",
    "elif is_array(\"genres\"):\n",
    "    genres_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.genres\").alias(\"pos\",\"g\")\n",
    "    )\n",
    "else:\n",
    "    genres_src = (\n",
    "        df.where(F.col(\"value.genres\").isNotNull())\n",
    "          .select(\n",
    "              F.col(\"value.id\").alias(\"article_id\"),\n",
    "              F.lit(0).alias(\"pos\"),\n",
    "              F.col(\"value.genres\").alias(\"g\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "genres = genres_src.select(\n",
    "    \"article_id\",\"pos\",\n",
    "    F.col(\"g.name.content\").alias(\"name_content\"),\n",
    "    F.col(\"g.name.language\").alias(\"name_language\"),\n",
    "    F.col(\"g.name.formatted.content\").alias(\"name_formatted_content\"),\n",
    "    F.col(\"g.name.formatted.contentType\").alias(\"name_contentType\"),\n",
    "    F.col(\"g.description.content\").alias(\"description_content\"),\n",
    "    F.col(\"g.description.language\").alias(\"description_language\"),\n",
    "    F.col(\"g.description.formatted.content\").alias(\"description_formatted_content\"),\n",
    "    F.col(\"g.description.formatted.contentType\").alias(\"description_contentType\"),\n",
    "    F.col(\"g.sourceId\").alias(\"sourceId\")\n",
    ")\n",
    "save(genres, \"genres\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3adfc8de-ab63-4dc7-877d-72a375e36305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "e) contributors (Base + Occupations, robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "53115d74-2823-4ab7-b415-789085bc79d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CONTRIBUTORS BASE â€“ contributors.items ODER contributors[] ODER einzelner Struct\n",
    "if has_items_struct(\"contributors\"):\n",
    "    contrib_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.contributors.items\").alias(\"pos\",\"c\")\n",
    "    )\n",
    "elif is_array(\"contributors\"):\n",
    "    contrib_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.contributors\").alias(\"pos\",\"c\")\n",
    "    )\n",
    "else:\n",
    "    contrib_src = (\n",
    "        df.where(F.col(\"value.contributors\").isNotNull())\n",
    "          .select(\n",
    "              F.col(\"value.id\").alias(\"article_id\"),\n",
    "              F.lit(0).alias(\"pos\"),\n",
    "              F.col(\"value.contributors\").alias(\"c\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "contributors_base = contrib_src.select(\n",
    "    \"article_id\",\"pos\",\n",
    "    F.col(\"c.name\").alias(\"contrib_name\"),\n",
    "    F.col(\"c.agent.person.name\").alias(\"person_name\"),\n",
    "    F.col(\"c.agent.team.name\").alias(\"team_name\"),\n",
    "    F.col(\"c.agent.department.name\").alias(\"department_name\")\n",
    ")\n",
    "save(contributors_base, \"contributors_base\")\n",
    "\n",
    "# OCCUPATIONS â€“ c.agent.person.occupations.items ODER occupations[] ODER einzelner Struct\n",
    "# Wir hÃ¤ngen an contrib_src an und explodieren (falls vorhanden)\n",
    "from pyspark.sql.types import ArrayType, StructType\n",
    "\n",
    "def occ_is_array():\n",
    "    # prÃ¼ft: value.contributors[*].agent.person.occupations ist Array?\n",
    "    return (is_array(\"contributors.items.agent.person.occupations\") or\n",
    "            is_array(\"contributors.agent.person.occupations\"))\n",
    "\n",
    "def occ_has_items():\n",
    "    # prÃ¼ft: ...occupations.items existiert?\n",
    "    return (has_items_struct(\"contributors\") and\n",
    "            is_array(\"contributors.items.agent.person.occupations.items\"))\n",
    "\n",
    "if occ_has_items():\n",
    "    occ = (\n",
    "        contrib_src\n",
    "        .select(\"article_id\",\"pos\",\"c\",\n",
    "                F.posexplode_outer(\"c.agent.person.occupations.items\").alias(\"occ_pos\",\"occ\"))\n",
    "    )\n",
    "elif occ_is_array():\n",
    "    occ = (\n",
    "        contrib_src\n",
    "        .select(\"article_id\",\"pos\",\"c\",\n",
    "                F.posexplode_outer(\"c.agent.person.occupations\").alias(\"occ_pos\",\"occ\"))\n",
    "    )\n",
    "else:\n",
    "    # einzelner Struct â†’ pos=0, aber nur wenn vorhanden\n",
    "    occ = (\n",
    "        contrib_src\n",
    "        .where(F.col(\"c.agent.person.occupations\").isNotNull())\n",
    "        .select(\"article_id\",\"pos\",\n",
    "                F.lit(0).alias(\"occ_pos\"),\n",
    "                F.col(\"c.agent.person.occupations\").alias(\"occ\"))\n",
    "    )\n",
    "\n",
    "contributors_occupations = occ.select(\n",
    "    \"article_id\",\n",
    "    \"pos\",\n",
    "    F.col(\"occ_pos\").alias(\"occupation_pos\"),\n",
    "    # name als Struct mit optional formatted\n",
    "    F.col(\"occ.name.language\").alias(\"occupation_language\"),\n",
    "    F.col(\"occ.name.content\").alias(\"occupation_content\"),\n",
    "    F.col(\"occ.name.formatted.content\").alias(\"occupation_formatted_content\"),\n",
    "    F.col(\"occ.name.formatted.contentType\").alias(\"occupation_contentType\")\n",
    ")\n",
    "save(contributors_occupations, \"contributors_occupations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d489f3-ef3d-48c9-88ff-fc681dbdf493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "f) resources (Pictures / Documents / Links â€“ ohne .items in name/description/altText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8e42c8-ad80-4194-85cb-37c359dacd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- RESOURCES (robust, ohne identifiers im Documents-Table) ---\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Quelle fÃ¼r resources: items[] | [] | einzelner Struct\n",
    "if has_items_struct(\"resources\"):\n",
    "    res_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.resources.items\").alias(\"pos\",\"r\")\n",
    "    )\n",
    "elif is_array(\"resources\"):\n",
    "    res_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.resources\").alias(\"pos\",\"r\")\n",
    "    )\n",
    "else:\n",
    "    res_src = (\n",
    "        df.where(F.col(\"value.resources\").isNotNull())\n",
    "          .select(\n",
    "              F.col(\"value.id\").alias(\"article_id\"),\n",
    "              F.lit(0).alias(\"pos\"),\n",
    "              F.col(\"value.resources\").alias(\"r\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "# PICTURES\n",
    "pictures = (\n",
    "    res_src.where(F.col(\"r.picture\").isNotNull())\n",
    "    .select(\n",
    "        \"article_id\",\"pos\",\n",
    "        F.col(\"r.picture.name.language\").alias(\"name_language\"),\n",
    "        F.col(\"r.picture.name.content\").alias(\"name_content\"),\n",
    "        F.col(\"r.picture.description.language\").alias(\"description_language\"),\n",
    "        F.col(\"r.picture.description.content\").alias(\"description_content\"),\n",
    "        F.col(\"r.picture.locator.url\").alias(\"url\"),\n",
    "        F.col(\"r.picture.locator.urn\").alias(\"urn\"),\n",
    "        F.col(\"r.picture.width\").alias(\"width\"),\n",
    "        F.col(\"r.picture.height\").alias(\"height\"),\n",
    "        F.col(\"r.picture.copyright\").alias(\"copyright\"),\n",
    "        F.col(\"r.picture.altText.language\").alias(\"alt_language\"),\n",
    "        F.col(\"r.picture.altText.content\").alias(\"alt_content\"),\n",
    "        F.col(\"r.picture.usage.sourceValue\").alias(\"usage_sourceValue\")\n",
    "    )\n",
    ")\n",
    "save(pictures, \"resources_pictures\")\n",
    "\n",
    "# DOCUMENTS  (ohne identifiers; wird in separater Tabelle behandelt)\n",
    "documents = (\n",
    "    res_src.where(F.col(\"r.document\").isNotNull())\n",
    "    .select(\n",
    "        \"article_id\",\"pos\",\n",
    "        F.col(\"r.document.name.language\").alias(\"name_language\"),\n",
    "        F.col(\"r.document.name.content\").alias(\"name_content\"),\n",
    "        F.col(\"r.document.description.language\").alias(\"description_language\"),\n",
    "        F.col(\"r.document.description.content\").alias(\"description_content\"),\n",
    "        F.col(\"r.document.locator.url\").alias(\"url\"),\n",
    "        F.col(\"r.document.locator.urn\").alias(\"urn\"),\n",
    "        F.col(\"r.document.productionDate\").alias(\"productionDate\"),\n",
    "        F.col(\"r.document.modificationDate\").alias(\"modificationDate\")\n",
    "    )\n",
    ")\n",
    "save(documents, \"resources_documents\")\n",
    "\n",
    "# LINKS\n",
    "links = (\n",
    "    res_src.where(F.col(\"r.link\").isNotNull())\n",
    "    .select(\n",
    "        \"article_id\",\"pos\",\n",
    "        F.col(\"r.link.name\").alias(\"name\"),\n",
    "        F.col(\"r.link.locator.url\").alias(\"url\"),\n",
    "        F.col(\"r.link.locator.urn\").alias(\"urn\")\n",
    "    )\n",
    ")\n",
    "save(links, \"resources_links\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db7be2a6-d80b-4c64-8c0f-6bf1acf44ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "g) accessConditions (+ Geo-Whitelist â€“ ohne .description.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ca4fd8-2bf5-436f-bc67-2f78e2ecc897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ACCESS CONDITIONS â€“ accessConditions.items  ODER accessConditions[]  ODER einzelner Struct\n",
    "if has_items_struct(\"accessConditions\"):\n",
    "    ac_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.accessConditions.items\").alias(\"pos\",\"a\")\n",
    "    )\n",
    "elif is_array(\"accessConditions\"):\n",
    "    ac_src = df.select(\n",
    "        F.col(\"value.id\").alias(\"article_id\"),\n",
    "        F.posexplode_outer(\"value.accessConditions\").alias(\"pos\",\"a\")\n",
    "    )\n",
    "else:\n",
    "    ac_src = (\n",
    "        df.where(F.col(\"value.accessConditions\").isNotNull())\n",
    "          .select(\n",
    "              F.col(\"value.id\").alias(\"article_id\"),\n",
    "              F.lit(0).alias(\"pos\"),\n",
    "              F.col(\"value.accessConditions\").alias(\"a\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "# Geo-Whitelist: a.geoBlockingWhitelist.items  ODER  a.geoBlockingWhitelist[]  ODER einzelner Struct\n",
    "geo_items = (\n",
    "    ac_src\n",
    "    .select(\n",
    "        \"article_id\",\"pos\",\"a\",\n",
    "        F.posexplode_outer(\"a.geoBlockingWhitelist.items\").alias(\"geo_pos\",\"geo\")\n",
    "    )\n",
    "    if (is_array(\"accessConditions.items.geoBlockingWhitelist.items\")\n",
    "        or is_array(\"accessConditions.geoBlockingWhitelist.items\"))\n",
    "    else (\n",
    "        ac_src.select(\n",
    "            \"article_id\",\"pos\",\"a\",\n",
    "            F.posexplode_outer(\"a.geoBlockingWhitelist\").alias(\"geo_pos\",\"geo\")\n",
    "        ) if (\n",
    "            is_array(\"accessConditions.items.geoBlockingWhitelist\")\n",
    "            or is_array(\"accessConditions.geoBlockingWhitelist\")\n",
    "        )\n",
    "        else ac_src.where(F.col(\"a.geoBlockingWhitelist\").isNotNull())\n",
    "                   .select(\"article_id\",\"pos\",\"a\",\n",
    "                           F.lit(0).alias(\"geo_pos\"),\n",
    "                           F.col(\"a.geoBlockingWhitelist\").alias(\"geo\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "access_conditions = geo_items.select(\n",
    "    \"article_id\",\"pos\",\n",
    "    F.col(\"a.name\").alias(\"name\"),\n",
    "    F.col(\"a.description.content\").alias(\"description_content\"),\n",
    "    F.col(\"a.availableFrom\").alias(\"availableFrom\"),\n",
    "    F.col(\"a.availableTo\").alias(\"availableTo\"),\n",
    "    F.col(\"a.blockReason\").alias(\"blockReason\"),\n",
    "    F.col(\"a.embeddingDisabled\").alias(\"embeddingDisabled\"),\n",
    "    F.col(\"geo_pos\").alias(\"geoWhitelist_pos\"),\n",
    "    F.col(\"geo.sourceValue\").alias(\"geoWhitelist_value\")\n",
    ")\n",
    "save(access_conditions, \"access_conditions\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_silver_explode_source_articles",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
