{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bb6c226-2a55-45af-bb45-409d79b10a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "n_5oRe0SXilM"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neworldemancer/DSF5/blob/master/Course_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290e45b7-1823-4505-9f14-117ce7dfeb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to machine learning & Data Analysis\n",
    "\n",
    "Basic introduction on how to perform typical machine learning tasks with Python.\n",
    "\n",
    "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
    "Data Science Lab, University Of Bern, 2023\n",
    "\n",
    "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n",
    "\n",
    "# Part 1: Introduction and Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a12aba-1b18-420a-a092-4595ab3be152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9SxiIczg1s1k"
   },
   "source": [
    "# What is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ae37ae-232c-43b7-8c06-002b90163127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ll5e8N9SVwVa"
   },
   "source": [
    "## Why Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797c1d30-58d9-4a25-814d-bc9987e7b73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "q6tHZQCywhGB"
   },
   "source": [
    "\n",
    "\n",
    "1.   \n",
    "2.   \n",
    "3.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6889c7-dd9a-432c-8b9a-b33ea1599f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5PbjhPxLmsI4"
   },
   "source": [
    "## Learning from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03cc91f-b55c-44b3-b174-f873a338195c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Xsd1MyT9eIdW"
   },
   "source": [
    "Unlike classical algorithms, created by a human to analyze some data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1810c2ff-eb43-4eaf-9499-efa5f6b0bd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XtoqE5XO3L1j"
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_1.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6be273b-6937-4b16-90a9-7471d6a5070e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PkelBk_L6ENw"
   },
   "source": [
    "in machine learning the data itself is used for to define the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f6e0a9-58b7-4b97-815d-f7b6f00648ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "A6X7b0AG6ENw"
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_2.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c87d963e-cde3-43ff-82cf-34db2f31d0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vBrquDTR6ENw"
   },
   "source": [
    "\n",
    "The boundaries are sometimes a bit fuzzy between classical and machine learning algorithm.\n",
    "\n",
    "In fact when we create algorithms, the problem in hand (namely the data  related to the problem), drives us to choose one or another algorithm. And we then tune it, to perform well on a task in hand.\n",
    "\n",
    "There are three macro-areas of ML algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b4d033-28b4-44a5-8e9f-ea22af4b3050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iHEUHwrN6ENx"
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_4.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30f6019-040c-4ac3-a313-5d67cf61d2e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "to8vOJeC1xjE"
   },
   "source": [
    "In this course we will explore the foundations of the first two macro-areas, the ones most often used in applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6086a21d-9e26-4f31-a5bf-bd74eb254b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CbOZSi7UmkSq"
   },
   "source": [
    "Recent examples.\n",
    "\n",
    "1. Hugginface https://github.com/huggingface/transformers and ChatGPT https://chat.openai.com/ from openAI unleashed the power of transformer architectures https://arxiv.org/abs/1706.03762.\n",
    "\n",
    "2. Segment anything https://github.com/facebookresearch/segment-anything\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg\" width=\"40%\"/>\n",
    "\n",
    "3. Mediapipe https://mediapipe-studio.webapps.google.com/home\n",
    "\n",
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_5.png\" width=\"40%\"/>\n",
    "\n",
    "All these applications would have been impossible without using enormous amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b90461-2bbd-4035-890b-dd81163e8e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ROFPolZpm21t"
   },
   "source": [
    "## Supervised Learning: Classification vs Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5273876-0e01-454a-bb65-a0d0764f04af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "70_dMCX340Rm"
   },
   "source": [
    "The two main tasks handled by (supervised) ML is regression and classification.\n",
    "\n",
    "In regression we aim at modeling the relationship between the system's response (dependent variable) and one or more explanatory variables (independent variables), acquiring a continuum of values.\n",
    "\n",
    "Examples of regression would be predicting the temperature for each day of the year, or expenses of the household as a function of the number of children and adults.\n",
    "\n",
    "In classification the aim is to identify what class does a data-point belong to. For example, the species or the iris plant based on the size of its petals, or whether an email is spam or not based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7d2301-2647-4990-9862-0e2d7e215a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qBXGs0xRERuv"
   },
   "source": [
    "## Performance measures (model evaluation and training)\n",
    "\n",
    "Performance measures evaluate how much the model reflects the dataset, and differ according to the task (Regression/classification), and to the application. Here are the most popular:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9449c7f6-2332-47c9-a5d6-2150fde7fd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Lx37P09Vkepw"
   },
   "source": [
    "### 1. Regression\n",
    "\n",
    "Let $\\hat y(x_i)$ be the prediction for input $x_i$, $y_i$ the true value, and $\\bar y=\\frac{1}{n}\\sum_i y_i$ the mean.\n",
    "\n",
    "- **Mean Square Error (MSE):**\n",
    "  $$\\mathrm{MSE} = \\frac{1}{n} \\sum_i (y_i - \\hat y(x_i))^2$$\n",
    "- **Root Mean Square Error (RMSE):**\n",
    "  $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_i (y_i - \\hat y(x_i))^2}$$\n",
    "- **Mean Absolute Error (MAE):**\n",
    "  $$\\mathrm{MAE} = \\frac{1}{n} \\sum_i |y_i - \\hat y(x_i)|$$\n",
    "- **Explained Variance ($R^2$):**\n",
    "  $$R^2 = 1 - \\frac{\\sum_i (y_i - \\hat y(x_i))^2}{\\sum_i (y_i - \\bar y)^2}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Classification\n",
    "\n",
    "The confusion matrix and related metrics are the workhorses for classification. We distinguish binary and multiclass cases:\n",
    "\n",
    "#### **Binary Case**\n",
    "\n",
    "|            | Predicted 0 (Negative) | Predicted 1 (Positive) |\n",
    "|------------|------------------------|------------------------|\n",
    "| **True 0** | TN (True Negative)     | FP (False Positive)    |\n",
    "| **True 1** | FN (False Negative)    | TP (True Positive)     |\n",
    "\n",
    "(class 0 is the negative one, class 1 the positive)\n",
    "\n",
    "- **Accuracy:**\n",
    "  $$\\mathrm{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$\n",
    "- **Precision:**\n",
    "  $$\\mathrm{Precision} = \\frac{TP}{TP + FP}$$\n",
    "  ( For every time that I detected class 1, how many times was I correct? )\n",
    "- **Recall:**\n",
    "  $$\\mathrm{Recall} = \\frac{TP}{TP + FN}$$\n",
    "  ( Among all possible samples with class 1, how many was I able to detect? )\n",
    "\n",
    "---\n",
    "\n",
    "#### **Multiclass Case**\n",
    "\n",
    "The confusion matrix is a $K \\times K$ table ($K$ = number of classes):\n",
    "\n",
    "|        | Pred 0 | Pred 1 | Pred 2 | Pred 3 | Pred 4 |\n",
    "|--------|--------|--------|--------|--------|--------|\n",
    "| True 0 | $a_{00}$ | $a_{01}$ | $a_{02}$ | $a_{03}$ | $a_{04}$ |\n",
    "| True 1 | $a_{10}$ | $a_{11}$ | $a_{12}$ | $a_{13}$ | $a_{14}$ |\n",
    "| True 2 | $a_{20}$ | $a_{21}$ | $a_{22}$ | $a_{23}$ | $a_{24}$ |\n",
    "| True 3 | $a_{30}$ | $a_{31}$ | $a_{32}$ | $a_{33}$ | $a_{34}$ |\n",
    "| True 4 | $a_{40}$ | $a_{41}$ | $a_{42}$ | $a_{43}$ | $a_{44}$ |\n",
    "\n",
    "- **Accuracy:**\n",
    "  $$\\mathrm{Accuracy} = \\frac{\\sum_{i=1}^K a_{ii}}{\\sum_{i,j=1}^K a_{ij}}$$\n",
    "- **Precision, Recall:**\n",
    "  We can define precision and recall *per class*. Try to answer the questions: ( *For every time that I detected class xxx, how many times was I correct?* , and *Among all possible samples with class xxx, how many was I able to detect?* )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1de007-82b7-4fec-afd9-f8c3c1f90cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "A similarity measure between the model and the true dataset is needed both for:\n",
    "\n",
    "- **training/fitting a model (loss function)** \n",
    "- **performance evaluation (metrics)**.\n",
    "\n",
    "For training, usually **differentiable loss functions** are employed, which can be minimized via gradient descent. For regression tasks, common choices are the Mean Square Error (MSE) or the Mean Absolute Error (MAE). For classification tasks, cross-entropy loss is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b36b6b-10b4-477d-b1da-0a484145a79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AD6zwuTHiYKA"
   },
   "source": [
    "## Actual aim: Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c9fa11-2319-4516-b603-cbdd3f934625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hdBuAT-46ENx"
   },
   "source": [
    "In supervised machine learning we distinguish two faces: `training` and `testing`. To measure model performance in an unbiased way, we need to use different data to test the model than the data that the model was trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4453c245-5f81-458c-98cd-bb198386d150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PxYGpTmk6ENx"
   },
   "source": [
    "Therefore we often use the 'train-test' split: e.g. 20% of all available dataset is reserved for model performance test, and the remaining 80% is used for actual model training (these percentages may vary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6331d59c-3f68-410e-82e4-f6de6536acb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CAVEAT: When the test set contains information about the training set that can spoil the resulting metrics we talk about `data leakage` or `information leakage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708ed72a-65a6-4bd9-aead-8aecf0799204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimizing generalization error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee053a2-4e2e-4a21-8fc2-0114d6877292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To understand the origins of poor generalization we need to identify two source of errors: bias and variance. In order to understand this concept we perform a thought experiment. Suppose we use N examples to fit our model, and get a prediction $f_1(x)$. Now we choose other N examples and get another prediction $f_2(x)$ for the same sample. We can follow this procedure many times collecting $f_i(x),i=1...N_{trials}$.\n",
    "\n",
    "The **variance** of the model is the spread of $f_i(x), i=1...N_{trials}$, saying how much the predictions are stable.\n",
    "\n",
    "The **bias** of the model compares the mean value of $f_i(x), i=1...N_{trials}$ w.r.t to the real value $f(x$)$.\n",
    "\n",
    "The total error has a bias and a variance component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c4207a-02a6-4df9-b1a5-7d4d04720f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_1.png\" width=\"100%\">\n",
    "    </td>\n",
    "    <td>\n",
    "        \n",
    "**Dictionary of terms**\n",
    "\n",
    "**variance:** suppose the model is trained again with different, but same number of examples. What would be the spread of the predictions on the test set?\n",
    "<br><br>\n",
    "\n",
    "**bias**: suppose the model is trained again with different, but same number of examples. How would the mean of these predictions differ from the real value?\n",
    "<br><br>\n",
    "\n",
    "**model capacity**: models with low capacity can fit just simple mappings (e.g. linear models)\n",
    "<br><br>\n",
    "\n",
    "**overfitting**: model is too complex and learns pattern leading to a very small training set error --> generalization error can be reduced.\n",
    "<br><br>\n",
    "\n",
    "**underfitting**: model is too simple w.r.t. the patterns that can be extracted from the data --> generalization error can be reduced.\n",
    "<br><br>\n",
    "\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60459865-e4f9-43f4-a8c5-d9296136d362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see these concept in a one dimensional example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a533265-5ae2-409a-af00-dbd2947fb513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# run code below, just once \n",
    "# %pip install --upgrade \"numpy<2.0.0\"\n",
    "# dbutils.library.restartPython()  # restart interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181b0dbd-8337-4582-b311-cfd7c9905378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import os\n",
    "from packaging import version\n",
    "import shutil\n",
    "import sys \n",
    "\n",
    "\n",
    "def download_and_extract_data(\n",
    "    url=\"https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz\",\n",
    "    target_dir=\"data\",\n",
    "    fname=\"colab_material.tgz\",\n",
    "    update_folder=False\n",
    "):\n",
    "    \"\"\"Download and extract a tar.gz dataset into target_dir.\"\"\"\n",
    "    \n",
    "    if update_folder and os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        cache_dir = os.path.abspath(\".\")\n",
    "\n",
    "        if version.parse(tf.__version__) >= version.parse(\"2.13.0\"):\n",
    "            # new behavior: fname must be only a filename\n",
    "            path = tf.keras.utils.get_file(\n",
    "                fname=fname,\n",
    "                origin=url,\n",
    "                cache_dir=cache_dir\n",
    "            )\n",
    "        else:\n",
    "            # old behavior: can pass full path\n",
    "            path = tf.keras.utils.get_file(\n",
    "                fname=os.path.join(cache_dir, fname),\n",
    "                origin=url\n",
    "            )\n",
    "        # extract tar into target_dir\n",
    "        with tarfile.open(path, \"r:gz\") as tar:\n",
    "            tar.extractall(target_dir)\n",
    "    else:\n",
    "        print('Data already present. Use update_folder = True to overwrite/update if desired.')\n",
    "    return os.path.abspath(target_dir)\n",
    "\n",
    "data_path = download_and_extract_data(update_folder=False)\n",
    "sys.path.append(data_path)\n",
    "print(\"Data available at:\", data_path)\n",
    "\n",
    "import utils.routines as routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57dab98-2b3a-4750-95d5-8db59091dc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "routines.plot_variance_example(num_points=5, seed=216)\n",
    "#routines.plot_variance_example(num_points=20, seed=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e620f4-03fb-4309-958c-1c1f814784ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#routines.plot_bias_example(num_points=5, frequency=1, seed=23)\n",
    "routines.plot_bias_example(num_points=100, frequency=1, seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f499feb-fc9b-4ae0-aed6-6429b660f389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The total error depends both on bias and variance, that therefore must be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b6f050-efb3-4034-b5c2-9201de40bae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_2.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5895d8f1-b724-4d8c-933c-3155ceb1e0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optimizing capacity is one of the reasons for which we have a pletora of methods in supervised machine learning. If we recognize the origin of the error (bias or variance) we may choose properly what model to try out next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2785d603-a209-4593-ae00-30018bdeeb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extensions to simple train-test splitting :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a24992-1ac4-4830-8514-e715e0d5ba12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cross validation:\n",
    "\n",
    "For small datasets the estimate on the test error may be unstable. A possible solution is using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e4a9eb-1ccc-4c37-ae03-6c43582c3a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/cv.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09724b58-5858-425a-bb69-3a860911dea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train-Val-Test split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9996cc6-b4e4-4662-8d58-13a4cceb5085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When selecting the best model it is natural to apply the previous procedure several times and choose the one minimizing the test error. \n",
    "\n",
    "This is OK but when reporting the final model performance (e.g. MSE) this may lead to a biased estimate, since the test set has been used when choosing the model and therefore the performance measure will not be independent of the data used to train it. \n",
    "\n",
    "That is why in such scenarios we hold-out the test set, and use a validation set to perform model selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffd76ae-5dc3-4cd1-a497-fb9cd928a8ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/train_val_test.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce65095-da04-47b1-a67d-8ce892f45459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NVSRftm8X1m1"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2406f2fa-8318-4527-81d0-c4e6f1943d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hVJn0ilgOS8F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free\n",
    "# software machine learning library for the Python programming language.\n",
    "# It features various classification, regression and clustering algorithms,\n",
    "# and is designed to interoperate with the Python numerical and scientific\n",
    "# libraries NumPy and SciPy. (from wiki)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# common visualization module\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# numeric library\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time as timer\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import utils.routines as routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14244723-7b65-4994-a227-0bdd24442153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need the function download_and_extract_data if the notebook is run on Colab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7785321-24e6-445e-b9bf-2e202d3fc277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "pclZR6uFklf_"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce81798-1414-4f93-bc9f-745ea612a0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "s_wxOrdWko8W"
   },
   "source": [
    "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5706abef-be87-4ada-af7c-3e4db6011a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8UQgU5I-lEll"
   },
   "source": [
    "## 1. Synthetic linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa84581-5ccf-4b9d-be05-49b2cf8d3bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jGfWOWRjlWPa"
   },
   "outputs": [],
   "source": [
    "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
    "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
    "\n",
    "  w = w or np.random.uniform(0.1, 10, n_d)\n",
    "  b = b or np.random.uniform(-10, 10)\n",
    "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
    "\n",
    "  print('true slopes: w =', w, ';  b =', b)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91ee0f0-2b45-446c-bcf9-eeada8c69a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "5RLYxGy_nBZG",
    "outputId": "4836570c-c789-4c16-bda7-37aa222f66fc"
   },
   "outputs": [],
   "source": [
    "x, y = get_linear(n_d=1, sigma=1)\n",
    "plt.plot(x[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf819153-78cd-4d5d-bd94-f2c16453a09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "10ODDOp4nX4S",
    "outputId": "5a70b4e8-5e8b-4cd6-e890-c5eb1d5261ca"
   },
   "outputs": [],
   "source": [
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=100)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=10)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b8d75c-1d52-4797-8d24-d1b5ca7ddc15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FJ5rjq7fIe8Q"
   },
   "source": [
    "## 2. House prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6acccd3-0c31-42d3-97b3-9e85271bf425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "A-45usskInlD"
   },
   "source": [
    "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3452e272-846e-4047-b7da-b86b2a655a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dVv2ID96IyN0"
   },
   "outputs": [],
   "source": [
    "def house_prices_dataset(return_df=False, return_df_xy=False, price_max=400000, area_max=40000, data_path='/Workspace/Users/david.schwelien@swissinfo.ch/CAS-Applied-Data-Science/DSF5 - copy'):\n",
    "  path = os.path.join(data_path, 'data/AmesHousing.csv')\n",
    "\n",
    "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False,  )\n",
    "\n",
    "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
    "  df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "  useful_fields = ['LotArea',\n",
    "                  'Utilities', 'OverallQual', 'OverallCond',\n",
    "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
    "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
    "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
    "                  'FullBath', 'HalfBath',\n",
    "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
    "                  'Functional','PoolArea',\n",
    "                  'YrSold', 'MoSold'\n",
    "                  ]\n",
    "  target_field = 'SalePrice'\n",
    "\n",
    "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
    "\n",
    "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
    "                  'LotFrontage': {'NA':0},\n",
    "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
    "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
    "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
    "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
    "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'CentralAir':  {'N':0, 'Y': 1},\n",
    "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
    "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
    "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
    "                  }\n",
    "\n",
    "  df_X = df[useful_fields].copy()\n",
    "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
    "  df_Y = df[target_field].copy()\n",
    "\n",
    "  x = df_X.to_numpy().astype(np.float32)\n",
    "  y = df_Y.to_numpy().astype(np.float32)\n",
    "\n",
    "  if price_max>0:\n",
    "    idxs = y<price_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  if area_max>0:\n",
    "    idxs = x[:,0]<area_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  return (x, y, df) if return_df else ((x, y, (df_X, df_Y)) if return_df_xy else (x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8663ee-b975-4609-8270-35d45f10b03b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "YqWU0eHts1RM",
    "outputId": "b61e8e82-7d5a-4191-b70a-80fed57c8d2e"
   },
   "outputs": [],
   "source": [
    "x, y, df = house_prices_dataset(return_df=True)\n",
    "print(x.shape, y.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a3de79-61ec-4e03-b256-d8de87efc315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "YDtzVS-1Mxxe",
    "outputId": "77dbfa01-62a5-49fa-fcb8-cc2a147d96d4"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531c3ad8-3bbb-4ac6-96be-758e87e58769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7696d0-06c0-48a7-aad8-17efc96fcdc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "91nj7znzMEpA",
    "outputId": "99410485-f9f2-4e59-b9ef-0f07cfb5b1e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(x[:, 0], y, '.')\n",
    "plt.xlabel('area, sq.ft')\n",
    "plt.ylabel('price, $');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e85c0d-bfb6-4744-a832-7b267344d7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "baseline_error = mean_squared_error(y, y_pred, squared=False)\n",
    "baseline_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f292e3d-30d8-4c9c-89d7-33cad45ad055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "q7CNxkPdNB4L"
   },
   "source": [
    "## 3. Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6dc413-e036-401c-8656-71b3a988e379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "j8wXhleONKgZ",
    "outputId": "7e0e974c-17e2-4cf9-bdc8-7a69f0537c5a"
   },
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
    "colors = \"ygr\"\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y == i\n",
    "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2767cb08-4258-4cb7-ae52-e7c09775af20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "NKcmdcZf0VO8",
    "outputId": "55f512ea-94f5-4a33-c295-d05b83dbd770"
   },
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
    "\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]  # affine transformation matrix\n",
    "x = np.dot(x, transformation)               # applied to point coordinated to make blobs less separable\n",
    "\n",
    "colors = \"ygr\"\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y == i\n",
    "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d177786-970c-4228-99af-86e02ce2870b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8S1jwU4cXQX4"
   },
   "source": [
    "## 4. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d0b1481-d521-40cc-9f09-9e10f69eabc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "e2u82UQ5XQX4"
   },
   "source": [
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2da49cf-0448-4e15-8c7a-b5641d46c73f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JaNaGGOkXQX5"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a712ae-1a70-4b5d-8667-ecc5d2803e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dlUY5gl8XQX7"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083d3ce5-2ba7-4480-a4d5-8ebf3edae6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "qtYtGEDdXQX8",
    "outputId": "7e7c988f-8e55-480b-dcee-99860e00a947"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5725ee-35d0-45a4-9630-135e51dd84fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ITfbaOgfYNsq"
   },
   "source": [
    "## 5. Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52326949-f7be-44b0-b0bf-f718341a9e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jgzzOS7YYTru"
   },
   "source": [
    "`Fashion-MNIST` is a dataset of Zalando's article imagesconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde8c525-a0e0-47fd-9f28-30ff8a728345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RcV2gzmuYljJ"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb607bd-915a-49de-a273-06e91c95ca02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SPw6-GoPbT6U"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b69e45-3571-4552-bcb4-3a50315eb776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "tHFd0sFHY4Li",
    "outputId": "19fc320d-bf19-4f5b-b36d-dd6a9f74ebe6"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400f5b68-dcc5-4e00-a9d0-a0080b0c7f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "b2LkoWfZEi4g"
   },
   "outputs": [],
   "source": [
    "fmnist_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07722574-5b9b-4784-a9b7-830083d2c943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iHEA0tCLagoV"
   },
   "source": [
    "Each of the training and test examples is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b79b012d-2498-409d-ad49-c22a4a3b05e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RHRXds9U9134"
   },
   "source": [
    "# `scikit-learn` interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b8d879-b815-48eb-bdd1-0eb8465efd7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "I2toQKrAzH_U"
   },
   "source": [
    "In this course we will primarily use the `scikit-learn` module.\n",
    "You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface.\n",
    "\n",
    "All the different ML approaches are implemented as classes with a set of same main methods:\n",
    "\n",
    "1. `fitter = ...`: Create fitter object.\n",
    "2. `fitter.fit(x, y[, sample_weight])`: Fit model to predict from list of smaples `x` a list of target values `y`.\n",
    "3. `y_pred = fitter.predict(X)`: Predict using the trained model.\n",
    "4. `s = fitter.score(x, y[, sample_weight])`: Obtain a relevant performance measure of the trained model.\n",
    "\n",
    "This allows one to easily replace one approach with another and find the best one for the problem at hand, by simply using a regression/classification object of another class, while the rest of the code can remain the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5f5fc2-e925-4c35-9dfc-6f3797aa0e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xqLR5-eQ2vtz"
   },
   "source": [
    "It is useful to know that generally in scikit-learn the input data is represented as a matrix $X$ of dimensions `n_samples x n_features` (also called the `design matrix`), whereas the supervised labels/values are stored in a matrix $Y$ of dimensions `n_samples x n_target` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5823e-930b-464c-a403-2239561c967e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K4qgOdz7Yyeb"
   },
   "source": [
    "# 1.Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84530a0-fd2e-4063-9d03-ae2575453826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Hh6lII-Hz8u-"
   },
   "source": [
    "In many cases the scalar value of interest - dependent variable - is (or can be approximated as) linear combination of the independent variables.\n",
    "\n",
    "In linear regression the estimator is searched in the form: $$\\hat{y}(\\bar{x} | w_0,\\bar{w}) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
    "\n",
    "The parameters $\\bar{w} = (w_1,..., w_p)$ and $w_0$ are designated as `coef_` and `intercept_` in `sklearn`.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/linear_model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875dd383-3ce2-43b5-805e-7517b8eab4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Vlf6_berQ1vq"
   },
   "source": [
    "## 1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c18b7e-ee63-47c0-85b1-8655b8be1b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zatxRr8bOuTs"
   },
   "source": [
    "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients $\\bar{w} = (w_1,..., w_p)$ and $w_0$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Mathematically it solves a problem of the form:  $$(w^{opt}_0,\\bar{w}^{opt}) = \\arg min_{w_0,\\bar{w}} \\sum_i \\left(\\hat{y}(\\bar{x_i} | w_0,\\bar{w})-y_i\\right)^2$$\n",
    "\n",
    "The function:\n",
    "\n",
    "$$L(w,\\bar{w}) \\equiv \\sum_i \\left(\\hat{y}(\\bar{x_i} | w_0,\\bar{w})-y_i\\right)^2$$\n",
    "\n",
    "is called the `loss function` for linear regression and we say that during training/fitting the loss function is `minimized`.\n",
    "\n",
    "In terms of the design matrix $X_{i,p}$, where $i$ ranges over the number of samples and $p$ across the number of features:\n",
    "\n",
    "$$L(w,\\bar{w})=\\sum_i \\left((w_0 + \\sum_p X_{i,p}w_p)-y_i\\right)^2$$\n",
    "\n",
    "This makes it explicit that the function to minimize is quadratic in $w$ and an analytical solution is therefore available (no numerical optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e52e1c-245b-4b41-8fc9-f557700e1291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "sqh7XwGkNg6r",
    "outputId": "c55c5ff0-ae94-4133-b74c-58d5deaa1192"
   },
   "outputs": [],
   "source": [
    "x, y = get_linear(n_d=1, sigma=3, n_points=30)  # p==1, 1D input\n",
    "plt.scatter(x, y);\n",
    "plt.xlabel('x')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23dc0749-5663-4bba-9650-85de455f4638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "IFawJfQJOKX3",
    "outputId": "9fdacb82-684c-4423-d0bd-a42ca012b069"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669f5c7a-0df5-472b-bd65-0028cd55c66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diHNLTNMOek5",
    "outputId": "0700b0cf-c1e9-4200-b3c3-0a656e2508ee"
   },
   "outputs": [],
   "source": [
    "w, w0 = reg.coef_, reg.intercept_\n",
    "print(w, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b14ad4f-18db-481c-a0f1-95f881487828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "hyeHY3bxPYSF",
    "outputId": "7571cce4-5953-4f73-c7dd-61238e9227d0"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, marker='*', label='data points')\n",
    "x_f = np.linspace(x.min(), x.max(), 10)\n",
    "y_f = w0 + w[0] * x_f\n",
    "plt.plot(x_f, y_f, label='fit', c='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c4f70f-0ca3-415e-8764-8c36d60c8bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNX-5gYOIi40",
    "outputId": "2952f62c-9a84-42c8-bcb7-8e68d7379a3b"
   },
   "outputs": [],
   "source": [
    "# mse\n",
    "print(np.mean( (y - reg.predict(x)) **2 ), metrics.mean_squared_error(y, reg.predict(x))) \n",
    "\n",
    "# rmse\n",
    "print(np.sqrt(np.mean( (y - reg.predict(x)) **2 )), np.sqrt(metrics.mean_squared_error(y, reg.predict(x)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c48713-efa9-4854-ab2c-1ee839aa7018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID0Hdzx0NvxF",
    "outputId": "457d48d4-104f-422f-8563-f66db972fc42"
   },
   "outputs": [],
   "source": [
    "# R2\n",
    "reg.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cdca8c-50d5-44b3-9e29-bd6fcbebf1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7rg2_DZCHgJE"
   },
   "source": [
    "Let's try 2D input.\n",
    "Additionally, here we will split the whole dataset into training and test subsets using the `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a096a375-8d25-4712-b6c1-e1d8e66d404f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "oK5MILosSI7d",
    "outputId": "7cbdc3a8-a44a-4ecc-93b3-53a713fce810"
   },
   "outputs": [],
   "source": [
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=1000, sigma=5)\n",
    "\n",
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', s=40)\n",
    "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', s=80)\n",
    "\n",
    "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
    "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
    "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
    "xx = np.stack((xx0, xx1), axis=-1)\n",
    "yy = reg.predict(xx)\n",
    "ax.plot_trisurf(xx0, xx1, yy, alpha=0.25, linewidth=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e72eb5-f5e1-467f-a7ef-f0020c326091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kW5GiLlhS3Y8",
    "outputId": "477c0484-bae2-44a7-87ac-6d1d984ddcf7"
   },
   "outputs": [],
   "source": [
    "# rmse\n",
    "\n",
    "print('train rmse =', np.sqrt( np.mean( (y_train - reg.predict(x_train))**2 ) ) )\n",
    "print('test rmse =', np.sqrt( np.mean( (y_test - reg.predict(x_test))**2  ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a649a2-867a-4f3e-aac9-69a4b644e53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_Fb1zb5S3ZG",
    "outputId": "8e3c53b7-0a76-4d13-fa83-1872a441666c"
   },
   "outputs": [],
   "source": [
    "# R2\n",
    "print('train R2 =', reg.score(x_train, y_train))\n",
    "print('test R2 =', reg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306631c5-4f72-4dd7-b435-0249400f0007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zI6s2Amob48j"
   },
   "source": [
    "## EXERCISE 1. Linear Regression for AMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56680ab-80dc-41f5-b446-0e79f1b5e034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this exercise we will inspect a regression model. We will build a trivial baseline over which to compare, and plot exact values vs predicted.\n",
    "\n",
    "Having a trivial baselines is often of paramout importance, especially in more complex settings, because it helps to decide if the model has any predictive power at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4e68ff-7e07-46f4-9c0f-5612c90df8a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARM-UP: built a baseline\n",
    "\n",
    "x, y = house_prices_dataset()\n",
    "print(x.shape,y.shape)\n",
    "_ = plt.hist(y, bins=20, density=True)\n",
    "plt.title('Distribution of house prices')\n",
    "plt.xlabel('$')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "baseline = np.mean(y)\n",
    "residuals = y - baseline\n",
    "print('rmse =', np.std(residuals))\n",
    "print('rmse =', np.sqrt( np.mean(residuals**2)   ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bc483e-eb1f-4ffe-85d6-380a10b9d238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zRi8SPiMb9FM"
   },
   "source": [
    "Use linear regression to fit house prices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d184d91-e26e-44d0-acba-cfc335811110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vaQVHyvPcHW2"
   },
   "outputs": [],
   "source": [
    "# 1. make train/test split\n",
    "\n",
    "# 2. fit the model\n",
    "\n",
    "# 3. evaluate RMSE, MAE and R2 on train and test datasets\n",
    "\n",
    "# 4. plot y vs predicted y for test and train parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0fc2d5-8c71-4f89-ab92-882cc85df462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. make train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# 2. fit the model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "# 3. evaluate RMSE, MAE and R2 on train and test datasets\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_train_pred = reg.predict(x_train)\n",
    "y_test_pred = reg.predict(x_test)\n",
    "\n",
    "print('Train RMSE:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print('Test RMSE:', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "print('Train MAE:', mean_absolute_error(y_train, y_train_pred))\n",
    "print('Test MAE:', mean_absolute_error(y_test, y_test_pred))\n",
    "print('Train R2:', r2_score(y_train, y_train_pred))\n",
    "print('Test R2:', r2_score(y_test, y_test_pred))\n",
    "\n",
    "# 4. plot y vs predicted y for test and train parts\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
    "plt.xlabel('True y (train)')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Train: True vs Predicted')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.xlabel('True y (test)')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Test: True vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9acb31-34c4-4c8c-95b4-6f0e373e9eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zZX9MQlORLfY"
   },
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144d6891-01e0-4885-b61c-141d65997489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yRUwQD5UR0Vf"
   },
   "source": [
    "## Logistic Regression: Binary Case\n",
    "\n",
    "Logistic regression is a **linear model for classification**, not regression. Typical binary outputs include:\n",
    "- {sold, not sold} (e.g., predicting if a product will be sold)\n",
    "- {failed, passed} (e.g., predicting if a student will fail a class)\n",
    "\n",
    "We map the two classes to targets $y_i \\in \\{0,1\\}$, called the positive and negative class, respectively.\n",
    "\n",
    "**Model:**\n",
    "The probability $p$ of a point belonging to the positive class is modeled as:\n",
    "$$p = \\sigma(w_0 + w_1 x_1 + ... + w_p x_p)$$\n",
    "where the sigmoid function is:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "The sigmoid maps the real-valued output of a linear function to $[0,1]$.\n",
    "\n",
    "**Loss function:**\n",
    "Parameters $w$ are trained to minimize:\n",
    "$$L(w_0, \\bar{w}) = -\\sum_i y_i \\log(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af70d651-3dc5-4ea4-a436-8bc0e69dc37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_y3zzbwc6EN1"
   },
   "source": [
    "## Logistic Regression: More Classes\n",
    "\n",
    "A **multiclass** model assigns each sample to one of several mutually exclusive classes (e.g., predicting treatment response: {negative, not significant, positive}).\n",
    "\n",
    "To extend the binary model, we use **multinomial logistic regression**. This computes logits for each class and aggregates them using the softmax operator:\n",
    "\n",
    "$$\n",
    "\\text{logits}_c = w_{c,0} + w_{c,1} x_1 + ... + w_{c,p} x_p\n",
    "$$\n",
    "$$\n",
    "p_c = \\text{softmax}(\\text{logits})_c = \\frac{e^{\\text{logits}_c}}{\\sum_{c'} e^{\\text{logits}_{c'}}}\n",
    "$$\n",
    "\n",
    "Probabilities are normalized before training.\n",
    "\n",
    "**Loss function:**\n",
    "Parameters are trained to minimize:\n",
    "$$\n",
    "L(\\{w_{0,c}\\}, \\{\\bar{w}_c\\}) = -\\sum_i \\left(\\sum_c y_i^{OH}(c) \\log p_{i,c}\\right)\n",
    "$$\n",
    "where $y_i^{OH}(c) = 1$ if sample $i$ belongs to class $c$, $0$ otherwise (one-hot encoding).\n",
    "\n",
    "This converts a multiclass label (e.g., $y_i \\in \\{cat, dog, other\\}$) into binary labels: $y_i^{OH}(dog), y_i^{OH}(cat), y_i^{OH}(other) \\in \\{0,1\\}$.\n",
    "\n",
    "---\n",
    "**Notes:**\n",
    "- The choices here are based on probability theory and maximum likelihood.\n",
    "- If classes are not exclusive (multilabel), fit individual binary models (one-vs-rest, `ovr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e559f30c-abc7-438d-b1ba-48ada07e3d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4bJHWawkq0ev",
    "outputId": "719fa255-98ed-4db0-c5b0-20611a3b5b5d"
   },
   "outputs": [],
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "x, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "x = np.dot(x, transformation)\n",
    "\n",
    "#for multi_class in ('multinomial', 'ovr'):\n",
    "for multi_class in ['multinomial']:\n",
    "\n",
    "    # do fit\n",
    "    clf = linear_model.LogisticRegression(solver='sag', max_iter=100,\n",
    "                             multi_class=multi_class, )\n",
    "    clf.fit(x, y)\n",
    "\n",
    "    # print the training scores\n",
    "    print(\"training accuracy : %.3f (%s)\" % (clf.score(x, y), multi_class))\n",
    "\n",
    "    # get range for visualization\n",
    "    x_0 = x[:, 0]\n",
    "    x_1 = x[:, 1]\n",
    "    x_min = x_0.min() - 1\n",
    "    x_max = x_0.max() + 1\n",
    "    y_min = x_1.min() - 1\n",
    "    y_max = x_1.max() + 1\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    routines.plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=clf)\n",
    "\n",
    "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = 'rbg'\n",
    "\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(x_0[idx], x_1[idx], c=color, cmap=plt.cm.Paired,\n",
    "                    edgecolor='gray', s=30, linewidth=0.2)\n",
    "\n",
    "    # Plot the three one-against-all classifiers\n",
    "    if multi_class=='ovr':\n",
    "        coef = clf.coef_\n",
    "        intercept = clf.intercept_\n",
    "        def plot_hyperplane(c, color):\n",
    "            def line(x0):\n",
    "                return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "            plt.plot([x_min, x_max], [line(x_min), line(x_max)],\n",
    "                    ls=\"--\", color=color)\n",
    "        for i, color in zip(clf.classes_, colors):\n",
    "            plot_hyperplane(i, color)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52524715-159d-46ca-8094-a8b83ee429ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AQ69XKdbZcA3"
   },
   "source": [
    "## EXERCISE 2. Logistic Regression for FMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586fb757-386c-48ad-9e43-a2eea6d5078e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this exercise we will inspect a multiclass classification model. We will extract some feature X for each sample, and use them to predict the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792add09-effc-4bf9-bc04-a53a98b40800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "__9jcqXzZaQp"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb05e1a-e486-4ec6-9560-abc9c57c41da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is a normalization step, which can help the training of some models. It does not hurt anyway \n",
    "\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29726ac-62ff-485e-aaa6-520d333bcfeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Instead of using the raw images as input to the model (what would be the issue?) we perform a feature extraction step. \n",
    "\n",
    "You can use the following function that takes the frequency content of the feature (power spectrum) as an example.\n",
    "\n",
    "The process of trying out several features extracted from the data as input to a model is called *(manual) feature engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66731d63-a3c6-4455-adb8-9307b168420d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Helper function for point 2\n",
    "\n",
    "def extract_features(images):\n",
    "    \"\"\"\n",
    "    Extracts power spectrum features from a list or array of images.\n",
    "\n",
    "    For each image, computes the 2D Fourier transform, calculates the power spectrum,\n",
    "    normalizes it, and flattens the result into a 1D feature vector.\n",
    "\n",
    "    Args:\n",
    "        images (array-like): List or array of 2D image arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_images, num_features) containing the power spectrum features for each image.\n",
    "    \"\"\"\n",
    "    def compute_power_spectrum(image):\n",
    "        fourier_transform = np.fft.fft2(image)\n",
    "        power_spectrum = np.abs(fourier_transform) ** 2\n",
    "        power_spectrum /= np.sum(power_spectrum)\n",
    "        return power_spectrum.flatten()\n",
    "    power_spectrum_features = []\n",
    "    for img in images:\n",
    "        spectrum = compute_power_spectrum(img)\n",
    "        power_spectrum_features.append(spectrum)\n",
    "    return np.array(power_spectrum_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7ee12b-8bdd-464f-a957-e01170402ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UJj7ofWD_Wp2"
   },
   "source": [
    "Now use a multinomial logistic regression classifier, and measure the accuracy, following the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca20ebd-de1c-43f2-aaf3-e6099c154874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CeIKcMeV_rmk"
   },
   "outputs": [],
   "source": [
    "# 1. Create classifier\n",
    "\n",
    "# 2. Extract Features\n",
    " \n",
    "# 3. fit the model\n",
    "\n",
    "# 4. evaluate accuracy on train and test datasets. Use for this the reg.score\n",
    "\n",
    "# 5. evaluate the confusion matrix on the test set using, from sklearn import metrics the metrics.confusion_matrix method (check documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ee19ec-5766-4b99-b40a-fb330ec5efdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# 1. Create classifier\n",
    "reg = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=100)\n",
    "\n",
    "# 2. Extract Features\n",
    "X_train = extract_features(train_images_norm)\n",
    "X_test = extract_features(test_images_norm)\n",
    "\n",
    "# 3. fit the model\n",
    "reg.fit(X_train, train_labels)\n",
    "\n",
    "# 4. evaluate accuracy on train and test datasets. Use for this the reg.score\n",
    "train_acc = reg.score(X_train, train_labels)\n",
    "test_acc = reg.score(X_test, test_labels)\n",
    "print(f\"Train accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# 5. evaluate the confusion matrix on the test set\n",
    "y_pred = reg.predict(X_test)\n",
    "cm = metrics.confusion_matrix(test_labels, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d2e79c-67cd-4771-ac5e-e48f8e23c70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can summarize the confusion matrix plotting precision and recall for each category. Given the confusion matrix you can use the following funtion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f014345d-cd67-48d1-902e-d122336249d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for point 6\n",
    "\n",
    "def get_class_specific_prec_recall(cm):\n",
    "    \"\"\"\n",
    "    Given a confusion matrix cm, computes precision and recall for each class.\n",
    "\n",
    "    Precision for class i: cm[i, i] / sum of column i (true positives / predicted positives)\n",
    "    Recall for class i: cm[i, i] / sum of row i (true positives / actual positives)\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): Confusion matrix of shape (n_classes, n_classes)\n",
    "\n",
    "    Returns:\n",
    "        precisions (list): Precision for each class\n",
    "        recalls (list): Recall for each class\n",
    "    \"\"\"\n",
    "    n_cat = cm.shape[0]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    # precision\n",
    "    for i in range(n_cat):\n",
    "        precisions.append(cm[i, i] / np.sum(cm[:, i]))\n",
    "\n",
    "    # recall\n",
    "    for i in range(n_cat):\n",
    "        recalls.append(cm[i, i] / np.sum(cm[i, :]))\n",
    "    return precisions, recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aadebe2-509a-4230-9f13-8dfeb8f32c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6 (optional). Inspect the confusion metrics, compute using matrix manipulations the precision and recall for each category and make a plot where on the x axes you have the categories and on the y axes precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be3fbfc-c019-4b22-b28b-1045df791ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6 (optional). Inspect the confusion metrics, compute using matrix manipulations the precision and recall for each category and make a plot where on the x axes you have the categories and on the y axes precision and recall.\n",
    "\n",
    "precisions, recalls = get_class_specific_prec_recall(cm)\n",
    "categories = np.arange(len(precisions))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(categories, precisions, marker='o', label='Precision')\n",
    "plt.plot(categories, recalls, marker='s', label='Recall')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall per Category')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(categories)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f06dee5-2a46-47d6-a9f8-c59da78a53e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reflections on exercise 2: \n",
    "\n",
    "1. Do you think that improving the model (making it more complex) we can improve the accuracy ?\n",
    "2. Do you think that improving the dimension of the training set we can improve the accuracy ?\n",
    "3. Do you think that changing the features (feature engineering we can improve the accuracy) ?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Course_1A",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "n_5oRe0SXilM",
    "9SxiIczg1s1k",
    "ll5e8N9SVwVa",
    "5PbjhPxLmsI4",
    "ROFPolZpm21t",
    "qBXGs0xRERuv",
    "AD6zwuTHiYKA",
    "NVSRftm8X1m1",
    "pclZR6uFklf_",
    "8UQgU5I-lEll",
    "FJ5rjq7fIe8Q",
    "q7CNxkPdNB4L",
    "8S1jwU4cXQX4",
    "ITfbaOgfYNsq",
    "x2NWxK0BFwyw",
    "RHRXds9U9134",
    "K4qgOdz7Yyeb",
    "Vlf6_berQ1vq",
    "zI6s2Amob48j",
    "zZX9MQlORLfY",
    "AQ69XKdbZcA3",
    "7-CGSS2OZKHD",
    "Bxtv48o-F1Ku",
    "EHZ-hHGuY5aG",
    "puQNgKN0wS7H",
    "vhhycm2S6wbz",
    "0dG6U6s3T95t"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_dsf5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
