{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec10ada8-3af1-4828-a75f-7ac007cf6cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33391dd2-5593-4f82-8478-d039fb6ebf78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Reading\n",
    "\n",
    "In this step, you will read the data from the provided GitHub URL and load it into a pandas DataFrame. This allows you to easily manipulate and analyze the data using pandas' powerful data analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dbf3667-55c0-4ad3-9ddd-a3c3fc668ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Tao-Pi/CAS-Applied-Data-Science/refs/heads/main/DSF5%20-%20copy/01_Module%20Final%20Assignment/Top-10000-posts-by-page-views-Sep-01-2025-Sep-30-2025-eng-swissinfo-ch-any.csv\"\n",
    "df = pd.read_csv(url)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82576dc1-eb3a-49a1-9d88-1aa11b09fc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extracting Article Content with BeautifulSoup\n",
    "\n",
    "In this step, we will use the BeautifulSoup library to loop over the URLs in our DataFrame and extract key components from each article page. Specifically, we will retrieve the article header, lead (subtitle or introduction), and the main article text. This process involves sending HTTP requests to each URL, parsing the HTML content, and selecting the relevant elements for extraction. The extracted information will be added as new columns to our DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178da0f0-5c40-487e-ba00-2a4fe9c422e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def extract_swi_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Header (Titel)\n",
    "        header = soup.find(class_=\"article-header\")\n",
    "        header_text = header.get_text(strip=True, separator=\" \") if header else None\n",
    "\n",
    "        # Lead (Untertitel oder Einstieg)\n",
    "        lead = soup.find(class_=\"lead-text\")\n",
    "        lead_text = lead.get_text(strip=True, separator=\" \") if lead else None\n",
    "\n",
    "        # Haupttext (Abs√§tze im Artikel)\n",
    "        article = soup.select_one(\"#main-content main article\")\n",
    "        if article:\n",
    "            paragraphs = [p.get_text(strip=True) for p in article.find_all(\"p\")]\n",
    "            article_text = \"\\n\".join(paragraphs)\n",
    "        else:\n",
    "            article_text = None\n",
    "\n",
    "        return {\n",
    "            \"header_text\": header_text,\n",
    "            \"lead_text\": lead_text,\n",
    "            \"article_text\": article_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"header_text\": None,\n",
    "            \"lead_text\": None,\n",
    "            \"article_text\": f\"Error: {e}\"\n",
    "        }\n",
    "\n",
    "urls = df[\"URL\"].tolist()\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = list(executor.map(extract_swi_article, urls))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "df[[\"header_text\", \"lead_text\", \"article_text\"]] = results_df\n",
    "\n",
    "display(df[[\"URL\", \"header_text\", \"lead_text\", \"article_text\"]])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
