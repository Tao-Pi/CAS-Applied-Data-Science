{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "713c4ac6-49d8-4142-b9e2-c0200d259459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VaNsYCMm00Uv"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KGzB/CAS-Applied-Data-Science/blob/master/Module-2/CAS-D2-Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe9f842-326b-4287-b7f6-194954960ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UnOP-6EzHZ9q"
   },
   "source": [
    "Notebook 3, Module 2, Statistical Inference for Data Science, CAS Applied Data Science, 2024-08-28, A. Mühlemann, University of Bern.\n",
    "\n",
    "*This notebook is based on the notebook by S. Haug and G. Conti from 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f88b1a04-dc27-4f42-bad4-756badb5071f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gpx2r-WAHZ9s"
   },
   "source": [
    "# Parameter estimation / regression\n",
    "\n",
    "**Average expected study time :** 3x45 min (depending on your background)\n",
    "\n",
    "**Learning outcomes :**\n",
    "\n",
    "- Know what is meant by parameter estimation and regression\n",
    "- Calculation of confidence intervals via Python\n",
    "- Perform linear regression with Python by example\n",
    "- Perform non-linear regression with Python by example\n",
    "- Know what non-parametric regression is\n",
    "- Perform linear regression with Python scikit-learn by example\n",
    "...\n",
    "\n",
    "**Main python modules used**\n",
    "- the Scipy.stat module https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "- the Scikit-learn module\n",
    "\n",
    "If you run this notebook on google colab, you will (probably) have no problems with importing the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a7e9d1-49eb-419b-a2ba-31284991d830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VWIThuikPO7W"
   },
   "source": [
    "**Link to weighted descriptive statistical methods in python**:\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.stats.weightstats.DescrStatsW.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e9fbf06-4de0-4c94-8a0c-4da5c5b0bbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YrHViWYQHZ9x"
   },
   "source": [
    "# 3. Inferential Statistics I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d71a5df-8d1b-4a8b-9400-7f5fb1b131b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3rFRb9rhHZ90"
   },
   "source": [
    "Import the Python libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bead01fe-5d55-4989-9582-892bd6acc5fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 4382,
     "status": "ok",
     "timestamp": 1724840282682,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "St85roiBHZ91"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752446b5-15a6-4170-8011-701a27ff027f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Y-6YDunfZZpk"
   },
   "source": [
    "In this notebook, we work with a data set that contains records of river melting dates in the Baltic region.\n",
    "The data set contains the columns\n",
    "\n",
    "- *YEAR*: year\n",
    "- *Riga*: day of year when ice melts in the western Dvina river\n",
    "- *Bolderaja*: day of year when ice melts in the Bullupe river\n",
    "- *Parnu*: day of year when ice melts in the Parnu river\n",
    "- *Tallinn*: day of year of ice break date in Tallinn port\n",
    "- *Narva*: day of year of ice break in the Narva river\n",
    "- *St_Petersburg*: day of year of ice break in the Newa river\n",
    "- *Kronstadt*: day of year of ice break in the gulf of Finland\n",
    "- *Porvoo*: day of year of ice break in the Porvoonjoki river\n",
    "- *Turku*: day of year of ice break in the Aura river\n",
    "- *Pori*: day of year of ice break in the Kokemaenjoki river\n",
    "\n",
    "Let us first read the data set from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4673ae7d-86c1-4348-b69a-f5435bfb946f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1724840282692,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "YSg-y1hgHZ99",
    "outputId": "2893362f-cbe8-4ed1-c458-715eba709ec5"
   },
   "outputs": [],
   "source": [
    "url = \"https://github.com/KGzB/CAS-Applied-Data-Science/blob/master/Module-2/baltikum_eisschmelze.csv?raw=true\"\n",
    "df = pd.read_csv(url, sep=\";\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c71d5e1-1a71-4742-bab0-ae92e5484323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "O9-YVTFpH9MD"
   },
   "source": [
    "We can see that there are places where we have a lot of missing data. Thus let us first look at the available records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27087f39-2075-471c-a2d6-311e55274737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2872,
     "status": "ok",
     "timestamp": 1724840285545,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "7moARexNIY0K",
    "outputId": "52057801-bc45-40e4-cf7a-8465714f21e5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "colours = ['seagreen', 'white']\n",
    "sns.heatmap(df.isnull(), cmap=sns.color_palette(colours))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57e643a9-c504-4021-b7aa-af6ad52f48c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HEI4Y963J62x"
   },
   "source": [
    "We can see that the time series in from Tallinn is the most complete one already starting in 1600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad315b20-b567-4184-8160-1734637d65b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cWPbHuDbVzVG"
   },
   "source": [
    "## 3.1 Model parameter estimation\n",
    "Let us start with model parameter estimation.\n",
    "To this end we look at the DOY (day of year) the ice broke up in St_Petersburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e29cc0f9-6a3d-4410-9879-2de988e785f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1724840285546,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "FA4RUOWbKeEv"
   },
   "outputs": [],
   "source": [
    "# Histogram of St Petersburg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "829c7ad3-b0cd-4ae0-a468-55a3cf2250a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QPjlMfAcMN7t"
   },
   "source": [
    "The histogram suggests that the feature is slightly skewed. But let us nevertheless how well a normal model would fit this feature. Because the mean $\\mu$ and the standard deviation $\\sigma$ (or variance $\\sigma^2$, respectively) completely characterize the form of the normal pdf, we need to estimate the mean and standard deviation from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab2cbec0-4ccc-49e8-9170-6f00f194c7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1724840285546,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "EHC4Q_mOVspZ"
   },
   "outputs": [],
   "source": [
    "# Estimation of mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d2a50b-1d3a-4f23-b472-262defa7d293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jT_04i14cuDM"
   },
   "source": [
    "Thus, as an estimate for the mean we get 111.20 and 8.67 for the standard deviation. Let us now visually compare the histogram the the estimated normal pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79f3b6a-8565-4df9-bb65-a9beb9c32fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1724840285547,
     "user": {
      "displayName": "Anja Mühlemann",
      "userId": "13180139022750424409"
     },
     "user_tz": -120
    },
    "id": "2cM7b2eiYwbK"
   },
   "outputs": [],
   "source": [
    "# Create histogram and add the normal pdf with the estimated mean and standard deviation from above\n",
    "\n",
    "# Calculate the normal pdf based on the estimates above\n",
    "\n",
    "# add the pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14f36c2-8b58-4456-850a-83d0ac72b1d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qlcjT_YeYr7T"
   },
   "source": [
    "Looks not too bad, but the left tail is probably a bit too heavy while the right tail is less heavier that for a normal distrubition. Thus, a slightly skewed distribution model would probably suit better. Tomorrow and below we will see more precise options to check normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a0627d-512d-4732-bab7-8d824ca5cd86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9EW2fVflHZ-i"
   },
   "source": [
    "### Example Exponential p.d.f.\n",
    "\n",
    "This example shall illustrate that even in perfect world where no measurement errors occure, just the fact that we work on a sample rather the whole population does introduce a lot of uncertainty. We now draw a sampe of size 50 from a exponential distribution with $\\beta=0.5$ and $\\mu=0$, where\n",
    "\n",
    "the exponential pdf is defined as\n",
    "\n",
    "$$f(x)=\\frac{1}{\\beta} e^{-(x-\\mu)/\\beta}     ,  x \\ge \\mu;\\beta>0$$.\n",
    "\n",
    "Then from our sample we estimate the scale and location. When you do rerun the code several times you can see that the estimates $\\hat{\\beta}$ and $\\hat{\\mu}$ are sometimes quite far from the true values $\\beta=0.5$ and $\\mu=0$. This uncertainty comes into play by working on a sample. Having measurement errors or more complicated settings increases the uncertainty even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84470743-186a-49fd-a4e6-bd22a53a7e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eu7CsUFHZ-j",
    "outputId": "8c3ffdca-92fa-4203-9488-8871751ef145"
   },
   "outputs": [],
   "source": [
    "# Let us fit data to an exponential distribution\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "# First generate a data set from a exponential distribution\n",
    "x = stats.expon.rvs(0.0,0.5,size=50) #  scale = 0.5, location = 0.00, 100 variates\n",
    "ax.hist(x, density=True, histtype='stepfilled', alpha=0.3)\n",
    "# Fit scale and location to the histogram/data\n",
    "loc, scale = stats.expon.fit(x) # ML estimator scale, lambda * exp(-lambda * x), scale =1/lambda\n",
    "print(' Location = %1.2f , Scale = %1.2f' % (loc,scale))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f57fdc-027b-4723-b734-2d98915ca1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OGXrcmQnHZ-o"
   },
   "source": [
    "If you run this code serval times, you'll observe that sometimes the estimates for location and scale are rather poor. Here, we know the true underlying distribution but in general we don't. Thus, it would be helpful to know the uncertainties on the fitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a7d35d1-7ac0-4870-b8cf-7fb9ce6be464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hOVBrT0WdjrZ"
   },
   "source": [
    "## 3.2 Confidence intervall\n",
    "At the moment we have no clue on how certain we are about point estimates for the mean and standard deviation we calculated above. Hence, it makes sense to additionally look at the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fe3c9ae-f5b5-4851-8b73-00caadd5dd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MqZxZqfjhOs5"
   },
   "outputs": [],
   "source": [
    "#95%-t-confidence interval for the mean of St. Petersburg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09df2e9a-739b-4bad-8599-9aff1e01965e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LOczDpjAj4m1"
   },
   "source": [
    "Unfortunately, there is no Python library that computes the confidence interval of standard deviation, so we have to do it \"by hand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11d9721c-42c5-4d35-b195-96387442f918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "X6E1O5idl5Cu"
   },
   "outputs": [],
   "source": [
    "# confidence intervall for the standard deviation of normally distributed data\n",
    "alpha = 0.05  # significance level = 5%\n",
    "n = len(df['St_Petersburg'])  # sample sizes\n",
    "s2 = np.var(df['St_Petersburg'], ddof=1)   # sample variance\n",
    "degf = n - 1                 # degrees of freedom\n",
    "\n",
    "upper = np.sqrt((n - 1) * s2 / stats.chi2.ppf(alpha / 2, degf))\n",
    "lower = np.sqrt((n - 1) * s2 / stats.chi2.ppf(1 - alpha / 2, degf))\n",
    "(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4826d58-7e93-48d7-bd59-a8b73e5047f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yFHy-HgEnVTI"
   },
   "source": [
    "Interpret the confidence intervals!\n",
    "\n",
    "You can find more on confidence intervals under https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers#chi_square - I found this Link quite helpful :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97dd79ac-9fcd-465b-ab3c-922a3362768c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ldd4t9BUHZ9y"
   },
   "source": [
    "## 3.1 About linear Regression\n",
    "\n",
    "Simple linear regression means fitting a straight line to data a set of points $(x,y)$. A line is described as\n",
    "\n",
    "$$y = ax + b$$\n",
    "\n",
    "Thus, two parameters $a$ (slope) and $b$ (intersection with y axis) are fitted.\n",
    "\n",
    "There are different fitting methods, mostly least squares or maximum likelihood are used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff112854-83ad-4019-9e54-9903669e3e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tCRukbUyHZ9z"
   },
   "source": [
    "### Linear regression in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ee24660-07f5-4c20-8183-cb384d910072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vVN3HiA_HZ98"
   },
   "source": [
    "When we look at the available data from Riga and Bolderaja, we see that there is a lot less measurements from Bolderaja. It would, however, be nice if we could reconstruct the missing measurement from Bolderaja by using our observations from Riga. Before we go ahead, we should probably check whether this is reasonable. Let us thus compare the two line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0870797c-5297-4675-94f6-bd3d6c221309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aWEngLcnTz5W"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Riga\n",
    "plt.plot(df['YEAR'], df['Riga'], marker='o', label='Riga', color='blue')\n",
    "\n",
    "# Plot Bolderaja\n",
    "plt.plot(df['YEAR'], df['Bolderaja'], marker='o', label='Bolderaja', color='orange')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('DOY')\n",
    "plt.title('Riga and Bolderaja')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38882c72-72a6-4f1c-af1e-24bae81d03ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "P4npY1AWTpui"
   },
   "source": [
    "There seems to indeed exist a relationship between the DOY the ice melts in Riga and Bolderaja. Thus, let us now try to estimate the missing data by using a simple linear regression model.\n",
    "\n",
    "We use https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html, using least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee14524f-cbc2-4920-bb47-113f2538a93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2i1LDcA4TXoD"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Run an OLS Regression and look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170a9ccf-a491-4cb5-bd39-3cf948382323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "V3BFeEvyHZ-J"
   },
   "source": [
    "Under *coef* we can see the estimates for $a$ and $b$. *P>|t|* corresponds to the p-value for the test with working hypothesis that the respective estimate ($\\hat{a}, \\hat{b}$) not equals 0. *Prob (F-statistic)* is the p-value for the test with working hypothesis, that at least one explanatory variable has a significant effect on the outcome. *R-squared* describes how well the variance in the data is explained by the model (1 = everything, 0 = nothing). One Problem with *R-squared* is that its value increases with the number of explanatory variables. Thus, often it makes more sense to look at *Adj. R-squared* which is corrected for the number of explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a24e0530-d271-4653-aff9-4761f1ae3101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8qvhN0jLYWjw"
   },
   "outputs": [],
   "source": [
    "# Draw Scatterplot and add model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c6c75ef-63f0-450f-9ad5-16f2d0d574e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4-meHNvEZVUZ"
   },
   "source": [
    "Let us now look at the reconstructed time series in Bolderaja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad3f25c4-b342-4380-9a46-d7bf0fdcb1e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vsbFmYE1XuHf"
   },
   "outputs": [],
   "source": [
    "# Make predictions for Bolderaja and plot the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4d781e-9863-452c-b8dd-c131be577923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xIR3DN_aaqJi"
   },
   "source": [
    "Let us look more closely the area, where we know the true ice melt times in Bolderaja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ed0589-0c44-49be-b331-a139b8f6f4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HZ0g5OqRa0IB"
   },
   "outputs": [],
   "source": [
    "# Zoom in to plt.xlim(1840, 1882) where we also have the true observations of Bolderaja and add these to the plot as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6527c52d-ca16-4df8-bb2e-0ad41f498c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WyBk8akCbOeb"
   },
   "source": [
    "What do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "505e0d23-5f81-430e-af4d-aa9afb728986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "O7m4p7Uw_G7k"
   },
   "source": [
    "### Diagnostics\n",
    "Credits to https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/ for implementing the diagnostic plots! This reference also helps with interpretation of those plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53046e94-092d-41be-9da0-c649ec2ec9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NFT6hL7eit2F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "plt.style.use(\"seaborn-v0_8\") # pretty matplotlib plots\n",
    "\n",
    "def graph(formula, x_range, label=None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting cook's distance lines\n",
    "    \"\"\"\n",
    "    x = x_range\n",
    "    y = formula(x)\n",
    "    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n",
    "\n",
    "\n",
    "def diagnostic_plots(X, y, model_fit=None):\n",
    "  \"\"\"\n",
    "  Function to reproduce the 4 base plots of an OLS model in R.\n",
    "\n",
    "  ---\n",
    "  Inputs:\n",
    "\n",
    "  X: A numpy array or pandas dataframe of the features to use in building the linear regression model\n",
    "\n",
    "  y: A numpy array or pandas series/dataframe of the target variable of the linear regression model\n",
    "\n",
    "  model_fit [optional]: a statsmodel.api.OLS model after regressing y on X. If not provided, will be\n",
    "                        generated from X, y\n",
    "  \"\"\"\n",
    "\n",
    "  model_fit = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "\n",
    "  # create dataframe from X, y for easier plot handling\n",
    "  dataframe = pd.concat([X, y], axis=1)\n",
    "\n",
    "  # model values\n",
    "  model_fitted_y = model_fit.fittedvalues\n",
    "  # model residuals\n",
    "  model_residuals = model_fit.resid\n",
    "  # normalized residuals\n",
    "  model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n",
    "  # absolute squared normalized residuals\n",
    "  model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "  # absolute residuals\n",
    "  model_abs_resid = np.abs(model_residuals)\n",
    "  # leverage, from statsmodels internals\n",
    "  model_leverage = model_fit.get_influence().hat_matrix_diag\n",
    "  # cook's distance, from statsmodels internals\n",
    "  model_cooks = model_fit.get_influence().cooks_distance[0]\n",
    "  tmp = pd.concat([model_fitted_y, model_residuals], axis=1, keys=['fitted', 'residuals'])\n",
    "\n",
    "\n",
    "  plot_lm_1 = plt.figure()\n",
    "  plot_lm_1.axes[0] = sns.residplot(data=tmp, x='fitted', y='residuals',\n",
    "                            lowess=True,\n",
    "                            scatter_kws={'alpha': 0.5},\n",
    "                            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "  plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n",
    "  plot_lm_1.axes[0].set_xlabel('Fitted values')\n",
    "  plot_lm_1.axes[0].set_ylabel('Residuals');\n",
    "\n",
    "  # annotations\n",
    "  abs_resid = model_abs_resid.sort_values(ascending=False)\n",
    "\n",
    "  QQ = ProbPlot(model_norm_residuals)\n",
    "  plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n",
    "  plot_lm_2.axes[0].set_title('Normal Q-Q')\n",
    "  plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\n",
    "  plot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n",
    "  # annotations\n",
    "  abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\n",
    "\n",
    "  plot_lm_3 = plt.figure()\n",
    "  plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);\n",
    "  tmp2 = pd.DataFrame(model_norm_residuals_abs_sqrt, columns = ['model_norm_residuals_abs_sqrt'])\n",
    "  tmp3 = pd.concat([model_fitted_y, tmp2], axis=1, keys=['fitted', 'residuals_normed'])\n",
    "  sns.regplot(data=tmp3, x='fitted', y='residuals_normed',\n",
    "                scatter=False,\n",
    "                ci=False,\n",
    "                lowess=True,\n",
    "                line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "  plot_lm_3.axes[0].set_title('Scale-Location')\n",
    "  plot_lm_3.axes[0].set_xlabel('Fitted values')\n",
    "  plot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n",
    "\n",
    "  # annotations\n",
    "  abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\n",
    "\n",
    "\n",
    "  plot_lm_4 = plt.figure();\n",
    "  plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);\n",
    "  tmp4 = pd.DataFrame(model_leverage, columns = ['model_leverage'])\n",
    "  tmp5 = pd.DataFrame(model_norm_residuals, columns = ['model_norm_residuals'])\n",
    "  tmp6 = pd.concat([tmp4, tmp5], axis=1, keys=['leverage', 'residuals_normed'])\n",
    "  sns.regplot(data=tmp6, x='leverage', y='residuals_normed',\n",
    "             scatter=False,\n",
    "             ci=False,\n",
    "             lowess=True,\n",
    "              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "  plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)\n",
    "  plot_lm_4.axes[0].set_ylim(-3, 5)\n",
    "  plot_lm_4.axes[0].set_title('Residuals vs Leverage')\n",
    "  plot_lm_4.axes[0].set_xlabel('Leverage')\n",
    "  plot_lm_4.axes[0].set_ylabel('Standardized Residuals');\n",
    "\n",
    "  # annotations\n",
    "  leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n",
    "  for i in leverage_top_3:\n",
    "      plot_lm_4.axes[0].annotate(i,\n",
    "                                 xy=(model_leverage[i],\n",
    "                                     model_norm_residuals[i]));\n",
    "\n",
    "  p = len(model_fit.params) # number of model parameters\n",
    "  graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x),\n",
    "        np.linspace(0.001, max(model_leverage), 50),\n",
    "        'Cook\\'s distance') # 0.5 line\n",
    "  graph(lambda x: np.sqrt((1 * p * (1 - x)) / x),\n",
    "        np.linspace(0.001, max(model_leverage), 50)) # 1 line\n",
    "  plot_lm_4.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60534285-d3b8-4f0d-9431-e172d5882c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iePh2Q_wiycC"
   },
   "outputs": [],
   "source": [
    "# run diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b66341-dbb6-4ac8-b536-c08359438911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fegXQ3D5qTas"
   },
   "source": [
    "Look at the reference and think about the assumptions of the ols regression. Are they fulfilled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3da79c5-cf8f-4951-810c-e7a9e7f6716e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rzjGIJouHZ-p"
   },
   "source": [
    "## 3.2 More on regression\n",
    "### 3.2.1 Non-linear regression\n",
    "\n",
    "If a line is not straight it is curved. There are many mathematical functions whose parameters we can try to fit to experimental data points. Some examples: Polynominals (first order is linear regression, second order is a parabola etc), exponential functions, normal function, sindoial wave function etc. You need to choose an approriate shape/function to obtain a good result.\n",
    "\n",
    "With the Scipy.stat module we can look for preprogrammed functions (in principle you can program your own function whose parameters you want to fit too): https://docs.scipy.org/doc/scipy/reference/stats.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb7b7ec0-ac68-4a26-a8cd-aeb7e38a53b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "831QxcoRHZ-p"
   },
   "source": [
    "The scipy.optimize module provides a more general non-linear least squares fit. Look at and play with this example. It is complex and you will probably use some time to fully understand every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c57f797-640d-40ba-abec-a8473b53d95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1wsB4y-wHZ-q"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func(x, a, b, c):\n",
    "     return a * np.exp(-b * x) + c\n",
    "\n",
    "xdata = np.linspace(0, 4, 50) #\n",
    "y = func(xdata, 2.5, 1.3, 0.5)\n",
    "plt.plot(xdata, y, 'g-', label='Generated data')\n",
    "np.random.seed(1729)\n",
    "y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
    "ydata = y + y_noise\n",
    "plt.plot(xdata, ydata, 'bo', label='Generated data with noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd0288de-4517-4fed-8ac5-9e3bd2992c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EssTbnMBvvI7"
   },
   "source": [
    "Let's fit the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fef5cb7-7ca8-45d4-88a3-82906688f7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FizQcBT1HZ-v"
   },
   "outputs": [],
   "source": [
    "popt, pcov = curve_fit(func, xdata, ydata)\n",
    "print(popt)\n",
    "perr = np.sqrt(np.diag(pcov)) # Standard deviation = square root of the variance being on the diagonal of the covariance matrix\n",
    "plt.plot(xdata, func(xdata, *popt), 'r-',label= \\\n",
    "         'fit: a=%5.3f +- %5.3f, \\n b=%5.3f +- %5.3f, \\n c=%5.3f +-%5.3f' % \\\n",
    "         (popt[0],perr[0],popt[1],perr[1],popt[2],perr[2]))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# pcov is the covariance matrix. The errors**2 of each parameter are the values on the diagonal.\n",
    "perr = np.sqrt(np.diag(pcov)) # Standard deviation = square root of the variance being on the diagonal of the covariance matrix\n",
    "perr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac15b856-efe5-4afb-8fde-cfe2551780d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9e_ZppEZvzi0"
   },
   "source": [
    "Compare it to the data and the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b61f698-a0c1-4289-8013-cf4d8c1ece1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aOuM_jywv6Vu"
   },
   "outputs": [],
   "source": [
    "plt.plot(xdata, y, 'g-', label='true')\n",
    "plt.plot(xdata, func(xdata, *popt), 'r-',label= 'fit')\n",
    "plt.plot(xdata, ydata, 'bo', label='Generated data with noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26c9dd5-fdab-42da-8f49-455b1c610fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sfYUc2gayS7n"
   },
   "source": [
    "Try to add a bit noise and rerun the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec95a3eb-21d2-448e-8be0-461ef472c8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "N399vCV4HZ-z"
   },
   "source": [
    "### 3.2.2 Non-parametric regression\n",
    "\n",
    "So far we have used functions (models) with some predefined model. The parameters we fitted to data. If we have no clue about the model, but a rought idea on the shape non-parametric methods like isotonic regression. However, these require more data as also the exact shape needs to guessed or fitted from the data. So normally a non-parametric method gives poorer results.\n",
    "\n",
    "There are several ways to do this in Python. You make look at this if you are interested:\n",
    "\n",
    "https://pythonhosted.org/PyQt-Fit/NonParam_tut.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e72ac27-c213-4681-86dd-7b3abaa0f79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uxKmRNrRHZ-0"
   },
   "source": [
    "### 3.2.3 Fitting with scikit-learn\n",
    "\n",
    "When it comes to machine learning, the `scikit-learn` module is much richer than the stats module. You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface.\n",
    "\n",
    "All the different ML approaches are implemented as classes with a set of same main methods:\n",
    "\n",
    "1. `fitter = ...`: Create object.\n",
    "2. `fitter.fit(x, y[, sample_weight])`: Fit model.\n",
    "3. `y_pred = fitter.predict(X)`: Predict using the model.\n",
    "4. `s = fitter.score(x, y[, sample_weight])`: Return an appropriate measure of model performance.\n",
    "\n",
    "This allows us to easily replace one approach with another and find the best one for the problem at hand, by simply using another regression/classification object, while the rest of the code can remain the same.\n",
    "\n",
    "*But you should be careful that there remains some justification! Otherwise you'll overfit. To be sure, you should use an approach like cross validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d2aace-8bd4-4e97-952f-b893fb139199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ezmW4pb2HZ-1"
   },
   "outputs": [],
   "source": [
    "# Let's write a method which can generate a linear dataset\n",
    "# with n_d dimensions and some gaussian noise to it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
    "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
    "\n",
    "  w = w or np.random.uniform(0.1, 10, n_d)\n",
    "  b = b or np.random.uniform(-10, 10)\n",
    "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
    "\n",
    "  print('true w =', w, ';  b =', b)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc33990-f594-415e-9eb4-2c30414bb0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GlsmKqA_HZ-7"
   },
   "outputs": [],
   "source": [
    "# Sample data from a straight line with noise to it\n",
    "x, y = get_linear(n_d=1, n_points=10)\n",
    "plt.plot(x[:, 0], y, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98aaf55-2cc9-4262-a68f-b2da02b42e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NbGHUG25Jr_n"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f12e3f-6425-4005-9c88-b3fec3164fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "94oQqhyTKHlX"
   },
   "outputs": [],
   "source": [
    "w, w0 = reg.coef_, reg.intercept_\n",
    "print(w, w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "483534a4-b148-48eb-8d7c-972228ad6651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QSNozJUJTSFa"
   },
   "source": [
    "scikit-learn does not support calculation of covariance on fitted parameters. That is a pity maybe. However, we can use numpy vectorized methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a23a3c-f331-403b-98bd-3295e66d9391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "rJIV5pKmUPSz"
   },
   "outputs": [],
   "source": [
    "np.std(y - reg.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bd896bf-8660-41a1-866b-10d5f34e4419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "RF6OAUOkUQ9b"
   },
   "outputs": [],
   "source": [
    "(np.cov(y - reg.predict(x)))**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda1538a-f1b7-4711-9aea-d328e6dcefb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QmYtZZnhM_ZB"
   },
   "source": [
    "What about ```.score()```?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7488513-eeb5-4b42-b951-a18d8e208131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "55dZuqcvM-49"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# For regression the .score() yields the coefficient of determination.\n",
    "print('Score: %.2f'\n",
    "      % reg.score(x, y))\n",
    "# Indeed, when calculating r2 \"by hand\" we end up with the same result.\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y, reg.predict(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a998f102-0cf3-489b-a281-cdb50765a819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "N7ebNwFpNhJz"
   },
   "source": [
    "What the default of ```.score()``` is can be found in each estimator’s documentation. If you like to choose your scoring function yourself, you can find the implemented versions under this link https://scikit-learn.org/stable/modules/model_evaluation.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0201987-acf0-4c18-ab5b-9841d17bbbef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "tzWRQdCATrl9"
   },
   "outputs": [],
   "source": [
    "# Plot the fitted model with plus minus one standard deviation.\n",
    "# Are the data points distributed as expected?\n",
    "plt.scatter(x, y, marker='*')\n",
    "x_f = np.linspace(x.min(), x.max(), 10)\n",
    "y_f = w0 + w[0] * x_f\n",
    "plt.plot(x_f, y_f)\n",
    "plt.plot(x_f, y_f+5.5)\n",
    "plt.plot(x_f, y_f-5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b16253b6-11c6-429d-a779-5d36728a61d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "JddYy37lHZ-_"
   },
   "outputs": [],
   "source": [
    "# Generate data points from a plane with some noise to it (default)\n",
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=100)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21710baa-71b3-4ade-96e0-1e213ef8e435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "VinHbzEjIBFm"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58c939c1-470a-44c0-b508-7b7ca79d01af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "pdeRD7-ZTf0A"
   },
   "outputs": [],
   "source": [
    "# When doing ML, it is recommended to split the dataset into a training\n",
    "# and a test set. The training set is used to learn the parameters. The\n",
    "# test set can be used to assess the performance of the trained model\n",
    "# (overfitting and underfitting). sklearn provides a helpful method doing that.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=100, sigma=5)\n",
    "\n",
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', color='b',s=40)\n",
    "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', color='r',s=80)\n",
    "\n",
    "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
    "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
    "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
    "xx = np.stack((xx0, xx1), axis=-1)\n",
    "yy = reg.predict(xx)\n",
    "ax.plot_trisurf(xx0, xx1, yy, alpha=0.5, color='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75b1a921-a204-46b9-a5d0-f89d33e85f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "3xw69ZnYY7R2"
   },
   "outputs": [],
   "source": [
    "print('Score: %.2f'\n",
    "      % reg.score(x, y))\n",
    "print('Standard deviation of error: %.2f'\n",
    "      % np.std(y-reg.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb848709-393a-454b-ba1a-dfd4790faaf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f2sQVIJP0r9B"
   },
   "source": [
    "### 3.2.4 - Much used ML performance measures\n",
    "\n",
    "1. Regression:\n",
    "* Mean Square Error (MSE): $mse=\\frac{1}{n}\\sum_i(y_i - \\hat y(\\bar x_i))^2$\n",
    "* Mean Absolute Error (MAE): $mae=\\frac{1}{n}\\sum_i|y_i - \\hat y(\\bar x_i)|$\n",
    "* Median Absolute Deviation (MAD): $mad=median(|y_i - \\hat y(\\bar x_i)|)$\n",
    "* Fraction of the explained variance: $R^2=1-\\frac{\\sum_i(y_i - \\hat y(\\bar x_i))^2}{\\sum_i(y_i - \\bar y_i)^2}$, where $\\bar y=\\frac{1}{n}\\sum_i y_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d17034-7894-46f8-a015-88da16d102f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7E9smfEP1ork"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827b79db-8e26-4da1-9b66-143a51f835f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "U1J7txZf1vCg"
   },
   "source": [
    "### 3.2.5 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff581f9-2636-4ed3-9276-6abdc03816dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "ljndBAw8TAml"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/krishnaik06/Multiple-Linear-Regression/master/50_Startups.csv\"\n",
    "ds = pd.read_csv(url)\n",
    "ds.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933cc9c8-8dd1-4742-8828-8cde017b49e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "2ajvIE33VMPr"
   },
   "outputs": [],
   "source": [
    "#Convert the 'States' into categorical columns\n",
    "states=pd.get_dummies(ds['State'],drop_first=True)\n",
    "\n",
    "# Convert boolean dummy variables to integers\n",
    "states = states.astype(int)\n",
    "\n",
    "# X explanatory variables, Y response\n",
    "X = ds[['R&D Spend','Administration','Marketing Spend']]\n",
    "X = pd.concat([X,states],axis=1)\n",
    "Y = ds['Profit']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Add intercept\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Create and fit the OLS model on training data\n",
    "model = sm.OLS(Y_train, X_train)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f397fa7-d2a7-4dfd-a265-3d4ce9afe1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0WeBubTSZXkH"
   },
   "source": [
    "Evaluate other scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0751501e-7b8b-47c7-ba7a-d669d6ff7e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "7_R4r2gIZWfn"
   },
   "outputs": [],
   "source": [
    "#evaluate MSE, MAD, and R2 on train and test datasets\n",
    "#prediction in sample:\n",
    "y_p_train = results.predict(X_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "X_test = sm.add_constant(X_test)\n",
    "y_p_test = results.predict(X_test)\n",
    "\n",
    "# mse\n",
    "print('train mse =', np.std(Y_train - y_p_train))\n",
    "print('test mse =', np.std(Y_test - y_p_test))\n",
    "# mae\n",
    "print('train mae =', np.mean(np.abs(Y_train - y_p_train)))\n",
    "print('test mae =', np.mean(np.abs(Y_test - y_p_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3461623d-600c-4a00-92ae-118f3d34b869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "gRpdrj-7aDSQ"
   },
   "outputs": [],
   "source": [
    "# plot y vs predicted y for test and train parts\n",
    "plt.plot(Y_train, y_p_train, 'b.', label='train')\n",
    "plt.plot(Y_test, y_p_test, 'r.', label='test')\n",
    "\n",
    "plt.plot([0], [0], 'w.')  # dummy to have origin\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b91de54-9094-4185-89a6-d49e3170a3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UAFWFkR4gaAf"
   },
   "source": [
    "## Questions\n",
    "\n",
    "We have now seen how to calculate confidence intervals and do linear regression with several python modules. The most popular ML models are implemented in sklearn. We don't have time practicing them in this Module. In Module 3 we will learn and practice deep neural networks.\n",
    "\n",
    "Please post a question about today's content here:\n",
    "https://forms.gle/ZY4pbMiEu1iYFYbp7\n",
    "\n",
    "Many thanks and see you tomorrow!\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "CAS-D2-Regression",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "N399vCV4HZ-z"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/KGzB/CAS-Applied-Data-Science/blob/master/Module-2/CAS-D2-Regression.ipynb",
     "timestamp": 1724773283337
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
