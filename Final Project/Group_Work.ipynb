{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919d6a04-380d-4219-a636-d1ed27effbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Acquisition (ETL) - test\n",
    "\n",
    "**Starting Point:**  \n",
    "The article data originates from a Kafka datastream. It is not normalized (so it cannot be analyzed directly) and requires Active Directory login (making collaboration difficult).  \n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "- **Processing steps:**  \n",
    "  1. Read article data from the Delta table populated from Kafka.\n",
    "  2. Flatten and transform nested fields (e.g., titles, resources, contributors) using a SQL view.\n",
    "  3. Create a Spark DataFrame from the flattened view and inspect the results.\n",
    "  4. Write the DataFrame to a Delta table for analytics and automation.\n",
    "  5. Export a <25MB Parquet sample with only public data for sharing (e.g., via GitHub).\n",
    "\n",
    "**Goal:**  \n",
    "The data should be available as a Parquet file for sharing. Since the dataset is large (5GB), only a public sample is exported for easy distribution.\n",
    "\n",
    "**Access Control:**  \n",
    "To guarantee data integrity and protect sensitive information, data distribution is based on user access rights. Entitled users can access the full confidential dataset, while restricted users are provided with only the public sample. This ensures that only authorized users can view sensitive data, maintaining compliance and data security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35abc26-aae0-4cc6-b0b5-4ff0d3bca2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-01: Read from Kafka (SQL)\n",
    "\n",
    "The following steps read article data from the Delta table `udp_prd_atomic.pdp.articles_v2`, which is assumed to be populated from a Kafka stream. The original Kafka data contains nested lists and complex structures (e.g., for multilingual fields or arrays of resources). In the transformation, the SQL view `articles_flat` flattens this nested data by extracting relevant fields and, where multiple values exist (such as for titles in different languages), selects the first available entryâ€”typically prioritizing German (`'de'`) or otherwise the first value. This process prepares the data, along with Kafka metadata (key, topic, partition, offset, timestamp), for further analysis in a flat, tabular format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf8dd3b-2ede-4d29-8400-9538c6d279b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-02: Create DataFrame and Visually Inspect Results\n",
    "\n",
    "The code below runs a Spark SQL query against the temporary view `articles_flat`, loads the result into a Spark DataFrame named `df`, and then displays the DataFrame for visual inspection. This step materializes the flattened article data so it can be further processed or written to a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e96a63a-ade2-4735-a697-bd4f3e60ea82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-03: Save Data to Delta Table\n",
    "\n",
    "In the next steps, the data will be saved both to a Delta table (for better automation and analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aab4d4-2ceb-40e9-baa2-e40589325f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to (private) Delta Table - all articles\n",
    "\n",
    "The following code appends the transformed DataFrame `df` to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`. It writes in **append** mode, uses the **Delta** format, and enables **schema merging** so that any new columns are automatically added to the target table without overwriting existing data.\n",
    "\n",
    "- Contains all articles (**confidential**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6e6098-965f-4ad5-b8aa-79264468c1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Write to (public) Parquet File - selected articles, manually Upload to GitHub\n",
    "\n",
    "Export a <25 MB sample of the data with only public data as a Parquet file for easy sharing via GitHub.  \n",
    "**Note:** The Parquet file must be manually downloaded from Databricks and then uploaded to your GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5409a74-7842-49e5-9a43-56626ee9ea3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...now manually:\n",
    "\n",
    "1. **Open the CSV file in Databricks:**\n",
    "   - Navigate to [Databricks Volume Browser](https://adb-4119964566130471.11.azuredatabricks.net/explore/data/volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume?o=4119964566130471) in the Databricks workspace file browser.\n",
    "\n",
    "2. **Download the file:**\n",
    "   - Right-click on `export_articles_v2_sample25mb.parquet` and select **\"Download\"** to save the file to your local machine.\n",
    "\n",
    "3. **Upload the file to GitHub:**\n",
    "   - Go to [GitHub Folder](https://github.com/Tao-Pi/CAS-Applied-Data-Science/tree/main/Module-3/01_Module%20Final%20Assignment).\n",
    "   - Click **\"Add file\"** > **\"Upload files\"**.\n",
    "   - Drag and drop `export_articles_v2_sample25mb.parquet` or use the file picker to select it.\n",
    "   - Commit the changes to upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca686f8-2f86-434b-9576-c57c9d1b8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step-04: Load Data Based on User Rights\n",
    "\n",
    "The next step is to load the data, with access determined by user rights:\n",
    "\n",
    "- **Restricted users** can load only the public data sample (e.g., the Parquet file exported for sharing).\n",
    "- **Entitled users** can load the full, confidential dataset from the Delta table.\n",
    "\n",
    "This ensures that sensitive information is only accessible to authorized users, while still allowing broader access to public data for collaboration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31d9e99-1110-4cc3-904c-8e3b616f14dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install fastparquet\n",
    "#%pip install -U sentence-transformers torch safetensors accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0706526-3796-466a-99d7-7ab6b3ff5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c9ed1a-0c3c-4aaf-9e1f-057c42cc38d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "has_read_access_udp_articles_v2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d121c8-4b18-4d6e-827d-7d9b240da360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset Overview\n",
    "\n",
    "In this chapter, we provide a brief overview of the dataset used for analysis. We indicate whether the loaded dataset is the full confidential version or the public sample, report the total number of articles available, and present a first look at the articles data to understand its structure and content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0836948-a298-4a9d-b408-a8f3fd8c97f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Check Dataset Version (Confidential vs Public)\n",
    "\n",
    "Here we check and indicate whether the loaded dataset is the full confidential version or the public sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60a1ebc-6a6d-4e81-bfe5-d956f13f8e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "        return f\"more than {n // 1_000_000} Mio.\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} Tsd.\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the full data set. This contains the full corpus of {format_rowcount(rowcount)} Articles published by SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the publically available (sampled) data set. This contains an excerpt of {format_rowcount(rowcount)} articles within SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7446805d-88cc-41ba-a63e-8b91905fab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Overview of the Data\n",
    "\n",
    "In this step, we provide an overview of the data contained in the loaded dataset. This includes a summary of the available articles and a first look at their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41c756-4f7a-4617-bf25-bf39bed8a164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Falls DataFrame leer ist â†’ leeres dict\n",
    "first_row = srgssr_article_corpus.iloc[0].to_dict() if not srgssr_article_corpus.empty else {}\n",
    "\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": col,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(col, None)\n",
    "    }\n",
    "    for col, dtype in srgssr_article_corpus.dtypes.items()\n",
    "]\n",
    "\n",
    "# SchÃ¶n anzeigen\n",
    "import pandas as pd\n",
    "pd.DataFrame(cols_info).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c53db45-2261-4f45-91ca-1196a86775ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: A Closer Look\n",
    "\n",
    "In this step, we take a deeper look at the loaded dataset, exploring its structure and content in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bfcf1f-6b0c-4056-b427-7f336aad844e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bc94dd-bc0a-4798-b51d-68d805ad6b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Analyses\n",
    "This is where analyses are performed. This is work in progress. Some ideas:\n",
    "\n",
    "**Story 1:** I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Story 2:** I want to find out what topics SRG writes about â€“ this could, for example, be used for navigation (News / Sport / etc.).\n",
    "\n",
    "**Story 3:** I want to translate all existing articles into all languages used. This way, I can multiply the offer easily. Instead of having some articles in French and some in English, I will have all articles available in all of our 11 languages.\n",
    "\n",
    "*List other ideas here...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fab85f79-57a4-4286-8fbe-1d7cf24b9453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srgssr_article_corpus = srgssr_article_corpus.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43080368-8370-42dd-8c45-da9b3b2684fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## USE CASE: Quickly Search All Existing Articles\n",
    "\n",
    "I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Approach:**\n",
    "- Implement a semantic search feature within Databricks that allows users to search articles by keywords, phrases, or topics.\n",
    "- Use text embeddings (e.g., with Sentence Transformers) to represent article content and enable similarity-based search.\n",
    "- Provide a simple search interface where users can enter queries and retrieve the most relevant articles.\n",
    "- Optionally, add filters for date, author, or branch to refine search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1703498d-08b9-4d99-b520-5229ab923a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c554b5-f293-4ec9-a15e-ba2d632e000b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TEXT_COL = \"content_text_csv\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "df = srgssr_article_corpus.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "model = get_embedder()\n",
    "emb_matrix = model.encode(\n",
    "    df[TEXT_COL].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "ids = df[ID_COL].tolist()\n",
    "texts = df[TEXT_COL].tolist()\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = emb_matrix @ q\n",
    "    top_idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i] for i in top_idx],\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n",
    "# Example usage:\n",
    "results = semantic_search(\"climate change\", top_k=10)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45aadfc-30df-4850-8189-86807f91fc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If needed (run once per environment):\n",
    "# %pip install -U sentence-transformers torch safetensors accelerate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----- 1) Prepare data (pandas) -----\n",
    "TEXT_COL = \"content_text_csv\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "df = srgssr_article_corpus.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# (Optional) downsample for quick prototyping\n",
    "# df = df.head(1000)\n",
    "\n",
    "# ----- 2) Load the embedder (cached) -----\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "# ----- 3) Compute corpus embeddings (NumPy) -----\n",
    "model = get_embedder()\n",
    "# normalize_embeddings=True gives unit vectors â†’ cosine = dot product\n",
    "emb_matrix = model.encode(\n",
    "    df[TEXT_COL].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# Keep references for lookup\n",
    "ids = df[ID_COL].tolist()\n",
    "texts = df[TEXT_COL].tolist()\n",
    "\n",
    "# ----- 4) Semantic search (cosine similarity) -----\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]  # shape (d,)\n",
    "    sims = emb_matrix @ q  # cosine similarity because both are normalized\n",
    "    # Argpartition for speed, then sort exact top_k\n",
    "    top_idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i] for i in top_idx],\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n",
    "# ----- 5) Example -----\n",
    "results = semantic_search(\"climate change\", top_k=10)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ece778-f9c3-469f-b11d-cab0530ec474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##USE CASE: find out what topics SRG writes about.\n",
    "**Approach:**  \n",
    "- Read the text from the `content_text_csv` column of the articles.\n",
    "- Compute similarity between article contents, e.g., by embedding the texts and using a Random Forest or other clustering/classification methods to group similar articles.\n",
    "- Identify clusters of similar content to reveal common topics or themes.\n",
    "- Use these clusters to enhance navigation and filtering options for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8f933a-3713-4069-b863-3937476d9563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For quick prototyping, we sample only 1000 articles here.\n",
    "# To run on the full dataset, remove the .limit(1000) line below.\n",
    "df_sample = srgssr_article_corpus.limit(1000)\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Load model once per executor for efficiency\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embedder():\n",
    "    if not hasattr(get_embedder, \"model\"):\n",
    "        get_embedder.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return get_embedder.model\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def embed_udf(texts: pd.Series) -> pd.Series:\n",
    "    model = get_embedder()\n",
    "    return pd.Series(model.encode(texts.tolist(), show_progress_bar=False, batch_size=64).tolist())\n",
    "\n",
    "# Add embeddings column (no translation)\n",
    "df_emb = df_sample.withColumn(\n",
    "    \"content_emb\", embed_udf(col(\"content_text_csv\"))\n",
    ")\n",
    "\n",
    "# Convert array to Spark Vector (required by ML algorithms)\n",
    "to_vector = pandas_udf(lambda arr: [Vectors.dense(x) if x else Vectors.dense([]) for x in arr], returnType=VectorUDT())\n",
    "df_vec = df_emb.withColumn(\"features\", to_vector(col(\"content_emb\")))\n",
    "\n",
    "# KMeans clustering (choose number of clusters, e.g., 10)\n",
    "k = 10\n",
    "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=k, seed=42)\n",
    "model = kmeans.fit(df_vec)\n",
    "\n",
    "# Assign cluster IDs\n",
    "df_clustered = model.transform(df_vec).select(\"id\", \"cluster\", \"content_text_csv\")\n",
    "\n",
    "# Show a sample of clustered articles\n",
    "display(df_clustered.orderBy(\"cluster\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fa3228-64a0-48c5-a251-143717b3555d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## USE CASE: Translate All Existing Articles into All Languages Used\n",
    "\n",
    "Goal: Automatically translate every article into all supported languages, so each article is available in every language used by SRG.\n",
    "\n",
    "**Approach:**\n",
    "- For each article, use the `ai_translate` function to generate translations for all target languages.\n",
    "- Store the translated articles alongside the originals for easy access and analytics.\n",
    "\n",
    "**Example (SQL):**\n",
    "sql\n",
    "SELECT\n",
    "  *,\n",
    "  ai_translate(title, 'fr') AS title_fr,\n",
    "  ai_translate(title, 'it') AS title_it,\n",
    "  ai_translate(title, 'en') AS title_en,\n",
    "  ai_translate(title, 'rm') AS title_rm\n",
    "FROM articles_flat\n",
    "\n",
    "*(Repeat for all relevant text fields and all required languages.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50182740-9fdf-4f7c-94bb-ffacb465ffdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume ai_translate(text: str, target_lang: str) -> str is defined and available\n",
    "\n",
    "from pyspark.sql.functions import col, struct, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "target_languages = [\"en\", \"fr\", \"it\", \"de\"]  # Example target languages\n",
    "\n",
    "def make_translate_udf(lang):\n",
    "    @udf(StringType())\n",
    "    def translate_udf(text):\n",
    "        return ai_translate(text, lang)\n",
    "    return translate_udf\n",
    "\n",
    "df = srgssr_article_corpus\n",
    "\n",
    "for lang in target_languages:\n",
    "    df = df.withColumn(f\"content_text_csv_{lang}\", make_translate_udf(lang)(col(\"content_text_csv\")))\n",
    "\n",
    "# Store the DataFrame with translations alongside originals\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"swi_audience_prd.pdp_articles_v2.articles_v2_translated\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68fcc40-d6f9-4967-a565-2cf481d3de78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Falls noch nicht installiert+neu gestartet:\n",
    "# %pip install -U transformers sentencepiece torch safetensors accelerate\n",
    "# dbutils.library.restartPython()\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 20 Zeilen holen (nur nÃ¶tige Spalten)\n",
    "pdf = (\n",
    "    srgssr_article_corpus\n",
    "    .select(\"id\", \"content_text_csv\")\n",
    "    .limit(20)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# 2) Pipeline lokal laden (CPU)\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", device=-1)\n",
    "\n",
    "# 3) Ãœbersetzen (Batch pro Text-Chunk â€“ hier simpel ohne Chunking)\n",
    "def translate_text(t):\n",
    "    if t is None or not str(t).strip():\n",
    "        return None\n",
    "    res = translator(t, truncation=True, max_length=1000)\n",
    "    return res[0][\"translation_text\"]\n",
    "\n",
    "pdf[\"content_text_csv_english\"] = [translate_text(t) for t in pdf[\"content_text_csv\"]]\n",
    "\n",
    "# 4) ZurÃ¼ck nach Spark (nur fÃ¼r Anzeige)\n",
    "df_test = spark.createDataFrame(pdf)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0b0b3a-d8b3-4922-a5a6-f77f8e37dd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# ðŸ§ª Nur 20 Zeilen nehmen\n",
    "df_sample = srgssr_article_corpus.limit(20)\n",
    "\n",
    "# ðŸ”„ Ãœbersetzungsspalte hinzufÃ¼gen\n",
    "df_test = df_sample.withColumn(\n",
    "    \"content_text_csv_english\",\n",
    "    translate_series_to_en(F.col(\"content_text_csv\"))\n",
    ")\n",
    "\n",
    "# ðŸ‘€ Ergebnis anzeigen\n",
    "df_test.select(\"id\", \"content_text_csv\", \"content_text_csv_english\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b220d3-50ac-4a2d-b7c6-70eed00e4561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Hints and Notes\n",
    "> **Hint:**  \n",
    "> For a temporary Jupyter environment to experiment or explore data, consider using [RenkuLab](https://renkulab.io/p/snsf-anoxia-project/proxy-proxy/sessions/01JX2TG1RZ9J0PQ53H3RT81BD4/start). RenkuLab offers cloud-based notebook sessionsâ€”no local setup required.\n",
    "\n",
    "> **Note:**  \n",
    "> RenkuLab sessions may require authentication and have limited resources. Save your work frequently, as sessions can time out or be terminated after inactivity."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4689788114568825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Group_Work",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
