{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919d6a04-380d-4219-a636-d1ed27effbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Acquisition (ETL) - test\n",
    "\n",
    "**Starting Point:**  \n",
    "The article data originates from a Kafka datastream. It is not normalized (so it cannot be analyzed directly) and requires Active Directory login (making collaboration difficult).  \n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "- **Processing steps:**  \n",
    "  1. Read article data from the Delta table populated from Kafka.\n",
    "  2. Flatten and transform nested fields (e.g., titles, resources, contributors) using a SQL view.\n",
    "  3. Create a Spark DataFrame from the flattened view and inspect the results.\n",
    "  4. Write the DataFrame to a Delta table for analytics and automation.\n",
    "  5. Export a <25MB Parquet sample with only public data for sharing (e.g., via GitHub).\n",
    "\n",
    "**Goal:**  \n",
    "The data should be available as a Parquet file for sharing. Since the dataset is large (5GB), only a public sample is exported for easy distribution.\n",
    "\n",
    "**Access Control:**  \n",
    "To guarantee data integrity and protect sensitive information, data distribution is based on user access rights. Entitled users can access the full confidential dataset, while restricted users are provided with only the public sample. This ensures that only authorized users can view sensitive data, maintaining compliance and data security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35abc26-aae0-4cc6-b0b5-4ff0d3bca2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-01: Read from Kafka (SQL)\n",
    "\n",
    "The following steps read article data from the Delta table `udp_prd_atomic.pdp.articles_v2`, which is assumed to be populated from a Kafka stream. The original Kafka data contains nested lists and complex structures (e.g., for multilingual fields or arrays of resources). In the transformation, the SQL view `articles_flat` flattens this nested data by extracting relevant fields and, where multiple values exist (such as for titles in different languages), selects the first available entryâ€”typically prioritizing German (`'de'`) or otherwise the first value. This process prepares the data, along with Kafka metadata (key, topic, partition, offset, timestamp), for further analysis in a flat, tabular format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf8dd3b-2ede-4d29-8400-9538c6d279b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-02: Create DataFrame and Visually Inspect Results\n",
    "\n",
    "The code below runs a Spark SQL query against the temporary view `articles_flat`, loads the result into a Spark DataFrame named `df`, and then displays the DataFrame for visual inspection. This step materializes the flattened article data so it can be further processed or written to a Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e96a63a-ade2-4735-a697-bd4f3e60ea82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-03: Save Data to Delta Table\n",
    "\n",
    "In the next steps, the data will be saved both to a Delta table (for better automation and analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aab4d4-2ceb-40e9-baa2-e40589325f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to (private) Delta Table - all articles\n",
    "\n",
    "The following code appends the transformed DataFrame `df` to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`. It writes in **append** mode, uses the **Delta** format, and enables **schema merging** so that any new columns are automatically added to the target table without overwriting existing data.\n",
    "\n",
    "- Contains all articles (**confidential**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6e6098-965f-4ad5-b8aa-79264468c1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Write to (public) Parquet File - selected articles, manually Upload to GitHub\n",
    "\n",
    "Export a <25 MB sample of the data with only public data as a Parquet file for easy sharing via GitHub.  \n",
    "**Note:** The Parquet file must be manually downloaded from Databricks and then uploaded to your GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5409a74-7842-49e5-9a43-56626ee9ea3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...now manually:\n",
    "\n",
    "1. **Open the CSV file in Databricks:**\n",
    "   - Navigate to [Databricks Volume Browser](https://adb-4119964566130471.11.azuredatabricks.net/explore/data/volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume?o=4119964566130471) in the Databricks workspace file browser.\n",
    "\n",
    "2. **Download the file:**\n",
    "   - Right-click on `export_articles_v2_sample25mb.parquet` and select **\"Download\"** to save the file to your local machine.\n",
    "\n",
    "3. **Upload the file to GitHub:**\n",
    "   - Go to [GitHub Folder](https://github.com/Tao-Pi/CAS-Applied-Data-Science/tree/main/Module-3/01_Module%20Final%20Assignment).\n",
    "   - Click **\"Add file\"** > **\"Upload files\"**.\n",
    "   - Drag and drop `export_articles_v2_sample25mb.parquet` or use the file picker to select it.\n",
    "   - Commit the changes to upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca686f8-2f86-434b-9576-c57c9d1b8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step-04: Load Data Based on User Rights\n",
    "\n",
    "The next step is to load the data, with access determined by user rights:\n",
    "\n",
    "- **Restricted users** can load only the public data sample (e.g., the Parquet file exported for sharing).\n",
    "- **Entitled users** can load the full, confidential dataset from the Delta table.\n",
    "\n",
    "This ensures that sensitive information is only accessible to authorized users, while still allowing broader access to public data for collaboration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c31d9e99-1110-4cc3-904c-8e3b616f14dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install fastparquet\n",
    "#%pip install -U sentence-transformers torch safetensors accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0706526-3796-466a-99d7-7ab6b3ff5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c9ed1a-0c3c-4aaf-9e1f-057c42cc38d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "has_read_access_udp_articles_v2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d121c8-4b18-4d6e-827d-7d9b240da360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset Overview\n",
    "\n",
    "In this chapter, we provide a brief overview of the dataset used for analysis. We indicate whether the loaded dataset is the full confidential version or the public sample, report the total number of articles available, and present a first look at the articles data to understand its structure and content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0836948-a298-4a9d-b408-a8f3fd8c97f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Check Dataset Version (Confidential vs Public)\n",
    "\n",
    "Here we check and indicate whether the loaded dataset is the full confidential version or the public sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60a1ebc-6a6d-4e81-bfe5-d956f13f8e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "        return f\"more than {n // 1_000_000} Mio.\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} Tsd.\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the full data set. This contains the full corpus of {format_rowcount(rowcount)} Articles published by SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the publically available (sampled) data set. This contains an excerpt of {format_rowcount(rowcount)} articles within SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7446805d-88cc-41ba-a63e-8b91905fab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Overview of the Data\n",
    "\n",
    "In this step, we provide an overview of the data contained in the loaded dataset. This includes a summary of the available articles and a first look at their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41c756-4f7a-4617-bf25-bf39bed8a164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Falls DataFrame leer ist â†’ leeres dict\n",
    "first_row = srgssr_article_corpus.iloc[0].to_dict() if not srgssr_article_corpus.empty else {}\n",
    "\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": col,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(col, None)\n",
    "    }\n",
    "    for col, dtype in srgssr_article_corpus.dtypes.items()\n",
    "]\n",
    "\n",
    "# SchÃ¶n anzeigen\n",
    "import pandas as pd\n",
    "pd.DataFrame(cols_info).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c53db45-2261-4f45-91ca-1196a86775ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: A Closer Look\n",
    "\n",
    "In this step, we take a deeper look at the loaded dataset, exploring its structure and content in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bfcf1f-6b0c-4056-b427-7f336aad844e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13bc94dd-bc0a-4798-b51d-68d805ad6b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Analyses\n",
    "This is where analyses are performed. This is work in progress. Some ideas:\n",
    "\n",
    "**Story 1:** I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Story 2:** I want to find out what topics SRG writes about â€“ this could, for example, be used for navigation (News / Sport / etc.).\n",
    "\n",
    "**Story 3:** I want to translate all existing articles into all languages used. This way, I can multiply the offer easily. Instead of having some articles in French and some in English, I will have all articles available in all of our 11 languages.\n",
    "\n",
    "*List other ideas here...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fab85f79-57a4-4286-8fbe-1d7cf24b9453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srgssr_article_corpus = srgssr_article_corpus.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43080368-8370-42dd-8c45-da9b3b2684fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## USE CASE: Quickly Search All Existing Articles\n",
    "\n",
    "I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Approach:**\n",
    "- Implement a semantic search feature within Databricks that allows users to search articles by keywords, phrases, or topics.\n",
    "- Use text embeddings (e.g., with Sentence Transformers) to represent article content and enable similarity-based search.\n",
    "- Provide a simple search interface where users can enter queries and retrieve the most relevant articles.\n",
    "- Optionally, add filters for date, author, or branch to refine search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1703498d-08b9-4d99-b520-5229ab923a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c554b5-f293-4ec9-a15e-ba2d632e000b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TEXT_COL = \"content_text_csv\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "df = srgssr_article_corpus.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "model = get_embedder()\n",
    "emb_matrix = model.encode(\n",
    "    df[TEXT_COL].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "ids = df[ID_COL].tolist()\n",
    "texts = df[TEXT_COL].tolist()\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = emb_matrix @ q\n",
    "    top_idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i] for i in top_idx],\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n",
    "# Example usage:\n",
    "results = semantic_search(\"climate change\", top_k=10)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ece778-f9c3-469f-b11d-cab0530ec474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##USE CASE: find out what topics SRG writes about.\n",
    "**Approach:**  \n",
    "- Read the text from the `content_text_csv` column of the articles.\n",
    "- Compute similarity between article contents, e.g., by embedding the texts and using a Random Forest or other clustering/classification methods to group similar articles.\n",
    "- Identify clusters of similar content to reveal common topics or themes.\n",
    "- Use these clusters to enhance navigation and filtering options for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813d22bc-3223-47fa-901f-7a5f5a4d8671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d9846e-5c50-4939-a1ce-baecc46d55b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Use existing embeddings from previous cell\n",
    "n_clusters = 10  # Adjust as needed\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "labels = kmeans.fit_predict(emb_matrix)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"content_text_csv\": texts,\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "# Extract topic keywords for each cluster\n",
    "def get_topic_keywords(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['content_text_csv'].tolist()\n",
    "    \n",
    "    # Combine all texts in the cluster\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract words (filter out very short words and common stopwords)\n",
    "    words = re.findall(r'\\b[a-zÃ¤Ã¶Ã¼Ã Ã©Ã¨ÃªÃ«Ã¯Ã´Ã¹Ã»]{4,}\\b', combined_text)\n",
    "    \n",
    "    # Simple stopwords (extend as needed)\n",
    "    stopwords = {'dass', 'sind', 'wird', 'wurden', 'wurde', 'haben', 'sein', \n",
    "                 'eine', 'einem', 'einen', 'einer', 'dies', 'diese', 'dieser',\n",
    "                 'auch', 'mehr', 'beim', 'Ã¼ber', 'nach', 'sich', 'oder', 'kann',\n",
    "                 'kÃ¶nnen', 'mÃ¼ssen', 'soll', 'sollen', 'noch', 'bereits', 'aber',\n",
    "                 'wenn', 'weil', 'denn', 'dann', 'sowie', 'dass', 'damit', 'with',\n",
    "                 'from', 'have', 'this', 'that', 'will', 'been', 'were', 'their',\n",
    "                 'what', 'which', 'when', 'where', 'there', 'pour', 'dans', 'avec',\n",
    "                 'sont', 'Ãªtre', 'cette', 'mais', 'plus', 'comme', 'fait'}\n",
    "    \n",
    "    # Filter and count words\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels and add to dataframe\n",
    "topic_labels = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    keywords = get_topic_keywords(cluster_id, df_clusters, top_n=3)\n",
    "    topic_labels[cluster_id] = keywords\n",
    "\n",
    "# Map topic keywords to each row\n",
    "df_clusters['cluster_topic'] = df_clusters['cluster'].map(topic_labels)\n",
    "\n",
    "# Print cluster summary\n",
    "print(\"Cluster Topics (based on most frequent keywords):\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    count = len(df_clusters[df_clusters['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {topic_labels[cluster_id]} ({count} articles)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "display(df_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046f91e3-3664-4df3-9456-7d9f17dec6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization of the topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd658fbd-6f8b-47e8-9f37-17671d315bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df5054a-e980-450a-9e31-0b33e2f28f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Reduce embeddings to 2D using UMAP for visualization\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(emb_matrix)\n",
    "\n",
    "# Extract topic keywords for each cluster\n",
    "def get_topic_keywords(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['content_text_csv'].tolist()\n",
    "    \n",
    "    # Combine all texts in the cluster\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract words (filter out very short words and common stopwords)\n",
    "    words = re.findall(r'\\b[a-zÃ¤Ã¶Ã¼Ã Ã©Ã¨ÃªÃ«Ã¯Ã´Ã¹Ã»]{4,}\\b', combined_text)\n",
    "    \n",
    "    # Simple stopwords (extend as needed)\n",
    "    stopwords = {'dass', 'sind', 'wird', 'wurden', 'wurde', 'haben', 'sein', \n",
    "                 'eine', 'einem', 'einen', 'einer', 'dies', 'diese', 'dieser',\n",
    "                 'auch', 'mehr', 'beim', 'Ã¼ber', 'nach', 'sich', 'oder', 'kann',\n",
    "                 'kÃ¶nnen', 'mÃ¼ssen', 'soll', 'sollen', 'noch', 'bereits', 'aber',\n",
    "                 'wenn', 'weil', 'denn', 'dann', 'sowie', 'dass', 'damit', 'with',\n",
    "                 'from', 'have', 'this', 'that', 'will', 'been', 'were', 'their',\n",
    "                 'what', 'which', 'when', 'where', 'there', 'pour', 'dans', 'avec',\n",
    "                 'sont', 'Ãªtre', 'cette', 'mais', 'plus', 'comme', 'fait'}\n",
    "    \n",
    "    # Filter and count words\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels\n",
    "topic_labels = {}\n",
    "print(\"Cluster Topics (based on most frequent keywords):\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    keywords = get_topic_keywords(cluster_id, df_clusters, top_n=3)\n",
    "    topic_labels[cluster_id] = keywords\n",
    "    print(f\"Cluster {cluster_id}: {keywords}\")\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d[:, 0], \n",
    "    embedding_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "# Add colorbar to show cluster mapping\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Topic Clusters Visualization with Keywords (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers with topic labels\n",
    "kmeans_centers_2d = reducer.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d[:, 0], \n",
    "    kmeans_centers_2d[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels for each cluster center\n",
    "for cluster_id in range(n_clusters):\n",
    "    x, y = kmeans_centers_2d[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"\\nCluster Distribution:\")\n",
    "cluster_counts = df_clusters['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id} ({topic_labels[cluster_id]}): {count} articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fa3228-64a0-48c5-a251-143717b3555d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## USE CASE: Translate All Existing Articles into All Languages Used\n",
    "\n",
    "Goal: Automatically translate every article into all supported languages, so each article is available in every language used by SRG.\n",
    "\n",
    "**Approach:**\n",
    "- For each article, use the `ai_translate` function to generate translations for all target languages.\n",
    "- Store the translated articles alongside the originals for easy access and analytics.\n",
    "\n",
    "**Example (SQL):**\n",
    "sql\n",
    "SELECT\n",
    "  *,\n",
    "  ai_translate(title, 'fr') AS title_fr,\n",
    "  ai_translate(title, 'it') AS title_it,\n",
    "  ai_translate(title, 'en') AS title_en,\n",
    "  ai_translate(title, 'rm') AS title_rm\n",
    "FROM articles_flat\n",
    "\n",
    "*(Repeat for all relevant text fields and all required languages.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8bef14-2614-4a0c-9467-7b99a460b25f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d01b63-ec4b-4be5-b03c-8c9bcae191f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: This cell demonstrates translation capability\n",
    "# In a Databricks environment, you would use ai_translate()\n",
    "# In a local environment, we'll use googletrans library\n",
    "\n",
    "# Check if we're in a Databricks environment\n",
    "try:\n",
    "    # Try to access spark (available in Databricks)\n",
    "    spark\n",
    "    is_databricks = True\n",
    "except NameError:\n",
    "    is_databricks = False\n",
    "\n",
    "if is_databricks:\n",
    "    # Databricks implementation using ai_translate\n",
    "    print(\"Running in Databricks environment - using ai_translate\")\n",
    "    \n",
    "    lang_map = {\n",
    "        \"en\": \"en\", \"fr\": \"fr\", \"it\": \"it\", \"es\": \"es\", \"de\": \"de\",\n",
    "        \"pt\": \"pt\", \"zh\": \"zh\", \"ja\": \"ja\", \"ru\": \"ru\", \"ar\": \"ar\"\n",
    "    }\n",
    "    \n",
    "    from pyspark.sql.functions import expr\n",
    "    \n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        srgssr_article_corpus_spark = spark.createDataFrame(srgssr_article_corpus)\n",
    "    else:\n",
    "        srgssr_article_corpus_spark = srgssr_article_corpus\n",
    "    \n",
    "    df_translated = srgssr_article_corpus_spark\n",
    "    for lang, db_lang in lang_map.items():\n",
    "        df_translated = df_translated.withColumn(\n",
    "            f\"content_text_{lang}\",\n",
    "            expr(f\"ai_translate(content_text_csv, '{db_lang}')\")\n",
    "        )\n",
    "    \n",
    "    display(df_translated)\n",
    "    \n",
    "else:\n",
    "    # Local environment implementation using googletrans\n",
    "    print(\"Running in local environment - translation demo with sample data\")\n",
    "    print(\"Note: Install googletrans library for actual translation: %pip install googletrans==4.0.0-rc1\")\n",
    "    print(\"\\nFor demonstration, showing the first 3 articles with target language columns:\")\n",
    "    \n",
    "    # Create a sample demonstration showing the concept\n",
    "    lang_map = {\n",
    "        \"en\": \"English\", \"fr\": \"French\", \"it\": \"Italian\", \n",
    "        \"es\": \"Spanish\", \"de\": \"German\"\n",
    "    }\n",
    "    \n",
    "    # Take a small sample for demonstration\n",
    "    sample_df = srgssr_article_corpus.head(3).copy()\n",
    "    \n",
    "    # Add placeholder columns to show the concept\n",
    "    for lang, lang_name in lang_map.items():\n",
    "        sample_df[f\"content_text_{lang}\"] = f\"[Translation to {lang_name} would appear here]\"\n",
    "    \n",
    "    print(\"\\nSample structure of translated dataframe:\")\n",
    "    display(sample_df[['id', 'content_text_csv'] + [f\"content_text_{lang}\" for lang in lang_map.keys()]])\n",
    "    \n",
    "    print(\"\\nðŸ’¡ To implement actual translation in local environment:\")\n",
    "    print(\"   1. Install: %pip install googletrans==4.0.0-rc1\")\n",
    "    print(\"   2. Use: from googletrans import Translator\")\n",
    "    print(\"   3. Translate: translator.translate(text, dest='fr').text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d264660-2b02-4352-b1d6-fbc3390ad27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Create a copy of the dataframe to store translations\n",
    "df_translated = srgssr_article_corpus.copy()\n",
    "\n",
    "# Function to translate text with error handling\n",
    "def translate_text(text, dest='en', max_retries=3):\n",
    "    \"\"\"Translate text to target language with retry logic\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Limit text length to avoid API issues (Google Translate has limits)\n",
    "    text_str = str(text)[:5000]  # Limit to 5000 characters\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translator.translate(text_str, dest=dest)\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Wait before retry\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Translation failed after {max_retries} attempts: {str(e)[:100]}\")\n",
    "                return text_str  # Return original text if translation fails\n",
    "    \n",
    "    return text_str\n",
    "\n",
    "# Translate the content_text_csv column\n",
    "print(\"Translating articles to English...\")\n",
    "print(f\"Total articles to translate: {len(df_translated)}\")\n",
    "print(\"Note: This may take several minutes depending on the number of articles.\")\n",
    "print(\"Google Translate API has rate limits, so we add small delays between requests.\\n\")\n",
    "\n",
    "# Translate with progress indicator\n",
    "translated_texts = []\n",
    "for idx, text in enumerate(df_translated['content_text_csv']):\n",
    "    if idx % 10 == 0:  # Progress update every 10 articles\n",
    "        print(f\"Progress: {idx}/{len(df_translated)} articles translated...\")\n",
    "    \n",
    "    translated = translate_text(text, dest='en')\n",
    "    translated_texts.append(translated)\n",
    "    \n",
    "    # Small delay to avoid rate limiting (adjust as needed)\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Add translated column\n",
    "df_translated['content_text_en'] = translated_texts\n",
    "\n",
    "print(f\"\\nâœ… Translation complete! Translated {len(df_translated)} articles.\")\n",
    "print(\"\\nShowing first 3 translated articles:\")\n",
    "display(df_translated[['id', 'content_text_csv', 'content_text_en']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ddfad8-e8a4-4318-ab8a-ada0927153c0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"content_text_csv\":676},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762956141217}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_translated[['id', 'content_text_csv', 'content_text_en']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90afd22e-e500-4727-afbb-d007f3867e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize topics of the translated (English) articles\n",
    "\n",
    "print(\"Creating embeddings for translated English articles...\")\n",
    "\n",
    "# Use the English translated column\n",
    "df_en = df_translated.copy()\n",
    "df_en['content_text_en'] = df_en['content_text_en'].fillna(\"\").astype(str)\n",
    "\n",
    "# Create embeddings for the English text\n",
    "model_en = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb_matrix_en = model_en.encode(\n",
    "    df_en['content_text_en'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# Perform clustering on English translations\n",
    "n_clusters_en = 10\n",
    "kmeans_en = KMeans(n_clusters=n_clusters_en, random_state=42, n_init=\"auto\")\n",
    "labels_en = kmeans_en.fit_predict(emb_matrix_en)\n",
    "\n",
    "# Create dataframe with cluster assignments\n",
    "df_clusters_en = pd.DataFrame({\n",
    "    \"id\": df_en['id'].tolist(),\n",
    "    \"original_text\": df_en['content_text_csv'].tolist(),\n",
    "    \"translated_text_en\": df_en['content_text_en'].tolist(),\n",
    "    \"cluster\": labels_en\n",
    "})\n",
    "\n",
    "# Extract topic keywords from English translations\n",
    "def get_topic_keywords_en(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from English articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['translated_text_en'].tolist()\n",
    "    \n",
    "    # Combine all texts in the cluster\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract English words\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "    \n",
    "    # English stopwords\n",
    "    stopwords = {\n",
    "        'this', 'that', 'with', 'from', 'have', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'will', 'would', 'could',\n",
    "        'should', 'about', 'after', 'also', 'many', 'more', 'most', 'other',\n",
    "        'some', 'such', 'than', 'them', 'then', 'these', 'they', 'very',\n",
    "        'into', 'just', 'like', 'only', 'over', 'said', 'same', 'says',\n",
    "        'does', 'make', 'made', 'well', 'much', 'even', 'back', 'through',\n",
    "        'year', 'years', 'being', 'people', 'according', 'since', 'during'\n",
    "    }\n",
    "    \n",
    "    # Filter and count words\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels\n",
    "topic_labels_en = {}\n",
    "print(\"\\nCluster Topics (based on English translated text):\")\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = get_topic_keywords_en(cluster_id, df_clusters_en, top_n=3)\n",
    "    topic_labels_en[cluster_id] = keywords\n",
    "\n",
    "# Add topic labels to dataframe\n",
    "df_clusters_en['cluster_topic'] = df_clusters_en['cluster'].map(topic_labels_en)\n",
    "\n",
    "# Print cluster summary\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    count = len(df_clusters_en[df_clusters_en['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {topic_labels_en[cluster_id]} ({count} articles)\")\n",
    "\n",
    "# Visualize with UMAP\n",
    "print(\"\\nCreating 2D visualization...\")\n",
    "reducer_en = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d_en = reducer_en.fit_transform(emb_matrix_en)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d_en[:, 0], \n",
    "    embedding_2d_en[:, 1], \n",
    "    c=labels_en, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Topic Clusters of Translated English Articles (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers\n",
    "kmeans_centers_2d_en = reducer_en.transform(kmeans_en.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d_en[:, 0], \n",
    "    kmeans_centers_2d_en[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels for each cluster center\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    x, y = kmeans_centers_2d_en[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels_en[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDisplaying clustered translated articles:\")\n",
    "display(df_clusters_en[['id', 'translated_text_en', 'cluster', 'cluster_topic']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93698dd4-21bf-426c-9e94-631b9b33971f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Map clusters to higher-level topic categories\n",
    "\n",
    "# Define keyword patterns for major topic categories\n",
    "topic_categories = {\n",
    "    'Politics': ['government', 'election', 'parliament', 'minister', 'political', 'policy', 'president', \n",
    "                 'vote', 'party', 'democrat', 'republican', 'law', 'congress', 'senate', 'council',\n",
    "                 'federal', 'state', 'referendum', 'campaign', 'diplomat'],\n",
    "    \n",
    "    'Sports': ['football', 'soccer', 'tennis', 'basketball', 'hockey', 'olympic', 'champion', 'team',\n",
    "               'player', 'match', 'game', 'tournament', 'league', 'coach', 'athlete', 'sport',\n",
    "               'championship', 'victory', 'defeat', 'goal', 'score'],\n",
    "    \n",
    "    'Economy': ['economy', 'economic', 'business', 'market', 'bank', 'finance', 'investment', 'trade',\n",
    "                'company', 'stock', 'price', 'inflation', 'currency', 'export', 'import', 'growth',\n",
    "                'gdp', 'employment', 'unemployment', 'budget', 'debt', 'profit'],\n",
    "    \n",
    "    'Science & Technology': ['science', 'technology', 'research', 'study', 'university', 'scientist',\n",
    "                             'experiment', 'discovery', 'innovation', 'digital', 'computer', 'internet',\n",
    "                             'software', 'data', 'artificial', 'intelligence', 'robot', 'space', 'energy'],\n",
    "    \n",
    "    'Health': ['health', 'medical', 'hospital', 'doctor', 'patient', 'disease', 'treatment', 'medicine',\n",
    "               'virus', 'vaccine', 'pandemic', 'covid', 'care', 'mental', 'clinic', 'drug', 'therapy'],\n",
    "    \n",
    "    'Environment': ['climate', 'environment', 'environmental', 'weather', 'temperature', 'global',\n",
    "                    'warming', 'carbon', 'pollution', 'sustainable', 'renewable', 'energy', 'nature',\n",
    "                    'forest', 'ocean', 'animal', 'species', 'biodiversity', 'ecological'],\n",
    "    \n",
    "    'Culture & Entertainment': ['culture', 'cultural', 'music', 'film', 'movie', 'concert', 'festival',\n",
    "                                'artist', 'museum', 'exhibition', 'theater', 'performance', 'book',\n",
    "                                'author', 'literature', 'entertainment', 'celebrity', 'show'],\n",
    "    \n",
    "    'Society': ['social', 'society', 'community', 'people', 'family', 'education', 'school', 'student',\n",
    "                'teacher', 'child', 'women', 'rights', 'justice', 'police', 'crime', 'court', 'prison']\n",
    "}\n",
    "\n",
    "def assign_category(cluster_keywords):\n",
    "    \"\"\"Assign a category based on keyword matching\"\"\"\n",
    "    keywords_lower = cluster_keywords.lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for category, category_keywords in topic_categories.items():\n",
    "        score = sum(1 for kw in category_keywords if kw in keywords_lower)\n",
    "        if score > 0:\n",
    "            scores[category] = score\n",
    "    \n",
    "    if scores:\n",
    "        # Return category with highest score\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Assign categories to each cluster\n",
    "cluster_categories = {}\n",
    "print(\"Mapping clusters to higher-level topic categories:\\n\")\n",
    "print(f\"{'Cluster':<10} {'Keywords':<40} {'Category':<25}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = topic_labels_en[cluster_id]\n",
    "    category = assign_category(keywords)\n",
    "    cluster_categories[cluster_id] = category\n",
    "    print(f\"{cluster_id:<10} {keywords:<40} {category:<25}\")\n",
    "\n",
    "# Add category column to dataframe\n",
    "df_clusters_en['topic_category'] = df_clusters_en['cluster'].map(cluster_categories)\n",
    "\n",
    "# Count articles per category\n",
    "print(\"\\n\\nArticle Distribution by Topic Category:\")\n",
    "print(\"=\" * 50)\n",
    "category_counts = df_clusters_en['topic_category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    print(f\"{category:<25} {count:>5} articles ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Visualize categories\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a color map for categories\n",
    "unique_categories = sorted(df_clusters_en['topic_category'].unique())\n",
    "category_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "category_color_map = {cat: color for cat, color in zip(unique_categories, category_colors)}\n",
    "\n",
    "# Map categories to colors for each point\n",
    "point_colors = df_clusters_en['topic_category'].map(category_color_map)\n",
    "\n",
    "# Create visualization with categories\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Left plot: Colored by category\n",
    "for category in unique_categories:\n",
    "    mask = df_clusters_en['topic_category'] == category\n",
    "    indices = df_clusters_en[mask].index\n",
    "    ax1.scatter(\n",
    "        embedding_2d_en[indices, 0],\n",
    "        embedding_2d_en[indices, 1],\n",
    "        c=[category_color_map[category]],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax1.set_title('Articles by Topic Category', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Pie chart of category distribution\n",
    "ax2.pie(\n",
    "    category_counts.values,\n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=[category_color_map[cat] for cat in category_counts.index]\n",
    ")\n",
    "ax2.set_title('Distribution of Articles by Topic Category', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample articles with categories\n",
    "print(\"\\n\\nSample Articles with Categories:\")\n",
    "display(df_clusters_en[['id', 'translated_text_en', 'cluster', 'cluster_topic', 'topic_category']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c434fb32-744e-4f71-9400-4e7f79de8adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze clusters that were categorized as \"Other\"\n",
    "\n",
    "print(\"Analyzing 'Other' category clusters:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "other_clusters = [cid for cid, cat in cluster_categories.items() if cat == 'Other']\n",
    "\n",
    "if other_clusters:\n",
    "    print(f\"Found {len(other_clusters)} cluster(s) categorized as 'Other':\")\n",
    "    print(\"\\nCluster details:\")\n",
    "    for cluster_id in other_clusters:\n",
    "        keywords = topic_labels_en[cluster_id]\n",
    "        count = len(df_clusters_en[df_clusters_en['cluster'] == cluster_id])\n",
    "        print(f\"\\nCluster {cluster_id}: {keywords}\")\n",
    "        print(f\"  - Article count: {count}\")\n",
    "        print(f\"  - Sample article text:\")\n",
    "        sample = df_clusters_en[df_clusters_en['cluster'] == cluster_id]['translated_text_en'].iloc[0][:200]\n",
    "        print(f\"    {sample}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nðŸ’¡ Suggestions to fix 'Other' categories:\")\n",
    "    print(\"1. The keywords extracted from these clusters don't match predefined category keywords\")\n",
    "    print(\"2. You can manually assign these clusters to appropriate categories\")\n",
    "    print(\"3. Or add more keywords to the topic_categories dictionary\")\n",
    "    print(\"4. Or use more sophisticated text from the full articles to categorize\")\n",
    "else:\n",
    "    print(\"âœ… All clusters successfully categorized! No 'Other' category found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850cec84-2a99-4102-b399-29b41812b466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enrich topic categories with keywords from the actual corpus\n",
    "\n",
    "print(\"Analyzing corpus to extract additional keywords for each category...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze each cluster to find common words\n",
    "all_cluster_words = {}\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    cluster_texts = df_clusters_en[df_clusters_en['cluster'] == cluster_id]['translated_text_en'].tolist()\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract words\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "    \n",
    "    # Extended stopwords\n",
    "    stopwords = {\n",
    "        'this', 'that', 'with', 'from', 'have', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'will', 'would', 'could',\n",
    "        'should', 'about', 'after', 'also', 'many', 'more', 'most', 'other',\n",
    "        'some', 'such', 'than', 'them', 'then', 'these', 'they', 'very',\n",
    "        'into', 'just', 'like', 'only', 'over', 'said', 'same', 'says',\n",
    "        'does', 'make', 'made', 'well', 'much', 'even', 'back', 'through',\n",
    "        'year', 'years', 'being', 'people', 'according', 'since', 'during',\n",
    "        'first', 'time', 'last', 'still', 'however', 'while', 'before'\n",
    "    }\n",
    "    \n",
    "    # Filter and count\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(words)\n",
    "    all_cluster_words[cluster_id] = word_counts.most_common(20)\n",
    "\n",
    "# Enhanced topic categories with corpus-derived keywords\n",
    "topic_categories_enhanced = {\n",
    "    'Politics': ['government', 'election', 'parliament', 'minister', 'political', 'policy', 'president', \n",
    "                 'vote', 'party', 'democrat', 'republican', 'law', 'congress', 'senate', 'council',\n",
    "                 'federal', 'state', 'referendum', 'campaign', 'diplomat', 'legislative', 'executive',\n",
    "                 'parliament', 'coalition', 'opposition', 'chancellor', 'mayor', 'governor', 'prime'],\n",
    "    \n",
    "    'Sports': ['football', 'soccer', 'tennis', 'basketball', 'hockey', 'olympic', 'champion', 'team',\n",
    "               'player', 'match', 'game', 'tournament', 'league', 'coach', 'athlete', 'sport',\n",
    "               'championship', 'victory', 'defeat', 'goal', 'score', 'final', 'world', 'cup',\n",
    "               'season', 'club', 'training', 'competition', 'medal', 'race', 'swimming', 'skiing'],\n",
    "    \n",
    "    'Economy & Business': ['economy', 'economic', 'business', 'market', 'bank', 'finance', 'investment', 'trade',\n",
    "                           'company', 'stock', 'price', 'inflation', 'currency', 'export', 'import', 'growth',\n",
    "                           'gdp', 'employment', 'unemployment', 'budget', 'debt', 'profit', 'financial',\n",
    "                           'corporate', 'industry', 'commercial', 'entrepreneur', 'revenue', 'sales', 'consumer'],\n",
    "    \n",
    "    'Science & Technology': ['science', 'technology', 'research', 'study', 'university', 'scientist',\n",
    "                             'experiment', 'discovery', 'innovation', 'digital', 'computer', 'internet',\n",
    "                             'software', 'data', 'artificial', 'intelligence', 'robot', 'space', 'energy',\n",
    "                             'tech', 'innovation', 'laboratory', 'academic', 'technical', 'engineering',\n",
    "                             'development', 'scientific', 'app', 'online', 'platform', 'system'],\n",
    "    \n",
    "    'Health': ['health', 'medical', 'hospital', 'doctor', 'patient', 'disease', 'treatment', 'medicine',\n",
    "               'virus', 'vaccine', 'pandemic', 'covid', 'care', 'mental', 'clinic', 'drug', 'therapy',\n",
    "               'healthcare', 'diagnosis', 'symptoms', 'infection', 'prevention', 'nursing', 'surgery',\n",
    "               'pharmaceutical', 'wellness', 'emergency', 'healthcare'],\n",
    "    \n",
    "    'Environment & Climate': ['climate', 'environment', 'environmental', 'weather', 'temperature', 'global',\n",
    "                              'warming', 'carbon', 'pollution', 'sustainable', 'renewable', 'energy', 'nature',\n",
    "                              'forest', 'ocean', 'animal', 'species', 'biodiversity', 'ecological', 'green',\n",
    "                              'conservation', 'wildlife', 'natural', 'emission', 'solar', 'wind', 'water'],\n",
    "    \n",
    "    'Culture & Entertainment': ['culture', 'cultural', 'music', 'film', 'movie', 'concert', 'festival',\n",
    "                                'artist', 'museum', 'exhibition', 'theater', 'performance', 'book',\n",
    "                                'author', 'literature', 'entertainment', 'celebrity', 'show', 'art',\n",
    "                                'cinema', 'gallery', 'dance', 'opera', 'creative', 'painting', 'song'],\n",
    "    \n",
    "    'Society & Education': ['social', 'society', 'community', 'people', 'family', 'education', 'school', 'student',\n",
    "                            'teacher', 'child', 'women', 'rights', 'justice', 'police', 'crime', 'court', 'prison',\n",
    "                            'learning', 'teaching', 'university', 'college', 'children', 'youth', 'citizenship',\n",
    "                            'welfare', 'public', 'human'],\n",
    "    \n",
    "    'International & Foreign Affairs': ['international', 'foreign', 'country', 'countries', 'world', 'global',\n",
    "                                        'nation', 'diplomatic', 'relations', 'treaty', 'ambassador', 'border',\n",
    "                                        'crisis', 'conflict', 'peace', 'war', 'alliance', 'united', 'nations'],\n",
    "    \n",
    "    'Media & Communication': ['media', 'news', 'press', 'journalist', 'television', 'radio', 'broadcast',\n",
    "                              'newspaper', 'magazine', 'report', 'reporter', 'channel', 'publishing',\n",
    "                              'communication', 'interview', 'announcement', 'statement']\n",
    "}\n",
    "\n",
    "# Function with enhanced categories\n",
    "def assign_category_enhanced(cluster_keywords):\n",
    "    \"\"\"Assign a category based on keyword matching with enhanced keywords\"\"\"\n",
    "    keywords_lower = cluster_keywords.lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for category, category_keywords in topic_categories_enhanced.items():\n",
    "        # Check for exact matches\n",
    "        score = sum(1 for kw in category_keywords if kw in keywords_lower)\n",
    "        if score > 0:\n",
    "            scores[category] = score\n",
    "    \n",
    "    if scores:\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Re-assign categories with enhanced keywords\n",
    "cluster_categories_enhanced = {}\n",
    "print(\"\\nRe-mapping clusters with ENHANCED topic categories:\\n\")\n",
    "print(f\"{'Cluster':<10} {'Keywords':<45} {'Category':<30}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = topic_labels_en[cluster_id]\n",
    "    category = assign_category_enhanced(keywords)\n",
    "    cluster_categories_enhanced[cluster_id] = category\n",
    "    print(f\"{cluster_id:<10} {keywords:<45} {category:<30}\")\n",
    "\n",
    "# Update dataframe with new categories\n",
    "df_clusters_en['topic_category_enhanced'] = df_clusters_en['cluster'].map(cluster_categories_enhanced)\n",
    "\n",
    "# Count articles per enhanced category\n",
    "print(\"\\n\\nArticle Distribution by ENHANCED Topic Category:\")\n",
    "print(\"=\" * 60)\n",
    "category_counts_enhanced = df_clusters_en['topic_category_enhanced'].value_counts()\n",
    "for category, count in category_counts_enhanced.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    print(f\"{category:<30} {count:>5} articles ({percentage:>5.1f}%)\")\n",
    "\n",
    "# Compare old vs new categorization\n",
    "print(\"\\n\\nComparison: Original vs Enhanced Categorization:\")\n",
    "print(\"=\" * 60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Category': df_clusters_en['topic_category'],\n",
    "    'Enhanced Category': df_clusters_en['topic_category_enhanced']\n",
    "})\n",
    "changed = comparison_df[comparison_df['Original Category'] != comparison_df['Enhanced Category']]\n",
    "if len(changed) > 0:\n",
    "    print(f\"âœ¨ {len(changed)} articles were re-categorized with enhanced keywords!\")\n",
    "    print(f\"\\nChanges by cluster:\")\n",
    "    for cluster_id in range(n_clusters_en):\n",
    "        cluster_mask = df_clusters_en['cluster'] == cluster_id\n",
    "        orig_cat = df_clusters_en[cluster_mask]['topic_category'].iloc[0]\n",
    "        new_cat = df_clusters_en[cluster_mask]['topic_category_enhanced'].iloc[0]\n",
    "        if orig_cat != new_cat:\n",
    "            print(f\"  Cluster {cluster_id}: '{orig_cat}' â†’ '{new_cat}'\")\n",
    "else:\n",
    "    print(\"No changes - all categories remained the same.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "32ca4a52-0569-4046-ad1a-7ebcb1a4030a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize clusters with enhanced categorization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a color map for enhanced categories\n",
    "unique_categories = sorted(df_clusters_en['topic_category_enhanced'].unique())\n",
    "category_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "category_color_map = {cat: color for cat, color in zip(unique_categories, category_colors)}\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(22, 10))\n",
    "\n",
    "# Left plot: Scatter plot colored by enhanced category\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "for category in unique_categories:\n",
    "    mask = df_clusters_en['topic_category_enhanced'] == category\n",
    "    indices = df_clusters_en[mask].index\n",
    "    ax1.scatter(\n",
    "        embedding_2d_en[indices, 0],\n",
    "        embedding_2d_en[indices, 1],\n",
    "        c=[category_color_map[category]],\n",
    "        label=category,\n",
    "        alpha=0.7,\n",
    "        s=60,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5\n",
    "    )\n",
    "\n",
    "# Add cluster centers with labels\n",
    "kmeans_centers_2d_en = reducer_en.transform(kmeans_en.cluster_centers_)\n",
    "ax1.scatter(\n",
    "    kmeans_centers_2d_en[:, 0], \n",
    "    kmeans_centers_2d_en[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=300, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "# Add cluster labels\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    x, y = kmeans_centers_2d_en[cluster_id]\n",
    "    category = cluster_categories_enhanced[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels_en[cluster_id]}\\n({category})\"\n",
    "    ax1.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(15, 15),\n",
    "        textcoords='offset points',\n",
    "        fontsize=8,\n",
    "        bbox=dict(boxstyle='round,pad=0.6', facecolor='yellow', alpha=0.8, edgecolor='black'),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2', color='black', lw=1.5),\n",
    "        zorder=6\n",
    "    )\n",
    "\n",
    "ax1.set_title('Article Clusters with Enhanced Topic Categories\\n(UMAP 2D Projection)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=9, framealpha=0.9, edgecolor='black')\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Right plot: Pie chart with enhanced categories\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "category_counts = df_clusters_en['topic_category_enhanced'].value_counts()\n",
    "colors = [category_color_map[cat] for cat in category_counts.index]\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    category_counts.values,\n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    textprops={'fontsize': 10, 'weight': 'bold'},\n",
    "    wedgeprops={'edgecolor': 'black', 'linewidth': 1.5}\n",
    ")\n",
    "\n",
    "# Improve percentage text visibility\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontsize(9)\n",
    "\n",
    "ax2.set_title('Distribution of Articles by Enhanced Topic Category\\n' + \n",
    "              f'(Total: {len(df_clusters_en)} articles)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLUSTER ANALYSIS WITH ENHANCED CATEGORIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Cluster':<10} {'Keywords':<35} {'Category':<30} {'Count':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = topic_labels_en[cluster_id]\n",
    "    category = cluster_categories_enhanced[cluster_id]\n",
    "    count = len(df_clusters_en[df_clusters_en['cluster'] == cluster_id])\n",
    "    print(f\"{cluster_id:<10} {keywords:<35} {category:<30} {count:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORY DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    bar = \"â–ˆ\" * int(percentage / 2)\n",
    "    print(f\"{category:<30} {count:>5} articles ({percentage:>5.1f}%) {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be86afb-25be-4937-88b7-a4aa8c27fd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b220d3-50ac-4a2d-b7c6-70eed00e4561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Hints and Notes\n",
    "> **Hint:**  \n",
    "> For a temporary Jupyter environment to experiment or explore data, consider using [RenkuLab](https://renkulab.io/p/snsf-anoxia-project/proxy-proxy/sessions/01JX2TG1RZ9J0PQ53H3RT81BD4/start). RenkuLab offers cloud-based notebook sessionsâ€”no local setup required.\n",
    "\n",
    "> **Note:**  \n",
    "> RenkuLab sessions may require authentication and have limited resources. Save your work frequently, as sessions can time out or be terminated after inactivity."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4689788114568825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Group_Work",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "crashpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
