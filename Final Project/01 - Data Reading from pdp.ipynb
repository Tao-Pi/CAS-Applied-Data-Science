{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919d6a04-380d-4219-a636-d1ed27effbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Acquisition (ETL)\n",
    "\n",
    "**Starting Point:**  \n",
    "The article data originates from a Kafka datastream. It is not normalized (so it cannot be analyzed directly) and requires Active Directory login (making collaboration difficult).  \n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "- **Processing steps:**  \n",
    "  1. Read article data from the Delta table populated from Kafka.\n",
    "  2. Flatten and transform nested fields (e.g., titles, resources, contributors) using a SQL view.\n",
    "  3. Create a Spark DataFrame from the flattened view and inspect the results.\n",
    "  4. Write the DataFrame to a Delta table for analytics and automation.\n",
    "  5. Export a <25MB Parquet sample with only public data for sharing (e.g., via GitHub).\n",
    "\n",
    "**Goal:**  \n",
    "The data should be available as a Parquet file for sharing. Since the dataset is large (5GB), only a public sample is exported for easy distribution.\n",
    "\n",
    "**Access Control:**  \n",
    "To guarantee data integrity and protect sensitive information, data distribution is based on user access rights. Entitled users can access the full confidential dataset, while restricted users are provided with only the public sample. This ensures that only authorized users can view sensitive data, maintaining compliance and data security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b90172e-2da5-4c57-a083-4b8a465d659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def has_read_permission(table_name):\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "has_read_access_udp_articles_v2 = has_read_permission(\"udp_prd_atomic.pdp.articles_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35abc26-aae0-4cc6-b0b5-4ff0d3bca2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-01: Read from Kafka (SQL)\n",
    "\n",
    "The following steps read article data from the Delta table `udp_prd_atomic.pdp.articles_v2`, which is assumed to be populated from a Kafka stream. The original Kafka data contains nested lists and complex structures (e.g., for multilingual fields or arrays of resources). In the transformation, the SQL view `articles_flat` flattens this nested data by extracting relevant fields and, where multiple values exist (such as for titles in different languages), selects the first available entryâ€”typically prioritizing German (`'de'`) or otherwise the first value. This process prepares the data, along with Kafka metadata (key, topic, partition, offset, timestamp), for further analysis in a flat, tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0aa6cb20-6266-411c-b68d-02820485cbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Only run main logic if has_read_access_udp_articles_v2 is true\n",
    "\n",
    "-- Uncomment below to run only when has_read_access_udp_articles_v2 = true\n",
    "-- IF has_read_access_udp_articles_v2 THEN\n",
    "CREATE OR REPLACE TEMP VIEW articles_flat AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    key,\n",
    "    topic,\n",
    "    partition,\n",
    "    offset,\n",
    "    timestamp,\n",
    "    value AS v\n",
    "  FROM udp_prd_atomic.pdp.articles_v2\n",
    ")\n",
    "SELECT\n",
    "  -- flat metadata\n",
    "  v.id AS id,\n",
    "  v.publisher AS publisher,\n",
    "  v.provenance AS provenance,\n",
    "  v.modificationDate AS modificationDate,\n",
    "  v.releaseDate AS releaseDate,\n",
    "  -- TITLE: always first entry\n",
    "  try_element_at(transform(coalesce(v.title, array()), x -> x.content), 1) AS title_auto,\n",
    "  -- LEAD: always first entry\n",
    "  try_element_at(transform(coalesce(v.lead, array()), x -> x.content), 1) AS lead_auto,\n",
    "  -- KICKER: always first entry\n",
    "  try_element_at(transform(coalesce(v.kicker, array()), x -> x.content), 1) AS kicker_auto,\n",
    "  -- Extract IDs specifically (not a Map!)\n",
    "  try_element_at(\n",
    "    transform(\n",
    "      filter(coalesce(v.identifiers, array()), x -> x.type = 'urn'   AND x.value IS NOT NULL),\n",
    "      x -> x.value\n",
    "    ),\n",
    "    1\n",
    "  ) AS id_urn,\n",
    "  try_element_at(\n",
    "    transform(\n",
    "      filter(coalesce(v.identifiers, array()), x -> x.type = 'srgId' AND x.value IS NOT NULL),\n",
    "      x -> x.value\n",
    "    ),\n",
    "    1\n",
    "  ) AS id_srg,\n",
    "  -- First available image\n",
    "  aggregate(\n",
    "    coalesce(v.resources, array()),\n",
    "    CAST(NULL AS STRING),\n",
    "    (acc, r) -> coalesce(acc, r.picture.locator.url),\n",
    "    acc -> acc\n",
    "  ) AS picture_url,\n",
    "  -- content.text.items -> CSV (space-separated)\n",
    "  CASE\n",
    "    WHEN v.content IS NOT NULL\n",
    "      THEN concat_ws(' ', coalesce(v.content.text, CAST(array() AS ARRAY<STRING>)))\n",
    "    ELSE NULL\n",
    "  END AS content_text_csv,\n",
    "  -- contributors -> CSV of names\n",
    "  CASE\n",
    "    WHEN v.contributors IS NOT NULL\n",
    "      THEN concat_ws(\n",
    "        ', ',\n",
    "        transform(\n",
    "          coalesce(v.contributors, array()),\n",
    "          c -> coalesce(\n",
    "            c.name,\n",
    "            c.agent.person.name,\n",
    "            c.agent.team.name,\n",
    "            c.agent.department.name\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ELSE NULL\n",
    "  END AS contributors_csv,\n",
    "  -- only first locator.url from resources -> CSV\n",
    "  try_element_at(\n",
    "    filter(\n",
    "      flatten(\n",
    "        transform(\n",
    "          coalesce(v.resources, array()),\n",
    "          r -> array(\n",
    "            r.document.locator.url,\n",
    "            r.picture.locator.url,\n",
    "            r.link.locator.url\n",
    "          )\n",
    "        )\n",
    "      ),\n",
    "      x -> x IS NOT NULL\n",
    "    ),\n",
    "    1\n",
    "  ) AS resources_locator_urls_csv,\n",
    "  -- Keywords -> CSV\n",
    "  CASE\n",
    "    WHEN v.keywords IS NOT NULL\n",
    "      THEN concat_ws(',', coalesce(v.keywords, CAST(array() AS ARRAY<STRING>)))\n",
    "    ELSE NULL\n",
    "  END AS keywords_csv,\n",
    "  -- Kafka metadata (key safely as STRING)\n",
    "  CAST(key AS STRING) AS key,\n",
    "  topic,\n",
    "  partition,\n",
    "  offset,\n",
    "  timestamp\n",
    "FROM base;\n",
    "-- END IF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf8dd3b-2ede-4d29-8400-9538c6d279b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-02: Create DataFrame and Visually Inspect Results\n",
    "\n",
    "The code below runs a Spark SQL query against the temporary view `articles_flat`, loads the result into a Spark DataFrame named `df`, and then displays the DataFrame for visual inspection. This step materializes the flattened article data so it can be further processed or written to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b0ecb9f3-66a9-4c6e-930b-cc6006cdc795",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"picture_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"resources_locator_urls_csv\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761544712864}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    articles_flat = spark.sql(\"SELECT * FROM articles_flat\").orderBy(\"modificationDate\", ascending=False)\n",
    "    display(articles_flat, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e96a63a-ade2-4735-a697-bd4f3e60ea82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-03: Save Data to Delta Table\n",
    "\n",
    "In the next steps, the data will be saved both to a Delta table (for better automation and analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aab4d4-2ceb-40e9-baa2-e40589325f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to (private) Delta Table - all articles\n",
    "\n",
    "The following code appends the transformed DataFrame `df` to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`. It writes in **append** mode, uses the **Delta** format, and enables **schema merging** so that any new columns are automatically added to the target table without overwriting existing data.\n",
    "\n",
    "- Contains all articles (**confidential**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0b24b138-b0bd-4dff-9e48-35481b053d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    articles_flat.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"swi_audience_prd.pdp_articles_v2.articles_v2_flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6e6098-965f-4ad5-b8aa-79264468c1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Write to (public) Parquet File - selected articles, manually Upload to GitHub\n",
    "\n",
    "Export a <25 MB sample of the data with only public data as a Parquet file for easy sharing via GitHub.  \n",
    "**Note:** The Parquet file must be manually downloaded from Databricks and then uploaded to your GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6689c36-f482-477e-8f2b-8461f6f105d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    from pyspark.sql.functions import col, current_date\n",
    "    import math\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # Estimate average row size in bytes using a sample of 100,000 rows\n",
    "    sample_row = articles_flat.limit(100000).toPandas()\n",
    "    avg_row_size = sample_row.memory_usage(index=True, deep=True).sum() / len(sample_row)\n",
    "\n",
    "    # Set maximum file size to 25 MB\n",
    "    max_bytes = 25 * 1024 * 1024  # 25 MB\n",
    "\n",
    "    # Parquet is more efficient than CSV, so increase row count estimate by a factor of 2\n",
    "    max_rows = int(math.floor((max_bytes / avg_row_size) * 2))\n",
    "\n",
    "    # Filter articles to only those published (releaseDate today or earlier), then sample up to max_rows\n",
    "    articles_flat_sampled = (\n",
    "        articles_flat\n",
    "        .filter(col(\"releaseDate\") <= current_date())\n",
    "        .limit(max_rows)\n",
    "    )\n",
    "\n",
    "    # Write the sampled DataFrame as a single Parquet file to a temporary directory\n",
    "    parquet_temp_dir = \"/Volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume/export_articles_v2_sample25mb_tmp_parquet\"\n",
    "    parquet_final_path = \"/Volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume/export_articles_v2_sample25mb.parquet\"\n",
    "\n",
    "    (articles_flat_sampled.coalesce(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(parquet_temp_dir)\n",
    "    )\n",
    "\n",
    "    # Move the single Parquet part file to the final destination\n",
    "    files = [f for f in os.listdir(parquet_temp_dir) if f.endswith(\".parquet\")]\n",
    "    if files:\n",
    "        src_file = os.path.join(parquet_temp_dir, files[0])\n",
    "        shutil.move(src_file, parquet_final_path)\n",
    "\n",
    "    # Remove the temporary directory after moving the file\n",
    "    shutil.rmtree(parquet_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5409a74-7842-49e5-9a43-56626ee9ea3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...now manually:\n",
    "\n",
    "1. **Open the CSV file in Databricks:**\n",
    "   - Navigate to [Databricks Volume Browser](https://adb-4119964566130471.11.azuredatabricks.net/explore/data/volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume?o=4119964566130471) in the Databricks workspace file browser.\n",
    "\n",
    "2. **Download the file:**\n",
    "   - Right-click on `export_articles_v2_sample25mb.parquet` and select **\"Download\"** to save the file to your local machine.\n",
    "\n",
    "3. **Upload the file to GitHub:**\n",
    "   - Go to [GitHub Folder](https://github.com/Tao-Pi/CAS-Applied-Data-Science/tree/main/Module-3/01_Module%20Final%20Assignment).\n",
    "   - Click **\"Add file\"** > **\"Upload files\"**.\n",
    "   - Drag and drop `export_articles_v2_sample25mb.parquet` or use the file picker to select it.\n",
    "   - Commit the changes to upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca686f8-2f86-434b-9576-c57c9d1b8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step-04: Load Data Based on User Rights\n",
    "\n",
    "The next step is to load the data, with access determined by user rights:\n",
    "\n",
    "- **Restricted users** can load only the public data sample (e.g., the Parquet file exported for sharing).\n",
    "- **Entitled users** can load the full, confidential dataset from the Delta table.\n",
    "\n",
    "This ensures that sensitive information is only accessible to authorized users, while still allowing broader access to public data for collaboration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0706526-3796-466a-99d7-7ab6b3ff5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Restricted users \n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url)\n",
    "\n",
    "# Entitled users\n",
    "if has_read_access_udp_articles_v2:\n",
    "    srgssr_article_corpus = spark.table(\"swi_audience_prd.pdp_articles_v2.articles_v2_flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cecfdc-221b-4fca-b2d5-a3394395d892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "    srgssr_article_corpus = pd.DataFrame(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d121c8-4b18-4d6e-827d-7d9b240da360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset Overview\n",
    "\n",
    "In this chapter, we provide a brief overview of the dataset used for analysis. We indicate whether the loaded dataset is the full confidential version or the public sample, report the total number of articles available, and present a first look at the articles data to understand its structure and content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0836948-a298-4a9d-b408-a8f3fd8c97f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Check Dataset Version (Confidential vs Public)\n",
    "\n",
    "Here we check and indicate whether the loaded dataset is the full confidential version or the public sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60a1ebc-6a6d-4e81-bfe5-d956f13f8e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "        return f\"more than {n // 1_000_000} Mio.\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} Tsd.\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the full data set. This contains the full corpus of {format_rowcount(rowcount)} Articles published by SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the publically available (sampled) data set. This contains an excerpt of {format_rowcount(rowcount)} articles within SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7446805d-88cc-41ba-a63e-8b91905fab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Overview of the Data\n",
    "\n",
    "In this step, we provide an overview of the data contained in the loaded dataset. This includes a summary of the available articles and a first look at their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41c756-4f7a-4617-bf25-bf39bed8a164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_row = srgssr_article_corpus.head(1)[0].asDict() if srgssr_article_corpus.head(1) else {}\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": name,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(name, None)\n",
    "    }\n",
    "    for name, dtype in srgssr_article_corpus.dtypes\n",
    "]\n",
    "\n",
    "display(cols_info, [\"column\", \"type\", \"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c53db45-2261-4f45-91ca-1196a86775ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: A Closer Look\n",
    "\n",
    "In this step, we take a deeper look at the loaded dataset, exploring its structure and content in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bfcf1f-6b0c-4056-b427-7f336aad844e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4689788114568825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Data Reading from pdp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
