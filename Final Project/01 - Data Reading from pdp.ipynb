{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fa5320-88b1-4264-b620-74292a18c3e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook processes article data from a Kafka stream and writes the transformed data to a Delta table. The main steps are:\n",
    "\n",
    "1. **Read from Kafka**: The source table `udp_prd_atomic.pdp.articles_v2` is assumed to be populated from a Kafka stream containing article data in a nested structure.\n",
    "\n",
    "2. **Flatten and Transform Data**: A SQL query creates a temporary view `articles_flat` that:\n",
    "   - Extracts and flattens metadata fields (e.g., `id`, `publisher`, `provenance`, dates).\n",
    "   - Selects German (`'de'`) or fallback values for fields like `title`, `lead`, and `kicker`.\n",
    "   - Extracts specific identifiers (e.g., `urn`, `srgId`).\n",
    "   - Aggregates and flattens arrays (e.g., resources, related articles, contributors, keywords) into CSV strings.\n",
    "   - Collects URLs from nested resource fields.\n",
    "   - Preserves Kafka metadata (key, topic, partition, offset, timestamp).\n",
    "\n",
    "3. **Create DataFrame**: The flattened view is loaded into a Spark DataFrame for further processing or inspection.\n",
    "\n",
    "4. **Write to Delta Table**: The DataFrame is appended to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`, enabling efficient storage and downstream analytics.\n",
    "\n",
    "Each step ensures that complex, nested Kafka data is transformed into a flat, analytics-ready format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35abc26-aae0-4cc6-b0b5-4ff0d3bca2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read from Kafka\n",
    "\n",
    "The following steps read article data from the Delta table `udp_prd_atomic.pdp.articles_v2`, which is assumed to be populated from a Kafka stream. This data includes both the article content and Kafka metadata (such as key, topic, partition, offset, and timestamp). The data is then prepared for further transformation and analysis in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0aa6cb20-6266-411c-b68d-02820485cbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW articles_flat AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    key,\n",
    "    topic,\n",
    "    partition,\n",
    "    offset,\n",
    "    timestamp,\n",
    "    value AS v\n",
    "  FROM udp_prd_atomic.pdp.articles_v2\n",
    ")\n",
    "SELECT\n",
    "  -- flache Metadaten\n",
    "  v.id,\n",
    "  v.publisher,\n",
    "  v.provenance,\n",
    "  v.modificationDate,\n",
    "  v.releaseDate,\n",
    "\n",
    "  -- TITLE: erst 'de', sonst erster Eintrag\n",
    "  coalesce(\n",
    "    CASE\n",
    "      WHEN size(filter(coalesce(v.title, array()), x -> x.language = 'de')) > 0\n",
    "        THEN element_at(transform(filter(v.title, x -> x.language = 'de'), x -> x.content), 1)\n",
    "    END,\n",
    "    CASE\n",
    "      WHEN size(coalesce(v.title, array())) > 0\n",
    "        THEN element_at(transform(v.title, x -> x.content), 1)\n",
    "    END\n",
    "  ) AS title_auto,\n",
    "\n",
    "  -- LEAD: erst 'de', sonst erster Eintrag\n",
    "  coalesce(\n",
    "    CASE\n",
    "      WHEN size(filter(coalesce(v.lead, array()), x -> x.language = 'de')) > 0\n",
    "        THEN element_at(transform(filter(v.lead, x -> x.language = 'de'), x -> x.content), 1)\n",
    "    END,\n",
    "    CASE\n",
    "      WHEN size(coalesce(v.lead, array())) > 0\n",
    "        THEN element_at(transform(v.lead, x -> x.content), 1)\n",
    "    END\n",
    "  ) AS lead_auto,\n",
    "\n",
    "  -- KICKER: erst 'de', sonst erster Eintrag\n",
    "  coalesce(\n",
    "    CASE\n",
    "      WHEN size(filter(coalesce(v.kicker, array()), x -> x.language = 'de')) > 0\n",
    "        THEN element_at(transform(filter(v.kicker, x -> x.language = 'de'), x -> x.content), 1)\n",
    "    END,\n",
    "    CASE\n",
    "      WHEN size(coalesce(v.kicker, array())) > 0\n",
    "        THEN element_at(transform(v.kicker, x -> x.content), 1)\n",
    "    END\n",
    "  ) AS kicker_auto,\n",
    "\n",
    "  -- IDs gezielt ziehen (kein Map!)\n",
    "  CASE\n",
    "    WHEN size(filter(coalesce(v.identifiers, array()), x -> x.type = 'urn'   AND x.value IS NOT NULL)) > 0\n",
    "      THEN element_at(transform(filter(v.identifiers, x -> x.type = 'urn'   AND x.value IS NOT NULL), x -> x.value), 1)\n",
    "  END AS id_urn,\n",
    "\n",
    "  CASE\n",
    "    WHEN size(filter(coalesce(v.identifiers, array()), x -> x.type = 'srgId' AND x.value IS NOT NULL)) > 0\n",
    "      THEN element_at(transform(filter(v.identifiers, x -> x.type = 'srgId' AND x.value IS NOT NULL), x -> x.value), 1)\n",
    "  END AS id_srg,\n",
    "\n",
    "  -- Erstes verfügbares Bild\n",
    "  aggregate(\n",
    "    coalesce(v.resources, array()),\n",
    "    CAST(NULL AS STRING),\n",
    "    (acc, r) -> coalesce(acc, r.picture.locator.url),\n",
    "    acc -> acc\n",
    "  ) AS picture_url,\n",
    "\n",
    "  -- relatedArticles -> CSV\n",
    "  concat_ws(',', coalesce(v.relatedArticles, CAST(array() AS ARRAY<STRING>))) AS related_articles_csv,\n",
    "\n",
    "  -- content.text.items -> CSV (Leerzeichen-getrennt)\n",
    "  CASE\n",
    "    WHEN v.content IS NOT NULL\n",
    "      THEN concat_ws(' ', coalesce(v.content.text, CAST(array() AS ARRAY<STRING>)))\n",
    "    ELSE NULL\n",
    "  END AS content_text_csv,\n",
    "\n",
    "  -- contributors -> CSV der Namen\n",
    "  concat_ws(\n",
    "    ', ',\n",
    "    transform(\n",
    "      coalesce(v.contributors, array()),\n",
    "      c -> coalesce(\n",
    "        c.name,\n",
    "        c.agent.person.name,\n",
    "        c.agent.team.name,\n",
    "        c.agent.department.name\n",
    "      )\n",
    "    )\n",
    "  ) AS contributors_csv,\n",
    "\n",
    "  -- alle locator.url aus resources -> CSV\n",
    "  concat_ws(\n",
    "    ',',\n",
    "    filter(\n",
    "      flatten(\n",
    "        transform(\n",
    "          coalesce(v.resources, array()),\n",
    "          r -> array(\n",
    "            r.document.locator.url,\n",
    "            r.picture.locator.url,\n",
    "            r.link.locator.url\n",
    "          )\n",
    "        )\n",
    "      ),\n",
    "      x -> x IS NOT NULL\n",
    "    )\n",
    "  ) AS resources_locator_urls_csv,\n",
    "\n",
    "  -- Keywords -> CSV\n",
    "  concat_ws(',', coalesce(v.keywords, CAST(array() AS ARRAY<STRING>))) AS keywords_csv,\n",
    "\n",
    "  -- Kafka-Metadaten (key sicher als STRING)\n",
    "  CAST(key AS STRING) AS key,\n",
    "  topic,\n",
    "  partition,\n",
    "  offset,\n",
    "  timestamp\n",
    "FROM base;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf8dd3b-2ede-4d29-8400-9538c6d279b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Data Frame\n",
    "\n",
    "The code below runs a Spark SQL query against the temporary view `articles_flat`, loads the result into a Spark DataFrame named `df`, and then displays the DataFrame for inspection. This step materializes the flattened article data so it can be further processed or written to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ecb9f3-66a9-4c6e-930b-cc6006cdc795",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"picture_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"resources_locator_urls_csv\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761544712864}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM articles_flat\")  # or: spark.table(\"articles_flat\")\n",
    "display(df, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aab4d4-2ceb-40e9-baa2-e40589325f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write to Delta Table\n",
    "\n",
    "The following code appends the transformed DataFrame `df` to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`. It writes in **append** mode, uses the **Delta** format, and enables **schema merging** so that any new columns are automatically added to the target table without overwriting existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af2346a-d5db-4dcb-aee8-a49147edffaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"swi_audience_prd.pdp_articles_v2.articles_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5014a3-ac9e-4e75-8b7c-7cac7c2d3ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define pandas timestamp bounds\n",
    "min_ts = pd.Timestamp.min\n",
    "max_ts = pd.Timestamp.max\n",
    "\n",
    "df_filtered = df.filter(\n",
    "    (df[\"releaseDate\"] >= min_ts) & (df[\"releaseDate\"] <= max_ts) &\n",
    "    (df[\"modificationDate\"] >= min_ts) & (df[\"modificationDate\"] <= max_ts)\n",
    ")\n",
    "# save to\n",
    "df_filtered.toPandas().to_csv(\n",
    "    \"/dbfs/FileStore/tmp/export.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9946b0c-8ab3-40c1-a4da-356f80d0a481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manual Step for Data Sharing on GitHub\n",
    "\n",
    "**Step 3 – Download the File**\n",
    "\n",
    "After the export, open the following link in your browser:\n",
    "\n",
    "https://adb-4119964566130471.11.azuredatabricks.net/files/tmp/export.csv\n",
    "\n",
    "→ The CSV file will be downloaded ✅  \n",
    "→ Manually move the file into your GitHub repository folder (e.g., `data/`)  \n",
    "→ Commit & Push"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6337301399684818,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Data Reading from pdp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
