{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "919d6a04-380d-4219-a636-d1ed27effbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Acquisition (ETL)\n",
    "\n",
    "**Starting Point:**  \n",
    "The article data originates from a Kafka datastream. It is not normalized (so it cannot be analyzed directly) and requires Active Directory login (making collaboration difficult).  \n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "- **Processing steps:**  \n",
    "  1. Read article data from the Delta table populated from Kafka.\n",
    "  2. Flatten and transform nested fields (e.g., titles, resources, contributors) using a SQL view.\n",
    "  3. Create a Spark DataFrame from the flattened view and inspect the results.\n",
    "  4. Write the DataFrame to a Delta table for analytics and automation.\n",
    "  5. Export a <25MB Parquet sample with only public data for sharing (e.g., via GitHub).\n",
    "\n",
    "**Goal:**  \n",
    "The data should be available as a Parquet file for sharing. Since the dataset is large (5GB), only a public sample is exported for easy distribution.\n",
    "\n",
    "**Access Control:**  \n",
    "To guarantee data integrity and protect sensitive information, data distribution is based on user access rights. Entitled users can access the full confidential dataset, while restricted users are provided with only the public sample. This ensures that only authorized users can view sensitive data, maintaining compliance and data security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248a9111-c12c-440e-ade5-addc5f3baa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Run only once at the beginning of the analysis\n",
    "# (to install all required libraries for translation and embeddings)\n",
    "\n",
    "%pip install pandas==2.1.4\n",
    "%pip install numpy==1.26.4\n",
    "%pip install pyarrow==14.0.1\n",
    "%pip install pyspark==4.0.0\n",
    "\n",
    "%pip install sentence-transformers==5.1.2\n",
    "%pip install transformers==4.48.1\n",
    "%pip install torch==2.7.0\n",
    "%pip install sentencepiece==0.1.99\n",
    "%pip install safetensors==0.3.1\n",
    "%pip install accelerate==0.24.0\n",
    "\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install scipy==1.12.1\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b90172e-2da5-4c57-a083-4b8a465d659a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def has_read_permission(table_name):\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "has_read_access_udp_articles_v2 = has_read_permission(\"udp_prd_atomic.pdp.articles_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35abc26-aae0-4cc6-b0b5-4ff0d3bca2ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-01: Read from Kafka (SQL)\n",
    "\n",
    "The following steps read article data from the Delta table `udp_prd_atomic.pdp.articles_v2`, which is assumed to be populated from a Kafka stream. The original Kafka data contains nested lists and complex structures (e.g., for multilingual fields or arrays of resources). In the transformation, the SQL view `articles_flat` flattens this nested data by extracting relevant fields and, where multiple values exist (such as for titles in different languages), selects the first available entryâ€”typically prioritizing German (`'de'`) or otherwise the first value. This process prepares the data, along with Kafka metadata (key, topic, partition, offset, timestamp), for further analysis in a flat, tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0aa6cb20-6266-411c-b68d-02820485cbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Only run main logic if has_read_access_udp_articles_v2 is true\n",
    "\n",
    "-- Uncomment below to run only when has_read_access_udp_articles_v2 = true\n",
    "-- IF has_read_access_udp_articles_v2 THEN\n",
    "CREATE OR REPLACE TEMP VIEW articles_flat AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    key,\n",
    "    topic,\n",
    "    partition,\n",
    "    offset,\n",
    "    timestamp,\n",
    "    value AS v\n",
    "  FROM udp_prd_atomic.pdp.articles_v2\n",
    ")\n",
    "SELECT\n",
    "  -- flat metadata\n",
    "  v.id AS id,\n",
    "  v.publisher AS publisher,\n",
    "  v.provenance AS provenance,\n",
    "  v.modificationDate AS modificationDate,\n",
    "  v.releaseDate AS releaseDate,\n",
    "  -- TITLE: always first entry\n",
    "  try_element_at(transform(coalesce(v.title, array()), x -> x.content), 1) AS title_auto,\n",
    "  -- LEAD: always first entry\n",
    "  try_element_at(transform(coalesce(v.lead, array()), x -> x.content), 1) AS lead_auto,\n",
    "  -- KICKER: always first entry\n",
    "  try_element_at(transform(coalesce(v.kicker, array()), x -> x.content), 1) AS kicker_auto,\n",
    "  -- Extract IDs specifically (not a Map!)\n",
    "  try_element_at(\n",
    "    transform(\n",
    "      filter(coalesce(v.identifiers, array()), x -> x.type = 'urn'   AND x.value IS NOT NULL),\n",
    "      x -> x.value\n",
    "    ),\n",
    "    1\n",
    "  ) AS id_urn,\n",
    "  try_element_at(\n",
    "    transform(\n",
    "      filter(coalesce(v.identifiers, array()), x -> x.type = 'srgId' AND x.value IS NOT NULL),\n",
    "      x -> x.value\n",
    "    ),\n",
    "    1\n",
    "  ) AS id_srg,\n",
    "  -- First available image\n",
    "  aggregate(\n",
    "    coalesce(v.resources, array()),\n",
    "    CAST(NULL AS STRING),\n",
    "    (acc, r) -> coalesce(acc, r.picture.locator.url),\n",
    "    acc -> acc\n",
    "  ) AS picture_url,\n",
    "  -- content.text.items -> CSV (space-separated)\n",
    "  CASE\n",
    "    WHEN v.content IS NOT NULL\n",
    "      THEN concat_ws(' ', coalesce(v.content.text, CAST(array() AS ARRAY<STRING>)))\n",
    "    ELSE NULL\n",
    "  END AS content_text_csv,\n",
    "  -- contributors -> CSV of names\n",
    "  CASE\n",
    "    WHEN v.contributors IS NOT NULL\n",
    "      THEN concat_ws(\n",
    "        ', ',\n",
    "        transform(\n",
    "          coalesce(v.contributors, array()),\n",
    "          c -> coalesce(\n",
    "            c.name,\n",
    "            c.agent.person.name,\n",
    "            c.agent.team.name,\n",
    "            c.agent.department.name\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "    ELSE NULL\n",
    "  END AS contributors_csv,\n",
    "  -- only first locator.url from resources -> CSV\n",
    "  try_element_at(\n",
    "    filter(\n",
    "      flatten(\n",
    "        transform(\n",
    "          coalesce(v.resources, array()),\n",
    "          r -> array(\n",
    "            r.document.locator.url,\n",
    "            r.picture.locator.url,\n",
    "            r.link.locator.url\n",
    "          )\n",
    "        )\n",
    "      ),\n",
    "      x -> x IS NOT NULL\n",
    "    ),\n",
    "    1\n",
    "  ) AS resources_locator_urls_csv,\n",
    "  -- Keywords -> CSV\n",
    "  CASE\n",
    "    WHEN v.keywords IS NOT NULL\n",
    "      THEN concat_ws(',', coalesce(v.keywords, CAST(array() AS ARRAY<STRING>)))\n",
    "    ELSE NULL\n",
    "  END AS keywords_csv,\n",
    "  -- Kafka metadata (key safely as STRING)\n",
    "  CAST(key AS STRING) AS key,\n",
    "  topic,\n",
    "  partition,\n",
    "  offset,\n",
    "  timestamp\n",
    "FROM base;\n",
    "-- END IF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf8dd3b-2ede-4d29-8400-9538c6d279b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-02: Create DataFrame and Visually Inspect Results\n",
    "\n",
    "The code below runs a Spark SQL query against the temporary view `articles_flat`, loads the result into a Spark DataFrame named `df`, and then displays the DataFrame for visual inspection. This step materializes the flattened article data so it can be further processed or written to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ecb9f3-66a9-4c6e-930b-cc6006cdc795",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"picture_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"resources_locator_urls_csv\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761544712864}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    articles_flat = spark.sql(\"SELECT * FROM articles_flat\").orderBy(\"modificationDate\", ascending=False)\n",
    "    display(articles_flat, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e96a63a-ade2-4735-a697-bd4f3e60ea82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step-03: Save Data to Delta Table\n",
    "\n",
    "In the next steps, the data will be saved both to a Delta table (for better automation and analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06aab4d4-2ceb-40e9-baa2-e40589325f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to (private) Delta Table - all articles\n",
    "\n",
    "The following code appends the transformed DataFrame `df` to the Delta table `swi_audience_prd.pdp_articles_v2.articles_v2`. It writes in **append** mode, uses the **Delta** format, and enables **schema merging** so that any new columns are automatically added to the target table without overwriting existing data.\n",
    "\n",
    "- Contains all articles (**confidential**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b24b138-b0bd-4dff-9e48-35481b053d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    articles_flat.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"swi_audience_prd.pdp_articles_v2.articles_v2_flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a6e6098-965f-4ad5-b8aa-79264468c1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Write to (public) Parquet File - selected articles, manually Upload to GitHub\n",
    "\n",
    "Export a <25 MB sample of the data with only public data as a Parquet file for easy sharing via GitHub.  \n",
    "**Note:** The Parquet file must be manually downloaded from Databricks and then uploaded to your GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6689c36-f482-477e-8f2b-8461f6f105d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if has_read_access_udp_articles_v2:\n",
    "    from pyspark.sql.functions import col, current_date\n",
    "    import math\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    # Estimate average row size in bytes using a sample of 100,000 rows\n",
    "    sample_row = articles_flat.limit(100000).toPandas()\n",
    "    avg_row_size = sample_row.memory_usage(index=True, deep=True).sum() / len(sample_row)\n",
    "\n",
    "    # Set maximum file size to 25 MB\n",
    "    max_bytes = 25 * 1024 * 1024  # 25 MB\n",
    "\n",
    "    # Parquet is more efficient than CSV, so increase row count estimate by a factor of 2\n",
    "    max_rows = int(math.floor((max_bytes / avg_row_size) * 2))\n",
    "\n",
    "    # Filter articles to only those published (releaseDate today or earlier), then sample up to max_rows\n",
    "    articles_flat_sampled = (\n",
    "        articles_flat\n",
    "        .filter(col(\"releaseDate\") <= current_date())\n",
    "        .limit(max_rows)\n",
    "    )\n",
    "\n",
    "    # Write the sampled DataFrame as a single Parquet file to a temporary directory\n",
    "    parquet_temp_dir = \"/Volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume/export_articles_v2_sample25mb_tmp_parquet\"\n",
    "    parquet_final_path = \"/Volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume/export_articles_v2_sample25mb.parquet\"\n",
    "\n",
    "    (articles_flat_sampled.coalesce(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(parquet_temp_dir)\n",
    "    )\n",
    "\n",
    "    # Move the single Parquet part file to the final destination\n",
    "    files = [f for f in os.listdir(parquet_temp_dir) if f.endswith(\".parquet\")]\n",
    "    if files:\n",
    "        src_file = os.path.join(parquet_temp_dir, files[0])\n",
    "        shutil.move(src_file, parquet_final_path)\n",
    "\n",
    "    # Remove the temporary directory after moving the file\n",
    "    shutil.rmtree(parquet_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5409a74-7842-49e5-9a43-56626ee9ea3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "...now manually:\n",
    "\n",
    "1. **Open the CSV file in Databricks:**\n",
    "   - Navigate to [Databricks Volume Browser](https://adb-4119964566130471.11.azuredatabricks.net/explore/data/volumes/swi_audience_prd/pdp_articles_v2/pdp_articles_v2_volume?o=4119964566130471) in the Databricks workspace file browser.\n",
    "\n",
    "2. **Download the file:**\n",
    "   - Right-click on `export_articles_v2_sample25mb.parquet` and select **\"Download\"** to save the file to your local machine.\n",
    "\n",
    "3. **Upload the file to GitHub:**\n",
    "   - Go to [GitHub Folder](https://github.com/Tao-Pi/CAS-Applied-Data-Science/tree/main/Module-3/01_Module%20Final%20Assignment).\n",
    "   - Click **\"Add file\"** > **\"Upload files\"**.\n",
    "   - Drag and drop `export_articles_v2_sample25mb.parquet` or use the file picker to select it.\n",
    "   - Commit the changes to upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca686f8-2f86-434b-9576-c57c9d1b8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step-04: Load Data Based on User Rights\n",
    "\n",
    "The next step is to load the data, with access determined by user rights:\n",
    "\n",
    "- **Restricted users** can load only the public data sample (e.g., the Parquet file exported for sharing).\n",
    "- **Entitled users** can load the full, confidential dataset from the Delta table.\n",
    "\n",
    "This ensures that sensitive information is only accessible to authorized users, while still allowing broader access to public data for collaboration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0706526-3796-466a-99d7-7ab6b3ff5026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Restricted users \n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url)\n",
    "\n",
    "# Entitled users\n",
    "if has_read_access_udp_articles_v2:\n",
    "    srgssr_article_corpus = spark.table(\"swi_audience_prd.pdp_articles_v2.articles_v2_flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cecfdc-221b-4fca-b2d5-a3394395d892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "    srgssr_article_corpus = pd.DataFrame(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68d121c8-4b18-4d6e-827d-7d9b240da360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset Overview\n",
    "\n",
    "In this chapter, we provide a brief overview of the dataset used for analysis. We indicate whether the loaded dataset is the full confidential version or the public sample, report the total number of articles available, and present a first look at the articles data to understand its structure and content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0836948-a298-4a9d-b408-a8f3fd8c97f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Check Dataset Version (Confidential vs Public)\n",
    "\n",
    "Here we check and indicate whether the loaded dataset is the full confidential version or the public sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60a1ebc-6a6d-4e81-bfe5-d956f13f8e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "        return f\"more than {n // 1_000_000} Mio.\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} Tsd.\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the full data set. This contains the full corpus of {format_rowcount(rowcount)} Articles published by SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the publically available (sampled) data set. This contains an excerpt of {format_rowcount(rowcount)} articles within SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7446805d-88cc-41ba-a63e-8b91905fab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2: Overview of the Data\n",
    "\n",
    "In this step, we provide an overview of the data contained in the loaded dataset. This includes a summary of the available articles and a first look at their structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41c756-4f7a-4617-bf25-bf39bed8a164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_row = srgssr_article_corpus.head(1)[0].asDict() if srgssr_article_corpus.head(1) else {}\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": name,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(name, None)\n",
    "    }\n",
    "    for name, dtype in srgssr_article_corpus.dtypes\n",
    "]\n",
    "\n",
    "display(cols_info, [\"column\", \"type\", \"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c53db45-2261-4f45-91ca-1196a86775ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: A Closer Look\n",
    "\n",
    "In this step, we take a deeper look at the loaded dataset, exploring its structure and content in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23bfcf1f-6b0c-4056-b427-7f336aad844e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"picture_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"resources_locator_urls_csv\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1761747471537}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13bc94dd-bc0a-4798-b51d-68d805ad6b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Analyses\n",
    "This is where analyses are performed. This is work in progress. Some ideas:\n",
    "\n",
    "**Story 1:** I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Story 2:** I want to find out what topics SRG writes about â€“ this could, for example, be used for navigation (News / Sport / etc.).\n",
    "\n",
    "**Story 3:** I want to translate all existing articles into all languages used. This way, I can multiply the offer easily. Instead of having some articles in French and some in English, I will have all articles available in all of our 11 languages.\n",
    "\n",
    "*List other ideas here...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab85f79-57a4-4286-8fbe-1d7cf24b9453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Sample the article corpus to a manageable size (1000 rows) for faster\n",
    "# prototyping and to avoid OOM issues during embedding / clustering.\n",
    "# This works for both Spark DataFrames and pandas DataFrames.\n",
    "# ----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "if not isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "    # Force convert Spark DataFrame to pandas DataFrame\n",
    "    srgssr_article_corpus = srgssr_article_corpus.limit(100).toPandas()\n",
    "else:\n",
    "    # pandas: take the first 100 rows (or fewer if the DataFrame is smaller)\n",
    "    srgssr_article_corpus = srgssr_article_corpus.head(100)\n",
    "\n",
    "display(srgssr_article_corpus, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43080368-8370-42dd-8c45-da9b3b2684fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## USE CASE: Quickly Search All Existing Articles\n",
    "\n",
    "I want to quickly search all existing articles without the need to use Google. I want to do that because when I write a story, I want to make sure the same story was not just written by my colleagues working in a different branch.\n",
    "\n",
    "**Approach:**\n",
    "- Implement a semantic search feature within Databricks that allows users to search articles by keywords, phrases, or topics.\n",
    "- Use text embeddings (e.g., with Sentence Transformers) to represent article content and enable similarity-based search.\n",
    "- Provide a simple search interface where users can enter queries and retrieve the most relevant articles.\n",
    "- Optionally, add filters for date, author, or branch to refine search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45aadfc-30df-4850-8189-86807f91fc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load or reuse the embedder model\n",
    "def get_embedder():\n",
    "    if not hasattr(get_embedder, \"model\"):\n",
    "        get_embedder.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return get_embedder.model\n",
    "\n",
    "def add_embeddings(df):\n",
    "    if \"content_emb\" not in df.columns:\n",
    "        model = get_embedder()\n",
    "        df[\"content_emb\"] = model.encode(df[\"content_text_csv\"].toPandas()[\"content_text_csv\"].tolist(), show_progress_bar=False, batch_size=64).tolist()\n",
    "    return df\n",
    "\n",
    "def to_vector(arr):\n",
    "    return np.array(arr)\n",
    "\n",
    "srgssr_article_corpus = add_embeddings(srgssr_article_corpus)\n",
    "srgssr_article_corpus[\"content_vec\"] = srgssr_article_corpus[\"content_emb\"].apply(to_vector)\n",
    "\n",
    "def semantic_search(query, top_k=10):\n",
    "    model = get_embedder()\n",
    "    query_emb = model.encode([query])[0]\n",
    "    query_vec = np.array(query_emb)\n",
    "    def cosine_similarity(vec):\n",
    "        return float(np.dot(vec, query_vec) / (np.linalg.norm(vec) * np.linalg.norm(query_vec) + 1e-10))\n",
    "    srgssr_article_corpus[\"similarity\"] = srgssr_article_corpus[\"content_vec\"].apply(cosine_similarity)\n",
    "    results = srgssr_article_corpus.sort_values(\"similarity\", ascending=False).head(top_k)[[\"id\", \"content_text_csv\", \"similarity\"]]\n",
    "    display(results)\n",
    "\n",
    "# Example usage: search for a topic/phrase/keyword\n",
    "semantic_search(\"climate change\", top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ece778-f9c3-469f-b11d-cab0530ec474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##USE CASE: find out what topics SRG writes about.\n",
    "**Approach:**  \n",
    "- Read the text from the `content_text_csv` column of the articles.\n",
    "- Compute similarity between article contents, e.g., by embedding the texts and using a Random Forest or other clustering/classification methods to group similar articles.\n",
    "- Identify clusters of similar content to reveal common topics or themes.\n",
    "- Use these clusters to enhance navigation and filtering options for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852edb45-cb44-4603-86a7-e0fbd9a2e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Assume 'articles_with_emb' DataFrame exists and has 'content_vec' and a label column 'label_col'\n",
    "# If no label column exists, create a dummy one for demonstration\n",
    "if \"label_col\" not in articles_with_emb.columns:\n",
    "    from pyspark.sql.functions import monotonically_increasing_id\n",
    "    articles_with_emb = articles_with_emb.withColumn(\"label_col\", (monotonically_increasing_id() % 2).cast(\"double\"))\n",
    "\n",
    "# Index label column if not already numeric\n",
    "label_indexer = StringIndexer(inputCol=\"label_col\", outputCol=\"label\")\n",
    "df_indexed = label_indexer.fit(articles_with_emb).transform(articles_with_emb)\n",
    "\n",
    "# Assemble features (already in 'content_vec')\n",
    "assembler = VectorAssembler(inputCols=[\"content_vec\"], outputCol=\"features\")\n",
    "df_features = assembler.transform(df_indexed)\n",
    "\n",
    "# Split into train/test\n",
    "train, test = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Random Forest\n",
    "clf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=20, seed=42)\n",
    "model = clf.fit(train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Random Forest accuracy:\", accuracy)\n",
    "print(\"Random Forest params:\", clf.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8f933a-3713-4069-b863-3937476d9563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For quick prototyping, we sample only 1000 articles here.\n",
    "# To run on the full dataset, remove the .limit(1000) line below.\n",
    "df_sample = srgssr_article_corpus.limit(1000)\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Load model once per executor for efficiency\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embedder():\n",
    "    if not hasattr(get_embedder, \"model\"):\n",
    "        get_embedder.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return get_embedder.model\n",
    "\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def embed_udf(texts: pd.Series) -> pd.Series:\n",
    "    model = get_embedder()\n",
    "    return pd.Series(model.encode(texts.tolist(), show_progress_bar=False, batch_size=64).tolist())\n",
    "\n",
    "# Add embeddings column (no translation)\n",
    "df_emb = df_sample.withColumn(\n",
    "    \"content_emb\", embed_udf(col(\"content_text_csv\"))\n",
    ")\n",
    "\n",
    "# Convert array to Spark Vector (required by ML algorithms)\n",
    "to_vector = pandas_udf(lambda arr: [Vectors.dense(x) if x else Vectors.dense([]) for x in arr], returnType=VectorUDT())\n",
    "df_vec = df_emb.withColumn(\"features\", to_vector(col(\"content_emb\")))\n",
    "\n",
    "# KMeans clustering (choose number of clusters, e.g., 10)\n",
    "k = 10\n",
    "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=k, seed=42)\n",
    "model = kmeans.fit(df_vec)\n",
    "\n",
    "# Assign cluster IDs\n",
    "df_clustered = model.transform(df_vec).select(\"id\", \"cluster\", \"content_text_csv\")\n",
    "\n",
    "# Show a sample of clustered articles\n",
    "display(df_clustered.orderBy(\"cluster\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1fa3228-64a0-48c5-a251-143717b3555d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## USE CASE: Translate All Existing Articles into All Languages Used\n",
    "\n",
    "Goal: Automatically translate every article into all supported languages, so each article is available in every language used by SRG.\n",
    "\n",
    "**Approach:**\n",
    "- For each article, use the `ai_translate` function to generate translations for all target languages.\n",
    "- Store the translated articles alongside the originals for easy access and analytics.\n",
    "\n",
    "**Example (SQL):**\n",
    "sql\n",
    "SELECT\n",
    "  *,\n",
    "  ai_translate(title, 'fr') AS title_fr,\n",
    "  ai_translate(title, 'it') AS title_it,\n",
    "  ai_translate(title, 'en') AS title_en,\n",
    "  ai_translate(title, 'rm') AS title_rm\n",
    "FROM articles_flat\n",
    "\n",
    "*(Repeat for all relevant text fields and all required languages.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50182740-9fdf-4f7c-94bb-ffacb465ffdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume ai_translate(text: str, target_lang: str) -> str is defined and available\n",
    "\n",
    "from pyspark.sql.functions import col, struct, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "target_languages = [\"en\", \"fr\", \"it\", \"de\"]  # Example target languages\n",
    "\n",
    "def make_translate_udf(lang):\n",
    "    @udf(StringType())\n",
    "    def translate_udf(text):\n",
    "        return ai_translate(text, lang)\n",
    "    return translate_udf\n",
    "\n",
    "df = srgssr_article_corpus\n",
    "\n",
    "for lang in target_languages:\n",
    "    df = df.withColumn(f\"content_text_csv_{lang}\", make_translate_udf(lang)(col(\"content_text_csv\")))\n",
    "\n",
    "# Store the DataFrame with translations alongside originals\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"swi_audience_prd.pdp_articles_v2.articles_v2_translated\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f68fcc40-d6f9-4967-a565-2cf481d3de78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Falls noch nicht installiert+neu gestartet:\n",
    "# %pip install -U transformers sentencepiece torch safetensors accelerate\n",
    "# dbutils.library.restartPython()\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 20 Zeilen holen (nur nÃ¶tige Spalten)\n",
    "pdf = (\n",
    "    srgssr_article_corpus\n",
    "    .select(\"id\", \"content_text_csv\")\n",
    "    .limit(20)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# 2) Pipeline lokal laden (CPU)\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", device=-1)\n",
    "\n",
    "# 3) Ãœbersetzen (Batch pro Text-Chunk â€“ hier simpel ohne Chunking)\n",
    "def translate_text(t):\n",
    "    if t is None or not str(t).strip():\n",
    "        return None\n",
    "    res = translator(t, truncation=True, max_length=1000)\n",
    "    return res[0][\"translation_text\"]\n",
    "\n",
    "pdf[\"content_text_csv_english\"] = [translate_text(t) for t in pdf[\"content_text_csv\"]]\n",
    "\n",
    "# 4) ZurÃ¼ck nach Spark (nur fÃ¼r Anzeige)\n",
    "df_test = spark.createDataFrame(pdf)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d0b0b3a-d8b3-4922-a5a6-f77f8e37dd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# ðŸ§ª Nur 20 Zeilen nehmen\n",
    "df_sample = srgssr_article_corpus.limit(20)\n",
    "\n",
    "# ðŸ”„ Ãœbersetzungsspalte hinzufÃ¼gen\n",
    "df_test = df_sample.withColumn(\n",
    "    \"content_text_csv_english\",\n",
    "    translate_series_to_en(F.col(\"content_text_csv\"))\n",
    ")\n",
    "\n",
    "# ðŸ‘€ Ergebnis anzeigen\n",
    "df_test.select(\"id\", \"content_text_csv\", \"content_text_csv_english\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24b220d3-50ac-4a2d-b7c6-70eed00e4561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Hints and Notes\n",
    "> **Hint:**  \n",
    "> For a temporary Jupyter environment to experiment or explore data, consider using [RenkuLab](https://renkulab.io/p/snsf-anoxia-project/proxy-proxy/sessions/01JX2TG1RZ9J0PQ53H3RT81BD4/start). RenkuLab offers cloud-based notebook sessionsâ€”no local setup required.\n",
    "\n",
    "> **Note:**  \n",
    "> RenkuLab sessions may require authentication and have limited resources. Save your work frequently, as sessions can time out or be terminated after inactivity."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4689788114568825,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 - Data Reading from pdp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
