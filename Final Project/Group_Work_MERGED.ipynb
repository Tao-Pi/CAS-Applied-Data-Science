{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7fb1f9",
   "metadata": {},
   "source": [
    "# 1. Introduction and Data Acquisition\n",
    "\n",
    "**Starting Point:**  \n",
    "The article data originates from a Kafka datastream. It is not normalized (so it cannot be analyzed directly) and requires Active Directory login (making collaboration difficult).  \n",
    "- [View Kafka topic data (AKHQ)](https://akhq.pdp.production.admin.srgssr.ch/ui/strimzi/topic/articles-v2/data?sort=NEWEST&partition=All)\n",
    "\n",
    "**Processing steps:**  \n",
    "1. Read article data from the Delta table populated from Kafka.\n",
    "2. Flatten and transform nested fields (e.g., titles, resources, contributors) using a SQL view.\n",
    "3. Create a Spark DataFrame from the flattened view and inspect the results.\n",
    "4. Write the DataFrame to a Delta table for analytics and automation.\n",
    "5. Export a <25MB Parquet sample with only public data for sharing (e.g., via GitHub).\n",
    "\n",
    "**Goal:**  \n",
    "The data should be available as a Parquet file for sharing. Since the dataset is large (5GB), only a public sample is exported for easy distribution.\n",
    "\n",
    "**Access Control:**  \n",
    "To guarantee data integrity and protect sensitive information, data distribution is based on user access rights. Entitled users can access the full confidential dataset, while restricted users are provided with only the public sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0dc52f",
   "metadata": {},
   "source": [
    "## 1.1 Installation and Setup\n",
    "\n",
    "Install required packages for the complete analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas pyarrow fastparquet\n",
    "%pip install sentence-transformers tf-keras\n",
    "%pip install scikit-learn matplotlib umap-learn\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "%pip install transformers accelerate torch bertopic hf-transfer seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df07aa",
   "metadata": {},
   "source": [
    "## 1.2 Load Data\n",
    "\n",
    "Load article data from the public Parquet file hosted on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58355b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url, engine=\"fastparquet\")\n",
    "has_read_access_udp_articles_v2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27a93a",
   "metadata": {},
   "source": [
    "# 2. Dataset Overview\n",
    "\n",
    "In this section, we provide a comprehensive overview of the dataset, including:\n",
    "- Dataset version (confidential vs public)\n",
    "- Total number of articles\n",
    "- Data structure and schema\n",
    "- Sample data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7179ebf",
   "metadata": {},
   "source": [
    "## 2.1 Check Dataset Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ead78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the full data set. This contains the full corpus of {format_rowcount(rowcount)} Articles published by SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"congrats: you have successfully read the publically available (sampled) data set. This contains an excerpt of {format_rowcount(rowcount)} articles within SRG-SSR as plain text together with some relevant metadata. You can access the dataframe object by calling 'srgssr_article_corpus' from Python now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853d876",
   "metadata": {},
   "source": [
    "## 2.2 Data Structure and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column information\n",
    "first_row = srgssr_article_corpus.iloc[0].to_dict() if not srgssr_article_corpus.empty else {}\n",
    "\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": col,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(col, None)\n",
    "    }\n",
    "    for col, dtype in srgssr_article_corpus.dtypes.items()\n",
    "]\n",
    "\n",
    "pd.DataFrame(cols_info).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5dd1d",
   "metadata": {},
   "source": [
    "## 2.3 Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56a09",
   "metadata": {},
   "source": [
    "## 2.4 Limit Dataset for Analysis\n",
    "\n",
    "For demonstration purposes, we'll work with the first 1000 articles to ensure reasonable processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgssr_article_corpus = srgssr_article_corpus.head(1000)\n",
    "print(f\"Working with {len(srgssr_article_corpus)} articles for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2fbdf",
   "metadata": {},
   "source": [
    "# 3. Semantic Search Implementation\n",
    "\n",
    "**Use Case:** Quickly search all existing articles without needing Google.\n",
    "\n",
    "**Goal:** Enable journalists to verify if a story has already been written by colleagues in different branches.\n",
    "\n",
    "**Approach:**\n",
    "- Use text embeddings (Sentence Transformers) to represent article content\n",
    "- Enable similarity-based semantic search\n",
    "- Return most relevant articles for any query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TEXT_COL = \"content_text_csv\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "# Prepare data\n",
    "df = srgssr_article_corpus.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Initialize embedding model\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "print(\"Creating embeddings for semantic search...\")\n",
    "model = get_embedder()\n",
    "emb_matrix = model.encode(\n",
    "    df[TEXT_COL].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "ids = df[ID_COL].tolist()\n",
    "texts = df[TEXT_COL].tolist()\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Search for semantically similar articles\"\"\"\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = emb_matrix @ q\n",
    "    top_idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i] for i in top_idx],\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n",
    "# Example search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Example: Searching for 'climate change' articles\")\n",
    "print(\"=\"*80)\n",
    "results = semantic_search(\"climate change\", top_k=10)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b28fa",
   "metadata": {},
   "source": [
    "# 4. Topic Clustering Analysis (K-means)\n",
    "\n",
    "**Use Case:** Discover what topics SRG writes about to improve navigation and content organization.\n",
    "\n",
    "**Approach:**\n",
    "- Use K-means clustering on article embeddings\n",
    "- Extract representative keywords for each cluster\n",
    "- Visualize topic distribution in 2D space using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 10\n",
    "print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "labels = kmeans.fit_predict(emb_matrix)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"content_text_csv\": texts,\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "# Extract topic keywords for each cluster\n",
    "def get_topic_keywords(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['content_text_csv'].tolist()\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    words = re.findall(r'\\b[a-zäöüàéèêëïôùû]{4,}\\b', combined_text)\n",
    "    \n",
    "    stopwords = {'dass', 'sind', 'wird', 'wurden', 'wurde', 'haben', 'sein', \n",
    "                 'eine', 'einem', 'einen', 'einer', 'dies', 'diese', 'dieser',\n",
    "                 'auch', 'mehr', 'beim', 'über', 'nach', 'sich', 'oder', 'kann',\n",
    "                 'können', 'müssen', 'soll', 'sollen', 'noch', 'bereits', 'aber',\n",
    "                 'wenn', 'weil', 'denn', 'dann', 'sowie', 'dass', 'damit', 'with',\n",
    "                 'from', 'have', 'this', 'that', 'will', 'been', 'were', 'their',\n",
    "                 'what', 'which', 'when', 'where', 'there', 'pour', 'dans', 'avec',\n",
    "                 'sont', 'être', 'cette', 'mais', 'plus', 'comme', 'fait'}\n",
    "    \n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    word_counts = Counter(words)\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels\n",
    "topic_labels = {}\n",
    "print(\"\\nCluster Topics (K-means, based on most frequent keywords):\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in range(n_clusters):\n",
    "    keywords = get_topic_keywords(cluster_id, df_clusters, top_n=3)\n",
    "    topic_labels[cluster_id] = keywords\n",
    "    count = len(df_clusters[df_clusters['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {keywords} ({count} articles)\")\n",
    "\n",
    "df_clusters['cluster_topic'] = df_clusters['cluster'].map(topic_labels)\n",
    "\n",
    "print(\"\\n\")\n",
    "display(df_clusters.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77b455",
   "metadata": {},
   "source": [
    "## 4.1 Visualize K-means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd81ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "print(\"Creating UMAP projection for visualization...\")\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(emb_matrix)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d[:, 0], \n",
    "    embedding_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('K-means Topic Clusters Visualization (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers with labels\n",
    "kmeans_centers_2d = reducer.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d[:, 0], \n",
    "    kmeans_centers_2d[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "for cluster_id in range(n_clusters):\n",
    "    x, y = kmeans_centers_2d[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"\\nK-means Cluster Distribution:\")\n",
    "cluster_counts = df_clusters['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id} ({topic_labels[cluster_id]}): {count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca45d79",
   "metadata": {},
   "source": [
    "# 5. Translation Pipeline\n",
    "\n",
    "**Use Case:** Translate all existing articles into all supported languages.\n",
    "\n",
    "**Goal:** Multiply content availability by making every article accessible in all 11 languages used by SRG.\n",
    "\n",
    "**Approach:**\n",
    "- Use Google Translate API (googletrans) for demonstration\n",
    "- Translate articles to English as target language\n",
    "- Handle rate limiting and errors gracefully\n",
    "\n",
    "**Note:** In a Databricks environment, you would use the `ai_translate()` function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "df_translated = srgssr_article_corpus.copy()\n",
    "\n",
    "def translate_text(text, dest='en', max_retries=3):\n",
    "    \"\"\"Translate text to target language with retry logic\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text_str = str(text)[:5000]  # Limit to 5000 characters\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translator.translate(text_str, dest=dest)\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Translation failed after {max_retries} attempts: {str(e)[:100]}\")\n",
    "                return text_str\n",
    "    \n",
    "    return text_str\n",
    "\n",
    "# Translate articles\n",
    "print(\"Translating articles to English...\")\n",
    "print(f\"Total articles to translate: {len(df_translated)}\")\n",
    "print(\"Note: This may take several minutes. Adding delays to avoid rate limiting.\\n\")\n",
    "\n",
    "translated_texts = []\n",
    "for idx, text in enumerate(df_translated['content_text_csv']):\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Progress: {idx}/{len(df_translated)} articles translated...\")\n",
    "    \n",
    "    translated = translate_text(text, dest='en')\n",
    "    translated_texts.append(translated)\n",
    "    \n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "df_translated['content_text_en'] = translated_texts\n",
    "\n",
    "print(f\"\\n✅ Translation complete! Translated {len(df_translated)} articles.\")\n",
    "print(\"\\nShowing first 3 translated articles:\")\n",
    "display(df_translated[['id', 'content_text_csv', 'content_text_en']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b377f",
   "metadata": {},
   "source": [
    "# 6. Enhanced Topic Categorization\n",
    "\n",
    "After translation, we can perform more sophisticated topic analysis on the English text.\n",
    "This section maps clusters to higher-level topic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4008616",
   "metadata": {},
   "source": [
    "## 6.1 Cluster Translated Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating embeddings for translated English articles...\")\n",
    "\n",
    "df_en = df_translated.copy()\n",
    "df_en['content_text_en'] = df_en['content_text_en'].fillna(\"\").astype(str)\n",
    "\n",
    "# Create embeddings for English text\n",
    "model_en = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb_matrix_en = model_en.encode(\n",
    "    df_en['content_text_en'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "n_clusters_en = 10\n",
    "kmeans_en = KMeans(n_clusters=n_clusters_en, random_state=42, n_init=\"auto\")\n",
    "labels_en = kmeans_en.fit_predict(emb_matrix_en)\n",
    "\n",
    "df_clusters_en = pd.DataFrame({\n",
    "    \"id\": df_en['id'].tolist(),\n",
    "    \"original_text\": df_en['content_text_csv'].tolist(),\n",
    "    \"translated_text_en\": df_en['content_text_en'].tolist(),\n",
    "    \"cluster\": labels_en\n",
    "})\n",
    "\n",
    "# Extract English keywords\n",
    "def get_topic_keywords_en(cluster_id, df_clusters, top_n=3):\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['translated_text_en'].tolist()\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "    \n",
    "    stopwords = {\n",
    "        'this', 'that', 'with', 'from', 'have', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'will', 'would', 'could',\n",
    "        'should', 'about', 'after', 'also', 'many', 'more', 'most', 'other',\n",
    "        'some', 'such', 'than', 'them', 'then', 'these', 'they', 'very',\n",
    "        'into', 'just', 'like', 'only', 'over', 'said', 'same', 'says',\n",
    "        'does', 'make', 'made', 'well', 'much', 'even', 'back', 'through',\n",
    "        'year', 'years', 'being', 'people', 'according', 'since', 'during'\n",
    "    }\n",
    "    \n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(words)\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "topic_labels_en = {}\n",
    "print(\"\\nCluster Topics (based on English translated text):\")\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = get_topic_keywords_en(cluster_id, df_clusters_en, top_n=3)\n",
    "    topic_labels_en[cluster_id] = keywords\n",
    "\n",
    "df_clusters_en['cluster_topic'] = df_clusters_en['cluster'].map(topic_labels_en)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    count = len(df_clusters_en[df_clusters_en['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {topic_labels_en[cluster_id]} ({count} articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1f121",
   "metadata": {},
   "source": [
    "## 6.2 Map to Higher-Level Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e57bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category mappings\n",
    "topic_categories_enhanced = {\n",
    "    'Politics': ['government', 'election', 'parliament', 'minister', 'political', 'policy', 'president', \n",
    "                 'vote', 'party', 'democrat', 'republican', 'law', 'congress', 'senate', 'council',\n",
    "                 'federal', 'state', 'referendum', 'campaign', 'diplomat', 'legislative', 'executive',\n",
    "                 'parliament', 'coalition', 'opposition', 'chancellor', 'mayor', 'governor', 'prime'],\n",
    "    \n",
    "    'Sports': ['football', 'soccer', 'tennis', 'basketball', 'hockey', 'olympic', 'champion', 'team',\n",
    "               'player', 'match', 'game', 'tournament', 'league', 'coach', 'athlete', 'sport',\n",
    "               'championship', 'victory', 'defeat', 'goal', 'score', 'final', 'world', 'cup',\n",
    "               'season', 'club', 'training', 'competition', 'medal', 'race', 'swimming', 'skiing'],\n",
    "    \n",
    "    'Economy & Business': ['economy', 'economic', 'business', 'market', 'bank', 'finance', 'investment', 'trade',\n",
    "                           'company', 'stock', 'price', 'inflation', 'currency', 'export', 'import', 'growth',\n",
    "                           'gdp', 'employment', 'unemployment', 'budget', 'debt', 'profit', 'financial',\n",
    "                           'corporate', 'industry', 'commercial', 'entrepreneur', 'revenue', 'sales', 'consumer'],\n",
    "    \n",
    "    'Science & Technology': ['science', 'technology', 'research', 'study', 'university', 'scientist',\n",
    "                             'experiment', 'discovery', 'innovation', 'digital', 'computer', 'internet',\n",
    "                             'software', 'data', 'artificial', 'intelligence', 'robot', 'space', 'energy',\n",
    "                             'tech', 'innovation', 'laboratory', 'academic', 'technical', 'engineering',\n",
    "                             'development', 'scientific', 'app', 'online', 'platform', 'system'],\n",
    "    \n",
    "    'Health': ['health', 'medical', 'hospital', 'doctor', 'patient', 'disease', 'treatment', 'medicine',\n",
    "               'virus', 'vaccine', 'pandemic', 'covid', 'care', 'mental', 'clinic', 'drug', 'therapy',\n",
    "               'healthcare', 'diagnosis', 'symptoms', 'infection', 'prevention', 'nursing', 'surgery',\n",
    "               'pharmaceutical', 'wellness', 'emergency', 'healthcare'],\n",
    "    \n",
    "    'Environment & Climate': ['climate', 'environment', 'environmental', 'weather', 'temperature', 'global',\n",
    "                              'warming', 'carbon', 'pollution', 'sustainable', 'renewable', 'energy', 'nature',\n",
    "                              'forest', 'ocean', 'animal', 'species', 'biodiversity', 'ecological', 'green',\n",
    "                              'conservation', 'wildlife', 'natural', 'emission', 'solar', 'wind', 'water'],\n",
    "    \n",
    "    'Culture & Entertainment': ['culture', 'cultural', 'music', 'film', 'movie', 'concert', 'festival',\n",
    "                                'artist', 'museum', 'exhibition', 'theater', 'performance', 'book',\n",
    "                                'author', 'literature', 'entertainment', 'celebrity', 'show', 'art',\n",
    "                                'cinema', 'gallery', 'dance', 'opera', 'creative', 'painting', 'song'],\n",
    "    \n",
    "    'Society & Education': ['social', 'society', 'community', 'people', 'family', 'education', 'school', 'student',\n",
    "                            'teacher', 'child', 'women', 'rights', 'justice', 'police', 'crime', 'court', 'prison',\n",
    "                            'learning', 'teaching', 'university', 'college', 'children', 'youth', 'citizenship',\n",
    "                            'welfare', 'public', 'human'],\n",
    "    \n",
    "    'International & Foreign Affairs': ['international', 'foreign', 'country', 'countries', 'world', 'global',\n",
    "                                        'nation', 'diplomatic', 'relations', 'treaty', 'ambassador', 'border',\n",
    "                                        'crisis', 'conflict', 'peace', 'war', 'alliance', 'united', 'nations'],\n",
    "    \n",
    "    'Media & Communication': ['media', 'news', 'press', 'journalist', 'television', 'radio', 'broadcast',\n",
    "                              'newspaper', 'magazine', 'report', 'reporter', 'channel', 'publishing',\n",
    "                              'communication', 'interview', 'announcement', 'statement']\n",
    "}\n",
    "\n",
    "def assign_category_enhanced(cluster_keywords):\n",
    "    \"\"\"Assign category based on keyword matching\"\"\"\n",
    "    keywords_lower = cluster_keywords.lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for category, category_keywords in topic_categories_enhanced.items():\n",
    "        score = sum(1 for kw in category_keywords if kw in keywords_lower)\n",
    "        if score > 0:\n",
    "            scores[category] = score\n",
    "    \n",
    "    if scores:\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Assign categories\n",
    "cluster_categories_enhanced = {}\n",
    "print(\"\\nMapping clusters to higher-level topic categories:\\n\")\n",
    "print(f\"{'Cluster':<10} {'Keywords':<45} {'Category':<30}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = topic_labels_en[cluster_id]\n",
    "    category = assign_category_enhanced(keywords)\n",
    "    cluster_categories_enhanced[cluster_id] = category\n",
    "    print(f\"{cluster_id:<10} {keywords:<45} {category:<30}\")\n",
    "\n",
    "df_clusters_en['topic_category_enhanced'] = df_clusters_en['cluster'].map(cluster_categories_enhanced)\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\n\\nArticle Distribution by Enhanced Topic Category:\")\n",
    "print(\"=\" * 60)\n",
    "category_counts_enhanced = df_clusters_en['topic_category_enhanced'].value_counts()\n",
    "for category, count in category_counts_enhanced.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    print(f\"{category:<30} {count:>5} articles ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23122cef",
   "metadata": {},
   "source": [
    "## 6.3 Visualize Enhanced Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UMAP projection for translated text\n",
    "reducer_en = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d_en = reducer_en.fit_transform(emb_matrix_en)\n",
    "\n",
    "# Color map for categories\n",
    "unique_categories = sorted(df_clusters_en['topic_category_enhanced'].unique())\n",
    "category_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "category_color_map = {cat: color for cat, color in zip(unique_categories, category_colors)}\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(22, 10))\n",
    "\n",
    "# Left: Scatter plot by category\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "for category in unique_categories:\n",
    "    mask = df_clusters_en['topic_category_enhanced'] == category\n",
    "    indices = df_clusters_en[mask].index\n",
    "    ax1.scatter(\n",
    "        embedding_2d_en[indices, 0],\n",
    "        embedding_2d_en[indices, 1],\n",
    "        c=[category_color_map[category]],\n",
    "        label=category,\n",
    "        alpha=0.7,\n",
    "        s=60,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5\n",
    "    )\n",
    "\n",
    "ax1.set_title('Article Clusters with Enhanced Topic Categories\\n(UMAP 2D Projection)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=9, framealpha=0.9, edgecolor='black')\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Right: Pie chart\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "category_counts = df_clusters_en['topic_category_enhanced'].value_counts()\n",
    "colors = [category_color_map[cat] for cat in category_counts.index]\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    category_counts.values,\n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    textprops={'fontsize': 10, 'weight': 'bold'},\n",
    "    wedgeprops={'edgecolor': 'black', 'linewidth': 1.5}\n",
    ")\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontsize(9)\n",
    "\n",
    "ax2.set_title('Distribution of Articles by Enhanced Topic Category\\n' + \n",
    "              f'(Total: {len(df_clusters_en)} articles)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fa023",
   "metadata": {},
   "source": [
    "# 7. BERTopic Analysis (Complete Implementation)\n",
    "\n",
    "This section implements advanced topic modeling using BERTopic with:\n",
    "- **Custom embedder**: Nvidia LLaMA-Embed-Nemotron-8B model\n",
    "- **UMAP**: Dimensionality reduction with configurable parameters\n",
    "- **HDBSCAN**: Hierarchical density-based clustering\n",
    "- **Custom topic labels**: Human-readable topic names\n",
    "- **Visualizations**: Bar charts and time series analysis\n",
    "- **Comparison**: Results compared with K-means clustering\n",
    "\n",
    "BERTopic provides more sophisticated topic modeling than K-means by:\n",
    "1. Using state-of-the-art embeddings\n",
    "2. Discovering topics hierarchically\n",
    "3. Handling noise and outliers better\n",
    "4. Providing interpretable topic representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe1625",
   "metadata": {},
   "source": [
    "## 7.1 Custom Embedder Class\n",
    "\n",
    "Define a custom embedder that wraps the Nvidia LLaMA embedding model for use with BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from IPython.display import display\n",
    "\n",
    "class CustomEmbedder:\n",
    "    \"\"\"Custom embedder using Nvidia LLaMA-Embed-Nemotron-8B model\"\"\"\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encode(self, texts, **kwargs):\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf66d4c",
   "metadata": {},
   "source": [
    "## 7.2 Load Model and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ae64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BERTOPIC ANALYSIS - LOADING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Nvidia LLaMA embedding model\n",
    "print(\"\\nLoading Nvidia LLaMA-Embed-Nemotron-8B model...\")\n",
    "MODEL_NAME = \"nvidia/llama-embed-nemotron-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "print(f\"✓ Loaded {MODEL_NAME}\")\n",
    "\n",
    "# Prepare data - use translated English text if available, otherwise original\n",
    "print(\"\\nPreparing data...\")\n",
    "if 'content_text_en' in df_translated.columns:\n",
    "    df_bertopic = df_translated.copy()\n",
    "    docs = df_bertopic[\"content_text_en\"].dropna().tolist()\n",
    "    print(f\"✓ Using translated English text: {len(docs)} documents\")\n",
    "else:\n",
    "    df_bertopic = srgssr_article_corpus.copy()\n",
    "    docs = df_bertopic[\"content_text_csv\"].dropna().tolist()\n",
    "    print(f\"✓ Using original text: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edcc1ec",
   "metadata": {},
   "source": [
    "## 7.3 Configure UMAP and HDBSCAN Parameters\n",
    "\n",
    "BERTopic uses UMAP for dimensionality reduction and HDBSCAN for clustering.\n",
    "These parameters can be tuned to adjust topic granularity and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfiguring UMAP and HDBSCAN parameters...\")\n",
    "\n",
    "# UMAP parameters\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=20,        # Higher = more global structure (default: 15)\n",
    "    n_components=5,        # Number of dimensions (default: 5)\n",
    "    min_dist=0.0,          # Minimum distance between points (default: 0.1)\n",
    "    metric='cosine',       # Distance metric\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "# HDBSCAN parameters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=220,   # Minimum size of clusters (increase for fewer, larger topics)\n",
    "    min_samples=15,         # Higher = more conservative clustering\n",
    "    metric='euclidean',     # Distance metric\n",
    "    cluster_selection_method='eom',  # 'eom' or 'leaf'\n",
    "    prediction_data=True    # Needed for transform\n",
    ")\n",
    "\n",
    "print(\"✓ UMAP configured: n_neighbors=20, n_components=5, min_dist=0.0\")\n",
    "print(\"✓ HDBSCAN configured: min_cluster_size=220, min_samples=15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8f771",
   "metadata": {},
   "source": [
    "## 7.4 Setup BERTopic and Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETTING UP BERTOPIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Setup embedding model\n",
    "embedding_model = CustomEmbedder(model, tokenizer)\n",
    "\n",
    "# Configure vectorizer with custom stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "custom_stopwords = list(ENGLISH_STOP_WORDS) + ['said', 'efe']\n",
    "vectorizer_model = CountVectorizer(stop_words=custom_stopwords)\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"✓ BERTopic configured with custom embedder\")\n",
    "\n",
    "# Run topic modeling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING TOPIC MODELING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take several minutes depending on the dataset size...\")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Add topics to dataframe\n",
    "df_clean = df_bertopic.dropna(subset=[\"content_text_en\" if 'content_text_en' in df_bertopic.columns else \"content_text_csv\"]).copy()\n",
    "df_clean[\"topic\"] = topics\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"RESULTS: Found {len(set(topics))} topics (including outliers)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "display(topic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f518eb",
   "metadata": {},
   "source": [
    "## 7.5 Assign Custom Topic Labels\n",
    "\n",
    "Replace automatic topic names with human-readable labels based on manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c955ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAssigning custom topic labels...\")\n",
    "\n",
    "# Define custom topic labels\n",
    "custom_topic_labels = {\n",
    "    -1: \"Outliers / Unassigned\",\n",
    "    0: \"Latin American Politics\",\n",
    "    1: \"Swiss Domestic Affairs\",\n",
    "    2: \"Africa & Middle East\",\n",
    "    3: \"International Sports News\",\n",
    "    4: \"Law, Crime, Public Safety\",\n",
    "    5: \"Arts & Culture\",\n",
    "    6: \"Business & Economics\",\n",
    "    7: \"International Security & Military Affairs\",\n",
    "    8: \"International Trade & Geopolitics\",\n",
    "    9: \"Natural Disaster & Humanitarian Response\",\n",
    "    10: \"US Domestic Affairs\",\n",
    "    11: \"Climate Action & Policy\",\n",
    "    12: \"Business & Economics in Latin America\"\n",
    "}\n",
    "\n",
    "# Map topics to custom labels\n",
    "df_clean['topic_label'] = df_clean['topic'].map(custom_topic_labels)\n",
    "\n",
    "print(\"✓ Custom topic labels assigned\")\n",
    "print(\"\\nSample articles with BERTopic topics:\")\n",
    "display(df_clean[['content_text_en' if 'content_text_en' in df_clean.columns else 'content_text_csv', \n",
    "                  'topic', 'topic_label']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880e1a0",
   "metadata": {},
   "source": [
    "## 7.6 Visualize Topic Distribution (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de829403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate topic percentages\n",
    "topic_counts = df_clean['topic_label'].value_counts()\n",
    "topic_percentages = (topic_counts / len(df_clean) * 100).sort_values(ascending=True)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(topic_percentages)))\n",
    "bars = ax.barh(topic_percentages.index, topic_percentages.values, color=colors)\n",
    "\n",
    "# Add bar labels\n",
    "for i, (bar, value) in enumerate(zip(bars, topic_percentages.values)):\n",
    "    ax.text(value + 0.3, i, f'{value:.1f}%', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Percentage of Corpus (%)', fontsize=11)\n",
    "ax.set_ylabel('')\n",
    "ax.set_title('BERTopic Distribution Across Corpus', fontsize=14, fontweight='bold')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bertopic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved chart to: bertopic_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb86aa7",
   "metadata": {},
   "source": [
    "## 7.7 Time Series Analysis (Small Multiples)\n",
    "\n",
    "Visualize how different topics evolve over time using small multiples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING TIME SERIES VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parse dates\n",
    "if 'releaseDate' in df_clean.columns:\n",
    "    df_clean['releaseDate'] = pd.to_datetime(df_clean['releaseDate'])\n",
    "    df_clean['date'] = df_clean['releaseDate'].dt.date\n",
    "    print(f\"Date range: {df_clean['releaseDate'].min()} to {df_clean['releaseDate'].max()}\")\n",
    "    \n",
    "    # Set date limits\n",
    "    date_min = datetime(2025, 10, 27)\n",
    "    date_max = datetime(2025, 11, 5)\n",
    "    \n",
    "    # Calculate topic totals\n",
    "    topic_totals = df_clean.groupby('topic_label').size().reset_index(name='total')\n",
    "    topic_totals = topic_totals.sort_values('total', ascending=False)\n",
    "    \n",
    "    # Separate outliers\n",
    "    outliers_mask = topic_totals['topic_label'] == 'Outliers / Unassigned'\n",
    "    outliers_topic = topic_totals[outliers_mask]['topic_label'].tolist()\n",
    "    other_topics = topic_totals[~outliers_mask]['topic_label'].tolist()\n",
    "    topics_list = other_topics + outliers_topic\n",
    "    \n",
    "    # Daily counts\n",
    "    daily_counts = df_clean.groupby(['topic_label', 'date']).size().reset_index(name='count')\n",
    "    \n",
    "    # Create small multiples\n",
    "    n_topics = len(topics_list)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_topics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 2.5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else axes\n",
    "    \n",
    "    for idx, topic in enumerate(topics_list):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        topic_data = daily_counts[daily_counts['topic_label'] == topic].copy()\n",
    "        topic_data = topic_data.sort_values('date')\n",
    "        topic_data['date'] = pd.to_datetime(topic_data['date'])\n",
    "        \n",
    "        total = topic_data['count'].sum()\n",
    "        \n",
    "        ax.plot(topic_data['date'], topic_data['count'], \n",
    "                color='steelblue', linewidth=1.5, marker='o', markersize=3, \n",
    "                markerfacecolor='steelblue', markeredgecolor='white', markeredgewidth=0.5,\n",
    "                label=f'Total: {total:,}')\n",
    "        \n",
    "        ax.set_title(topic, fontsize=9, fontweight='bold', pad=5)\n",
    "        \n",
    "        if topic == 'Outliers / Unassigned':\n",
    "            ax.set_ylim(0, 385)\n",
    "        else:\n",
    "            ax.set_ylim(0, 190)\n",
    "        \n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        ax.set_xlim(date_min, date_max)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), fontsize=7)\n",
    "        \n",
    "        ax.legend(loc='upper right', fontsize=7, framealpha=0.9, handlelength=0, handletextpad=0, markerscale=0)\n",
    "    \n",
    "    for idx in range(n_topics, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('BERTopic Distribution Over Time', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout(h_pad=1.5, w_pad=1.5)\n",
    "    plt.savefig('bertopic_time_series.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: bertopic_time_series.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠ No releaseDate column found - skipping time series visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5798ee",
   "metadata": {},
   "source": [
    "## 7.8 Save BERTopic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = \"articles_with_bertopic.csv\"\n",
    "df_clean.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"✓ Saved all articles with BERTopic topics to: {output_file}\")\n",
    "\n",
    "# Save model (optional - can be large)\n",
    "# topic_model.save(\"bertopic_model\")\n",
    "# print(\"✓ Saved BERTopic model to: bertopic_model/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BERTOPIC ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52073d42",
   "metadata": {},
   "source": [
    "## 7.9 Compare BERTopic vs K-means\n",
    "\n",
    "Compare the results from BERTopic (hierarchical, density-based) with K-means (centroid-based) clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: BERTopic vs K-means\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### K-means Results (Section 4):\")\n",
    "print(f\"- Algorithm: Centroid-based clustering\")\n",
    "print(f\"- Number of clusters: {n_clusters} (pre-defined)\")\n",
    "print(f\"- All documents assigned to a cluster: Yes\")\n",
    "print(f\"- Outlier handling: No explicit outlier detection\")\n",
    "print(f\"- Topic representation: Based on most frequent keywords\")\n",
    "\n",
    "print(\"\\n### BERTopic Results (Section 7):\")\n",
    "print(f\"- Algorithm: Hierarchical density-based (HDBSCAN)\")\n",
    "print(f\"- Number of topics: {len(set(topics))} (automatically discovered)\")\n",
    "print(f\"- Outliers detected: {len([t for t in topics if t == -1])} documents\")\n",
    "print(f\"- Outlier handling: Explicit outlier topic (-1)\")\n",
    "print(f\"- Topic representation: Using c-TF-IDF\")\n",
    "\n",
    "print(\"\\n### Key Differences:\")\n",
    "print(\"1. **Flexibility**: BERTopic discovers topics automatically; K-means requires pre-defined k\")\n",
    "print(\"2. **Outliers**: BERTopic identifies noise/outliers; K-means forces all points into clusters\")\n",
    "print(\"3. **Embeddings**: BERTopic uses advanced LLaMA embeddings; K-means used lighter MiniLM\")\n",
    "print(\"4. **Interpretability**: BERTopic provides c-TF-IDF scores; K-means uses raw keyword frequency\")\n",
    "print(\"5. **Hierarchy**: BERTopic supports hierarchical topics; K-means is flat\")\n",
    "\n",
    "print(\"\\n### When to Use Each:\")\n",
    "print(\"- **K-means**: Fast, simple, good for exploration with known number of categories\")\n",
    "print(\"- **BERTopic**: More sophisticated, handles noise, better for discovery of unknown topics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2862fa",
   "metadata": {},
   "source": [
    "# 8. Summary\n",
    "\n",
    "This comprehensive analysis demonstrated multiple approaches to understanding the SRG SSR article corpus:\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Data Acquisition & Processing\n",
    "- Successfully loaded and processed 1,000 articles from the public sample\n",
    "- Implemented robust data pipeline with translation capabilities\n",
    "\n",
    "### 2. Semantic Search (Section 3)\n",
    "- Implemented efficient semantic search using Sentence Transformers\n",
    "- Enables journalists to quickly find related articles\n",
    "- Reduces duplicate story creation across branches\n",
    "\n",
    "### 3. K-means Clustering (Section 4)\n",
    "- Identified 10 distinct topic clusters\n",
    "- Visualized topics in 2D space using UMAP\n",
    "- Extracted representative keywords for each cluster\n",
    "\n",
    "### 4. Translation Pipeline (Section 5)\n",
    "- Successfully translated articles to English\n",
    "- Demonstrated scalability to multiple languages\n",
    "- Enables multilingual content availability\n",
    "\n",
    "### 5. Enhanced Categorization (Section 6)\n",
    "- Mapped clusters to 10 high-level categories:\n",
    "  - Politics\n",
    "  - Sports\n",
    "  - Economy & Business\n",
    "  - Science & Technology\n",
    "  - Health\n",
    "  - Environment & Climate\n",
    "  - Culture & Entertainment\n",
    "  - Society & Education\n",
    "  - International Affairs\n",
    "  - Media & Communication\n",
    "\n",
    "### 6. BERTopic Analysis (Section 7)\n",
    "- Advanced topic modeling with Nvidia LLaMA embeddings\n",
    "- Discovered 13 distinct topics (plus outliers)\n",
    "- Provided temporal analysis showing topic evolution\n",
    "- Better handling of outliers compared to K-means\n",
    "\n",
    "## Methodological Comparison\n",
    "\n",
    "| Aspect | K-means | BERTopic |\n",
    "|--------|---------|----------|\n",
    "| Clusters | 10 (pre-defined) | 13 (discovered) |\n",
    "| Outliers | None | Explicitly handled |\n",
    "| Embedding | MiniLM-L6 | LLaMA-8B |\n",
    "| Speed | Fast | Slower but more accurate |\n",
    "| Use Case | Quick exploration | Deep analysis |\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **For Journalists**: Use semantic search (Section 3) to quickly find related articles\n",
    "2. **For Content Strategy**: Use BERTopic results (Section 7) for editorial planning\n",
    "3. **For Navigation**: Implement enhanced categories (Section 6) for user-facing organization\n",
    "4. **For Multilingual**: Scale translation pipeline (Section 5) to all 11 languages\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Apply to full dataset (not just 1,000 sample)\n",
    "2. Implement real-time topic tracking\n",
    "3. Build automated translation for all languages\n",
    "4. Create interactive dashboard for exploration\n",
    "5. Integrate with CMS for automatic categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884424c",
   "metadata": {},
   "source": [
    "# 9. Appendix\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Models Used\n",
    "- **Semantic Search & K-means**: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- **BERTopic**: `nvidia/llama-embed-nemotron-8b`\n",
    "- **Translation**: Google Translate API\n",
    "\n",
    "### Libraries\n",
    "- pandas, numpy: Data manipulation\n",
    "- sentence-transformers: Text embeddings\n",
    "- scikit-learn: K-means clustering\n",
    "- umap-learn: Dimensionality reduction\n",
    "- BERTopic: Advanced topic modeling\n",
    "- hdbscan: Density-based clustering\n",
    "- matplotlib, seaborn: Visualization\n",
    "- transformers: HuggingFace models\n",
    "\n",
    "### Parameters\n",
    "\n",
    "**UMAP:**\n",
    "- n_neighbors: 20\n",
    "- n_components: 5\n",
    "- min_dist: 0.0\n",
    "- metric: cosine\n",
    "\n",
    "**HDBSCAN:**\n",
    "- min_cluster_size: 220\n",
    "- min_samples: 15\n",
    "- metric: euclidean\n",
    "\n",
    "**K-means:**\n",
    "- n_clusters: 10\n",
    "- random_state: 42\n",
    "\n",
    "## Data Access\n",
    "\n",
    "**Public Sample:**\n",
    "- URL: https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\n",
    "- Size: <25MB\n",
    "- Articles: 1,000 (sampled)\n",
    "\n",
    "**Full Dataset (Confidential):**\n",
    "- Location: Databricks Delta table\n",
    "- Access: Requires SRG SSR credentials\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [RenkuLab](https://renkulab.io) - Cloud-based Jupyter environment\n",
    "- [BERTopic Documentation](https://maartengr.github.io/BERTopic/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions about this analysis or data access, contact the SRG SSR data team.\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Date:** November 2025  \n",
    "**Notebook Version:** 1.0 (Comprehensive Merged)  \n",
    "**Authors:** CAS ADS Module 3 Project Team"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crashpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
