{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac8fe24",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition (ETL)\n",
    "\n",
    "## Installation and Setup\n",
    "First, install all required libraries for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# Install NLP and ML libraries\n",
    "%pip install sentence-transformers torch safetensors accelerate\n",
    "%pip install scikit-learn scipy\n",
    "%pip install matplotlib umap-learn\n",
    "\n",
    "# Install translation library\n",
    "%pip install googletrans==4.0.0-rc1\n",
    "\n",
    "# Optional: For advanced BERTopic analysis\n",
    "# %pip install bertopic hdbscan transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05456a",
   "metadata": {},
   "source": [
    "## Load Article Corpus\n",
    "\n",
    "Load the article corpus from the public Parquet file hosted on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c27f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from GitHub\n",
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url, engine=\"fastparquet\")\n",
    "\n",
    "# For full dataset access (requires permissions):\n",
    "# srgssr_article_corpus = spark.table(\"swi_audience_prd.pdp_articles_v2.articles_v2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec14054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check access level\n",
    "has_read_access_udp_articles_v2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74c22d",
   "metadata": {},
   "source": [
    "# 2. Dataset Overview\n",
    "\n",
    "## Check Dataset Version and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rowcount(n):\n",
    "    if n >= 1_000_000:\n",
    "        return f\"more than {n // 1_000_000} million\"\n",
    "    elif n >= 1_000:\n",
    "        return f\"more than {n // 1_000} thousand\"\n",
    "    else:\n",
    "        return f\"{n}\"\n",
    "\n",
    "if has_read_access_udp_articles_v2:\n",
    "    rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"✓ Full dataset loaded: {format_rowcount(rowcount)} articles from SRG-SSR\")\n",
    "else:\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        rowcount = len(srgssr_article_corpus)\n",
    "    else:\n",
    "        rowcount = srgssr_article_corpus.count()\n",
    "    print(f\"✓ Public sample loaded: {format_rowcount(rowcount)} articles from SRG-SSR\")\n",
    "    print(\"You can access the dataframe via 'srgssr_article_corpus'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00e8f1",
   "metadata": {},
   "source": [
    "## Data Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdcbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "first_row = srgssr_article_corpus.iloc[0].to_dict() if not srgssr_article_corpus.empty else {}\n",
    "\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": col,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": first_row.get(col, None)\n",
    "    }\n",
    "    for col, dtype in srgssr_article_corpus.dtypes.items()\n",
    "]\n",
    "\n",
    "print(f\"Dataset shape: {srgssr_article_corpus.shape}\")\n",
    "print(f\"Columns: {len(cols_info)}\")\n",
    "pd.DataFrame(cols_info).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750ff14",
   "metadata": {},
   "source": [
    "## Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8add82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(srgssr_article_corpus.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafeae3",
   "metadata": {},
   "source": [
    "## Limit Dataset for Testing\n",
    "\n",
    "For faster iteration during development, we can work with a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706be068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with subset for faster processing (optional)\n",
    "srgssr_article_corpus = srgssr_article_corpus.head(1000)\n",
    "print(f\"Working with {len(srgssr_article_corpus)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083fcbe",
   "metadata": {},
   "source": [
    "# 3. Semantic Search Implementation\n",
    "\n",
    "## USE CASE: Quickly Search Articles Without Google\n",
    "\n",
    "**Goal:** Search all articles without external tools. This helps writers check if a story was already written by colleagues in different branches.\n",
    "\n",
    "**Approach:**\n",
    "- Use Sentence Transformers to create semantic embeddings\n",
    "- Implement similarity-based search\n",
    "- Enable keyword, phrase, or topic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TEXT_COL = \"content_text_csv\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "# Prepare data\n",
    "df = srgssr_article_corpus.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# Initialize model (singleton pattern)\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "print(\"Creating embeddings for semantic search...\")\n",
    "model = get_embedder()\n",
    "emb_matrix = model.encode(\n",
    "    df[TEXT_COL].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "ids = df[ID_COL].tolist()\n",
    "texts = df[TEXT_COL].tolist()\n",
    "\n",
    "print(f\"✓ Created {emb_matrix.shape[0]} embeddings of dimension {emb_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b946f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Search articles using semantic similarity\"\"\"\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = emb_matrix @ q\n",
    "    top_idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i][:200] + \"...\" for i in top_idx],  # Truncate for display\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n",
    "# Example usage\n",
    "results = semantic_search(\"climate change\", top_k=10)\n",
    "print(\"Top 10 articles about 'climate change':\")\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed30a3e",
   "metadata": {},
   "source": [
    "# 4. Topic Clustering Analysis\n",
    "\n",
    "## USE CASE: Discover What Topics SRG Writes About\n",
    "\n",
    "**Goal:** Identify common topics and themes in SRG articles to enhance navigation and filtering.\n",
    "\n",
    "**Approach:**\n",
    "- Use K-means clustering on semantic embeddings\n",
    "- Extract topic keywords from each cluster\n",
    "- Visualize clusters in 2D using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c889948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Perform clustering\n",
    "n_clusters = 10\n",
    "print(f\"Clustering articles into {n_clusters} topics...\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "labels = kmeans.fit_predict(emb_matrix)\n",
    "\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"content_text_csv\": texts,\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "print(f\"✓ Clustering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic keywords for each cluster\n",
    "def get_topic_keywords(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['content_text_csv'].tolist()\n",
    "    \n",
    "    # Combine all texts in the cluster\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract words (filter out very short words and common stopwords)\n",
    "    words = re.findall(r'\\b[a-zäöüàéèêëïôùû]{4,}\\b', combined_text)\n",
    "    \n",
    "    # Multilingual stopwords\n",
    "    stopwords = {\n",
    "        'dass', 'sind', 'wird', 'wurden', 'wurde', 'haben', 'sein', \n",
    "        'eine', 'einem', 'einen', 'einer', 'dies', 'diese', 'dieser',\n",
    "        'auch', 'mehr', 'beim', 'über', 'nach', 'sich', 'oder', 'kann',\n",
    "        'können', 'müssen', 'soll', 'sollen', 'noch', 'bereits', 'aber',\n",
    "        'wenn', 'weil', 'denn', 'dann', 'sowie', 'damit', 'with',\n",
    "        'from', 'have', 'this', 'that', 'will', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'pour', 'dans', 'avec',\n",
    "        'sont', 'être', 'cette', 'mais', 'plus', 'comme', 'fait'\n",
    "    }\n",
    "    \n",
    "    # Filter and count words\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels and add to dataframe\n",
    "topic_labels = {}\n",
    "print(\"\\nCluster Topics (based on most frequent keywords):\")\n",
    "print(\"=\" * 70)\n",
    "for cluster_id in range(n_clusters):\n",
    "    keywords = get_topic_keywords(cluster_id, df_clusters, top_n=3)\n",
    "    topic_labels[cluster_id] = keywords\n",
    "    count = len(df_clusters[df_clusters['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {keywords:<40} ({count} articles)\")\n",
    "\n",
    "# Map topic keywords to each row\n",
    "df_clusters['cluster_topic'] = df_clusters['cluster'].map(topic_labels)\n",
    "\n",
    "print(\"\\n✓ Topic extraction complete\")\n",
    "display(df_clusters.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafbeb0",
   "metadata": {},
   "source": [
    "## Visualize Topic Clusters with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed943d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "print(\"Reducing embeddings to 2D for visualization...\")\n",
    "\n",
    "# Reduce embeddings to 2D using UMAP\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(emb_matrix)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d[:, 0], \n",
    "    embedding_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Topic Clusters Visualization with Keywords (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers with topic labels\n",
    "kmeans_centers_2d = reducer.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d[:, 0], \n",
    "    kmeans_centers_2d[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels for each cluster center\n",
    "for cluster_id in range(n_clusters):\n",
    "    x, y = kmeans_centers_2d[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"\\nCluster Distribution:\")\n",
    "cluster_counts = df_clusters['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(df_clusters)) * 100\n",
    "    print(f\"Cluster {cluster_id} ({topic_labels[cluster_id]}): {count} articles ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecaaa90",
   "metadata": {},
   "source": [
    "# 5. Translation Pipeline\n",
    "\n",
    "## USE CASE: Translate All Articles to English\n",
    "\n",
    "**Goal:** Translate the entire corpus to English for consistent analysis and categorization.\n",
    "\n",
    "**Approach:**\n",
    "- Use Google Translate API (googletrans library)\n",
    "- Implement retry logic and rate limiting\n",
    "- Handle errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import time\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Create a copy of the dataframe to store translations\n",
    "df_translated = srgssr_article_corpus.copy()\n",
    "\n",
    "# Function to translate text with error handling\n",
    "def translate_text(text, dest='en', max_retries=3):\n",
    "    \"\"\"Translate text to target language with retry logic\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Limit text length to avoid API issues (Google Translate has limits)\n",
    "    text_str = str(text)[:5000]  # Limit to 5000 characters\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = translator.translate(text_str, dest=dest)\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)  # Wait before retry\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Translation failed after {max_retries} attempts: {str(e)[:100]}\")\n",
    "                return text_str  # Return original text if translation fails\n",
    "    \n",
    "    return text_str\n",
    "\n",
    "print(\"Translating articles to English...\")\n",
    "print(f\"Total articles to translate: {len(df_translated)}\")\n",
    "print(\"Note: This may take several minutes. Google Translate API has rate limits.\\n\")\n",
    "\n",
    "# Translate with progress indicator\n",
    "translated_texts = []\n",
    "for idx, text in enumerate(df_translated['content_text_csv']):\n",
    "    if idx % 50 == 0:  # Progress update every 50 articles\n",
    "        print(f\"Progress: {idx}/{len(df_translated)} articles translated...\")\n",
    "    \n",
    "    translated = translate_text(text, dest='en')\n",
    "    translated_texts.append(translated)\n",
    "    \n",
    "    # Small delay to avoid rate limiting\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Add translated column\n",
    "df_translated['content_text_en'] = translated_texts\n",
    "\n",
    "print(f\"\\n✓ Translation complete! Translated {len(df_translated)} articles.\")\n",
    "print(\"\\nShowing first 3 translated articles:\")\n",
    "display(df_translated[['id', 'content_text_csv', 'content_text_en']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4cc31",
   "metadata": {},
   "source": [
    "# 6. Enhanced Topic Categorization\n",
    "\n",
    "## Cluster Translated Articles\n",
    "\n",
    "Create embeddings and clusters for the translated English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating embeddings for translated English articles...\")\n",
    "\n",
    "# Use the English translated column\n",
    "df_en = df_translated.copy()\n",
    "df_en['content_text_en'] = df_en['content_text_en'].fillna(\"\").astype(str)\n",
    "\n",
    "# Create embeddings for the English text\n",
    "model_en = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb_matrix_en = model_en.encode(\n",
    "    df_en['content_text_en'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# Perform clustering on English translations\n",
    "n_clusters_en = 10\n",
    "kmeans_en = KMeans(n_clusters=n_clusters_en, random_state=42, n_init=\"auto\")\n",
    "labels_en = kmeans_en.fit_predict(emb_matrix_en)\n",
    "\n",
    "# Create dataframe with cluster assignments\n",
    "df_clusters_en = pd.DataFrame({\n",
    "    \"id\": df_en['id'].tolist(),\n",
    "    \"original_text\": df_en['content_text_csv'].tolist(),\n",
    "    \"translated_text_en\": df_en['content_text_en'].tolist(),\n",
    "    \"cluster\": labels_en\n",
    "})\n",
    "\n",
    "print(f\"✓ Created {emb_matrix_en.shape[0]} embeddings and {n_clusters_en} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df181db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topic keywords from English translations\n",
    "def get_topic_keywords_en(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from English articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['translated_text_en'].tolist()\n",
    "    \n",
    "    # Combine all texts in the cluster\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract English words\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "    \n",
    "    # English stopwords\n",
    "    stopwords = {\n",
    "        'this', 'that', 'with', 'from', 'have', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'will', 'would', 'could',\n",
    "        'should', 'about', 'after', 'also', 'many', 'more', 'most', 'other',\n",
    "        'some', 'such', 'than', 'them', 'then', 'these', 'they', 'very',\n",
    "        'into', 'just', 'like', 'only', 'over', 'said', 'same', 'says',\n",
    "        'does', 'make', 'made', 'well', 'much', 'even', 'back', 'through',\n",
    "        'year', 'years', 'being', 'people', 'according', 'since', 'during',\n",
    "        'first', 'time', 'last', 'still', 'however', 'while', 'before'\n",
    "    }\n",
    "    \n",
    "    # Filter and count words\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Get top keywords\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels\n",
    "topic_labels_en = {}\n",
    "print(\"\\nCluster Topics (based on English translated text):\")\n",
    "print(\"=\" * 70)\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = get_topic_keywords_en(cluster_id, df_clusters_en, top_n=3)\n",
    "    topic_labels_en[cluster_id] = keywords\n",
    "    count = len(df_clusters_en[df_clusters_en['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {keywords:<40} ({count} articles)\")\n",
    "\n",
    "# Add topic labels to dataframe\n",
    "df_clusters_en['cluster_topic'] = df_clusters_en['cluster'].map(topic_labels_en)\n",
    "\n",
    "print(\"\\n✓ Topic extraction complete for English articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54888c14",
   "metadata": {},
   "source": [
    "## Visualize English Article Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating 2D visualization of English article clusters...\")\n",
    "\n",
    "# Reduce embeddings to 2D using UMAP\n",
    "reducer_en = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d_en = reducer_en.fit_transform(emb_matrix_en)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d_en[:, 0], \n",
    "    embedding_2d_en[:, 1], \n",
    "    c=labels_en, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Topic Clusters of Translated English Articles (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers\n",
    "kmeans_centers_2d_en = reducer_en.transform(kmeans_en.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d_en[:, 0], \n",
    "    kmeans_centers_2d_en[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels for each cluster center\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    x, y = kmeans_centers_2d_en[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels_en[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete\")\n",
    "display(df_clusters_en[['id', 'translated_text_en', 'cluster', 'cluster_topic']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd26797",
   "metadata": {},
   "source": [
    "## Hierarchical Topic Categorization\n",
    "\n",
    "Map clusters to high-level categories (Politics, Sports, Culture, Science, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyword patterns for major topic categories\n",
    "topic_categories = {\n",
    "    'Politics': ['government', 'election', 'parliament', 'minister', 'political', 'policy', 'president', \n",
    "                 'vote', 'party', 'democrat', 'republican', 'law', 'congress', 'senate', 'council',\n",
    "                 'federal', 'state', 'referendum', 'campaign', 'diplomat'],\n",
    "    \n",
    "    'Sports': ['football', 'soccer', 'tennis', 'basketball', 'hockey', 'olympic', 'champion', 'team',\n",
    "               'player', 'match', 'game', 'tournament', 'league', 'coach', 'athlete', 'sport',\n",
    "               'championship', 'victory', 'defeat', 'goal', 'score'],\n",
    "    \n",
    "    'Economy & Business': ['economy', 'economic', 'business', 'market', 'bank', 'finance', 'investment', 'trade',\n",
    "                           'company', 'stock', 'price', 'inflation', 'currency', 'export', 'import', 'growth',\n",
    "                           'gdp', 'employment', 'unemployment', 'budget', 'debt', 'profit'],\n",
    "    \n",
    "    'Science & Technology': ['science', 'technology', 'research', 'study', 'university', 'scientist',\n",
    "                             'experiment', 'discovery', 'innovation', 'digital', 'computer', 'internet',\n",
    "                             'software', 'data', 'artificial', 'intelligence', 'robot', 'space', 'energy'],\n",
    "    \n",
    "    'Health': ['health', 'medical', 'hospital', 'doctor', 'patient', 'disease', 'treatment', 'medicine',\n",
    "               'virus', 'vaccine', 'pandemic', 'covid', 'care', 'mental', 'clinic', 'drug', 'therapy'],\n",
    "    \n",
    "    'Environment & Climate': ['climate', 'environment', 'environmental', 'weather', 'temperature', 'global',\n",
    "                              'warming', 'carbon', 'pollution', 'sustainable', 'renewable', 'energy', 'nature',\n",
    "                              'forest', 'ocean', 'animal', 'species', 'biodiversity', 'ecological'],\n",
    "    \n",
    "    'Culture & Entertainment': ['culture', 'cultural', 'music', 'film', 'movie', 'concert', 'festival',\n",
    "                                'artist', 'museum', 'exhibition', 'theater', 'performance', 'book',\n",
    "                                'author', 'literature', 'entertainment', 'celebrity', 'show'],\n",
    "    \n",
    "    'Society & Education': ['social', 'society', 'community', 'people', 'family', 'education', 'school', 'student',\n",
    "                            'teacher', 'child', 'women', 'rights', 'justice', 'police', 'crime', 'court', 'prison']\n",
    "}\n",
    "\n",
    "def assign_category(cluster_keywords):\n",
    "    \"\"\"Assign a category based on keyword matching\"\"\"\n",
    "    keywords_lower = cluster_keywords.lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for category, category_keywords in topic_categories.items():\n",
    "        score = sum(1 for kw in category_keywords if kw in keywords_lower)\n",
    "        if score > 0:\n",
    "            scores[category] = score\n",
    "    \n",
    "    if scores:\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Assign categories to each cluster\n",
    "cluster_categories = {}\n",
    "print(\"Mapping clusters to higher-level topic categories:\\n\")\n",
    "print(f\"{'Cluster':<10} {'Keywords':<40} {'Category':<25}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    keywords = topic_labels_en[cluster_id]\n",
    "    category = assign_category(keywords)\n",
    "    cluster_categories[cluster_id] = category\n",
    "    print(f\"{cluster_id:<10} {keywords:<40} {category:<25}\")\n",
    "\n",
    "# Add category column to dataframe\n",
    "df_clusters_en['topic_category'] = df_clusters_en['cluster'].map(cluster_categories)\n",
    "\n",
    "# Count articles per category\n",
    "print(\"\\n\\nArticle Distribution by Topic Category:\")\n",
    "print(\"=\" * 50)\n",
    "category_counts = df_clusters_en['topic_category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    print(f\"{category:<25} {count:>5} articles ({percentage:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7ea55",
   "metadata": {},
   "source": [
    "## Visualize Categories with Scatter Plot and Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e997817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map for categories\n",
    "unique_categories = sorted(df_clusters_en['topic_category'].unique())\n",
    "category_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "category_color_map = {cat: color for cat, color in zip(unique_categories, category_colors)}\n",
    "\n",
    "# Create visualization with categories\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Left plot: Colored by category\n",
    "for category in unique_categories:\n",
    "    mask = df_clusters_en['topic_category'] == category\n",
    "    indices = df_clusters_en[mask].index\n",
    "    ax1.scatter(\n",
    "        embedding_2d_en[indices, 0],\n",
    "        embedding_2d_en[indices, 1],\n",
    "        c=[category_color_map[category]],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax1.set_title('Articles by Topic Category', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('UMAP Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('UMAP Dimension 2', fontsize=12)\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Pie chart of category distribution\n",
    "ax2.pie(\n",
    "    category_counts.values,\n",
    "    labels=category_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=[category_color_map[cat] for cat in category_counts.index]\n",
    ")\n",
    "ax2.set_title('Distribution of Articles by Topic Category', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Category visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e7a03",
   "metadata": {},
   "source": [
    "## Enhance Categories with Corpus-Derived Keywords\n",
    "\n",
    "Analyze actual word frequencies in each cluster to expand keyword lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e29291",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing corpus to extract additional keywords...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze each cluster to find common words\n",
    "all_cluster_words = {}\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    cluster_texts = df_clusters_en[df_clusters_en['cluster'] == cluster_id]['translated_text_en'].tolist()\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    # Extract words\n",
    "    words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "    \n",
    "    # Extended stopwords\n",
    "    stopwords = {\n",
    "        'this', 'that', 'with', 'from', 'have', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'will', 'would', 'could',\n",
    "        'should', 'about', 'after', 'also', 'many', 'more', 'most', 'other',\n",
    "        'some', 'such', 'than', 'them', 'then', 'these', 'they', 'very',\n",
    "        'into', 'just', 'like', 'only', 'over', 'said', 'same', 'says',\n",
    "        'does', 'make', 'made', 'well', 'much', 'even', 'back', 'through',\n",
    "        'year', 'years', 'being', 'people', 'according', 'since', 'during',\n",
    "        'first', 'time', 'last', 'still', 'however', 'while', 'before'\n",
    "    }\n",
    "    \n",
    "    # Filter and count\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "    word_counts = Counter(words)\n",
    "    all_cluster_words[cluster_id] = word_counts.most_common(20)\n",
    "\n",
    "print(\"Top 10 words per cluster:\")\n",
    "for cluster_id in range(min(3, n_clusters_en)):  # Show first 3 clusters\n",
    "    print(f\"\\nCluster {cluster_id} ({topic_labels_en[cluster_id]}):\")\n",
    "    top_words = [word for word, count in all_cluster_words[cluster_id][:10]]\n",
    "    print(f\"  {', '.join(top_words)}\")\n",
    "\n",
    "print(\"\\n✓ Corpus analysis complete\")\n",
    "print(\"\\nThese words can be used to enhance the topic_categories dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf6b9d",
   "metadata": {},
   "source": [
    "# 7. BERTopic Analysis (Advanced - Optional)\n",
    "\n",
    "This section demonstrates advanced topic modeling using BERTopic with custom embeddings.\n",
    "\n",
    "**Note:** This requires additional libraries and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use BERTopic analysis\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from hdbscan import HDBSCAN\n",
    "# \n",
    "# # Use existing embeddings\n",
    "# print(\"Training BERTopic model...\")\n",
    "# \n",
    "# # Configure BERTopic with custom settings\n",
    "# vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, max_df=0.95)\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', prediction_data=True)\n",
    "# \n",
    "# # Create BERTopic model with pre-computed embeddings\n",
    "# topic_model = BERTopic(\n",
    "#     embedding_model=model_en,\n",
    "#     vectorizer_model=vectorizer_model,\n",
    "#     hdbscan_model=hdbscan_model,\n",
    "#     verbose=True\n",
    "# )\n",
    "# \n",
    "# # Fit the model\n",
    "# topics, probs = topic_model.fit_transform(\n",
    "#     df_en['content_text_en'].tolist(),\n",
    "#     embeddings=emb_matrix_en\n",
    "# )\n",
    "# \n",
    "# # Visualize topics\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.show()\n",
    "# \n",
    "# # Get topic information\n",
    "# topic_info = topic_model.get_topic_info()\n",
    "# display(topic_info)\n",
    "# \n",
    "# print(\"✓ BERTopic analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f400bb6",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "1. ✅ **Data Acquisition**: Loaded SRG-SSR article corpus from GitHub/Databricks\n",
    "2. ✅ **Semantic Search**: Implemented fast similarity-based article search\n",
    "3. ✅ **Topic Clustering**: Identified 10 main topic clusters using K-means\n",
    "4. ✅ **Translation**: Translated full corpus to English for consistent analysis\n",
    "5. ✅ **Categorization**: Mapped clusters to high-level categories (Politics, Sports, etc.)\n",
    "6. ✅ **Visualization**: Created UMAP projections and pie charts for topic distribution\n",
    "7. ✅ **Enhancement**: Extracted corpus-specific keywords to improve categorization\n",
    "\n",
    "## Potential Next Steps\n",
    "\n",
    "- **Expand Translation**: Translate to multiple languages (FR, IT, DE, etc.)\n",
    "- **Time Analysis**: Analyze topic trends over time\n",
    "- **Author Analysis**: Identify topics by author or publication\n",
    "- **Advanced Models**: Experiment with BERTopic, LDA, or transformer-based models\n",
    "- **Production Pipeline**: Automate the workflow for continuous updates\n",
    "- **Interactive Dashboard**: Build a Streamlit/Dash app for exploration\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [UMAP Documentation](https://umap-learn.readthedocs.io/)\n",
    "- [BERTopic Documentation](https://maartengr.github.io/BERTopic/)\n",
    "- [SRG-SSR GitHub Repository](https://github.com/Tao-Pi/CAS-Applied-Data-Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92f5a9",
   "metadata": {},
   "source": [
    "# Appendix: Alternative Approaches\n",
    "\n",
    "## A. Databricks-Specific Translation\n",
    "\n",
    "If running in Databricks environment, use the native `ai_translate` function:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Check if in Databricks\n",
    "try:\n",
    "    spark\n",
    "    is_databricks = True\n",
    "except NameError:\n",
    "    is_databricks = False\n",
    "\n",
    "if is_databricks:\n",
    "    # Convert to Spark DataFrame\n",
    "    if isinstance(srgssr_article_corpus, pd.DataFrame):\n",
    "        df_spark = spark.createDataFrame(srgssr_article_corpus)\n",
    "    else:\n",
    "        df_spark = srgssr_article_corpus\n",
    "    \n",
    "    # Add translation columns\n",
    "    lang_map = {\"en\": \"en\", \"fr\": \"fr\", \"it\": \"it\", \"de\": \"de\"}\n",
    "    df_translated = df_spark\n",
    "    for lang, db_lang in lang_map.items():\n",
    "        df_translated = df_translated.withColumn(\n",
    "            f\"content_text_{lang}\",\n",
    "            expr(f\"ai_translate(content_text_csv, '{db_lang}')\")\n",
    "        )\n",
    "    \n",
    "    display(df_translated)\n",
    "```\n",
    "\n",
    "## B. Reading from Delta Tables\n",
    "\n",
    "For users with access to the full dataset:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def has_read_permission(table_name):\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {table_name} LIMIT 1\")\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "# Check permissions\n",
    "has_access = has_read_permission(\"udp_prd_atomic.pdp.articles_v2\")\n",
    "\n",
    "if has_access:\n",
    "    # Read full dataset\n",
    "    df = spark.table(\"udp_prd_atomic.pdp.articles_v2\")\n",
    "    srgssr_article_corpus = df.toPandas()\n",
    "```\n",
    "\n",
    "## C. Custom Embedding Models\n",
    "\n",
    "For specialized domains, consider fine-tuned models:\n",
    "\n",
    "```python\n",
    "# German-specific model\n",
    "model_de = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "# Larger model for better quality\n",
    "model_large = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Custom model (requires training)\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "# ... fine-tuning code ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354665f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Notebook Version:** 1.0 (Merged)  \n",
    "**Last Updated:** November 26, 2025  \n",
    "**Authors:** CAS Applied Data Science Module 3 Team  \n",
    "**License:** For educational purposes only\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
