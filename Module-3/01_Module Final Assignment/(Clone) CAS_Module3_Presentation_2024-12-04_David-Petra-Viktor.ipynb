{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f290cd01-23e2-41f2-872f-f61ad9a46a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This project is based on the complete SRG article corpus, which is continuously ingested into Databricks via Kafka. However, due to the large size of the full dataset (millions of articles), all analyses in this notebook are performed on a representative sample. This makes the data volume manageable for computationally intensive tasks (such as translation) and enables feasible data sharing, as distributing the full dataset is not practical.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d014eac9-7126-42c8-807c-5b3a5c0940a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objectives and Use Cases\n",
    "\n",
    "This project explores how machine learning can support editorial analysis and content understanding. The main objectives are:\n",
    "\n",
    "### 1. Improved Content Search\n",
    "- **Goal:** Enhance search capabilities using semantic representations instead of simple keyword matching.\n",
    "- **Background:** As of today, there is no known way to search all articles published in SRG, which creates major redundancies when the same article is produced multiple times throughout different business units or teams.\n",
    "- **Approach:** Use sentence embeddings (Sentence-Transformers) to represent article texts as dense vectors that capture semantic meaning. \n",
    "\n",
    "### 2. Automated Translation Pipeline\n",
    "- **Goal:** Automate article translation (initially to English, later to all 10 Swissinfo languages) to:\n",
    "  - Enable analyses that are comparable across languages.\n",
    "  - Reduce manual labor and redundancies in a multilingual organization.\n",
    "- **Background:** Without translations into a common language, analyses are not comparable across languages. This also helps reduce manual labor and redundancies in a multilingual organization, where there is currently no streamlined process for adapting articles.\n",
    "- **Approach:** See the notebook section \"Translation Pipeline\" for details. We implement an automated translation workflow using AI models to translate articles, providing a first step toward streamlined multilingual content production. All translations require human proofreading and editorial approval in accordance with our guidelines.\n",
    "\n",
    "### 3. Topic Clustering\n",
    "- **Goal:** Automatically group articles into coherent thematic clusters to:\n",
    "  - Gain an overview of existing topics in the full SRG corpus.\n",
    "  - Quantify the number of articles per topic.\n",
    "- **Background:** \n",
    "  - Previously, topic classification was based on management decisions or human classifications (e.g., fiction, news, sport), without leveraging advanced methods.\n",
    "  - No unified topic classification existed across SRG's business units (SRF, RTS, RSI, RTR, SWI), making cross-unit comparison impossible.\n",
    "- **Approach:** Compare and evaluate several clustering methods (K-Means with/without translations, BERTopic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f30b5a4a-14fe-4693-be94-1674caec629f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installation and Setup\n",
    "\n",
    "This section covers the following steps:\n",
    "- **Dependency Installation:** Installs all required Python packages for the project using `%pip install` commands. This ensures the environment has all necessary libraries for data processing, machine learning, translation, and visualization.\n",
    "- **Library Imports:** Imports essential Python libraries such as pandas, numpy, matplotlib, scikit-learn, sentence-transformers, and others needed for data analysis, embedding generation, clustering, and translation.\n",
    "- **Data Loading:** Reads the SRG article sample dataset from a public Parquet file hosted on GitHub into a pandas DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f07e8d-c2b8-4c0e-b736-4cf3d456b67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run installs only on the first execution of this notebook.\n",
    "# They install all required dependencies for the full pipeline.\n",
    "# ------------------------------------------------------------\n",
    "RUN_INSTALLS = False # set True manually on first run\n",
    "\n",
    "if RUN_INSTALLS:\n",
    "    %pip install accelerate\n",
    "    %pip install bertopic\n",
    "    %pip install fastparquet\n",
    "    %pip install googletrans==4.0.0-rc1\n",
    "    %pip install hf-transfer\n",
    "    %pip install matplotlib\n",
    "    %pip install pandas\n",
    "    %pip install pyarrow\n",
    "    %pip install scikit-learn\n",
    "    %pip install seaborn\n",
    "    %pip install sentence-transformers\n",
    "    %pip install tf-keras\n",
    "    %pip install torch\n",
    "    %pip install transformers\n",
    "    %pip install --upgrade \"typing_extensions>=4.12.0\"\n",
    "    %pip install umap-learn\n",
    "\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637cd2d8-4fd1-484e-b354-eaf332937433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import fastparquet\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from collections import Counter\n",
    "from googletrans import Translator\n",
    "from hdbscan import HDBSCAN\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a850ea-07dc-45bc-b46b-d10b1a3b7884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\"\n",
    "srgssr_article_corpus = pd.read_parquet(url, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d81655-be96-4a05-9e76-59b31457d83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Dataset Overview\n",
    "\n",
    "In this section, we provide a comprehensive overview of the dataset, including:\n",
    "- Dataset version (confidential vs public)\n",
    "- Total number of articles\n",
    "- Data structure and schema\n",
    "- Sample data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c33eec2-602b-4b28-8985-5d5132563184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Data Structure and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2331a72e-bba5-4979-9690-9b3737c734e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show column information in a more presentation-friendly format\n",
    "first_row = srgssr_article_corpus.iloc[0].to_dict() if not srgssr_article_corpus.empty else {}\n",
    "\n",
    "cols_info = [\n",
    "    {\n",
    "        \"column\": col,\n",
    "        \"type\": str(dtype),\n",
    "        \"example\": str(first_row.get(col, \"\"))[:80] + (\"...\" if first_row.get(col) and len(str(first_row.get(col))) > 80 else \"\")\n",
    "    }\n",
    "    for col, dtype in srgssr_article_corpus.dtypes.items()\n",
    "]\n",
    "\n",
    "df_cols_info = pd.DataFrame(cols_info)[[\"column\", \"type\", \"example\"]]\n",
    "display(df_cols_info.style.set_properties(subset=[\"example\"], **{'white-space': 'pre-wrap', 'max-width': '400px'}).set_table_styles([{'selector': 'th', 'props': [('font-size', '14px')]}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b104547f-fedd-4995-96aa-d8f713f58718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.3 Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0d4cb1-6f01-4edc-a487-0565ebb84276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srgssr_article_corpus.head(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50c61d5f-3fec-415d-8796-48be6c3d4479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1.3 Key Findings Dataset Overview\n",
    "\n",
    "- The data provided is a set of articles (full text) together with some metadata (e.g. release date and URL).\n",
    "- Articles are in multiple languages.\n",
    "- There is an id that uniquely identifies the articles.\n",
    "- Because this is a sample that was exported for this exercise (a Parquet file hosted on GitHub), this sample is easy to use.\n",
    "- On the negative side, this sample implies that if rerunning the same analyses in the future, some results made here (\"what topics do we write about\") will no longer hold true.\n",
    "- For this reason, the analyses here are more to compare different methods and as a proof of concept that the data can serve use cases. To implement this as a full pipeline (final project), all references to \"fixed values\" (like the custom labels on detected clusters, etc.) need to be removed/replaced with machine-generated results (e.g. results from LLM). This is a future project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7153378e-729e-48d4-a5d7-b2eaab299169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Translation Pipeline\n",
    "\n",
    "**Use Case:** Translate all existing articles into all supported languages.\n",
    "\n",
    "**Goal:** Multiply content availability by making every article accessible in all 11 languages used by SRG.\n",
    "\n",
    "**Approach:**\n",
    "- Use Google Translate API (googletrans) for demonstration\n",
    "- Translate articles to English as target language\n",
    "- Handle rate limiting and errors gracefully\n",
    "\n",
    "**Note:** In a Databricks environment, you would use the `ai_translate()` function instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b01a4001-2ef7-4349-8ea6-2b69d376de31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Translation Pipeline Control Logic Summary\n",
    "\n",
    "This cell manages the translation of article content to English and the storage/loading of results:\n",
    "\n",
    "- **Control Flag:** `rerun_translations` determines whether to perform translations from scratch or load existing results.\n",
    "- **Translation Process:** If rerun is enabled, each article's text is translated to English using Google Translate with error handling and rate limiting. Results are saved to a Parquet file.\n",
    "- **Loading Results:** If rerun is disabled, the cell loads the translated dataset from the specified Parquet file. If the file does not exist, an error is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60dcba8a-11c0-4a94-b47f-e4fc32063cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 1) Control flag: decide whether to rerun translations\n",
    "# ======================================================\n",
    "rerun_translations = False   # <-- Per default, set to *False*. Set to True to run translations again\n",
    "\n",
    "# Path for the output Parquet file\n",
    "PARQUET_PATH = \"srgssr_article_corpus_translated.parquet\"\n",
    "# For DBFS you could use:\n",
    "# PARQUET_PATH = \"/dbfs/FileStore/srgssr_article_corpus_translated.parquet\"\n",
    "\n",
    "# ======================================================\n",
    "# 2) Either: run translations and save results\n",
    "#    Or: load existing Parquet file\n",
    "# ======================================================\n",
    "\n",
    "if rerun_translations:\n",
    "    print(\"üîÅ rerun_translations = True ‚Üí Running translations from scratch.\\n\")\n",
    "\n",
    "    # Copy original dataset\n",
    "    df_translated = srgssr_article_corpus.copy()\n",
    "\n",
    "    # Initialize translator\n",
    "    translator = Translator()\n",
    "\n",
    "    def translate_text(text, dest='en', max_retries=3):\n",
    "        \"\"\"Translate text with retry logic to handle API errors.\"\"\"\n",
    "        if pd.isna(text) or text == \"\":\n",
    "            return \"\"\n",
    "        \n",
    "        text_str = str(text)[:5000]  # Safety limit: Google API max length\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                result = translator.translate(text_str, dest=dest)\n",
    "                return result.text\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Translation failed after {max_retries} attempts: {str(e)[:100]}\")\n",
    "                    return text_str  # return original text when failing\n",
    "        \n",
    "        return text_str\n",
    "\n",
    "    # Run translations\n",
    "    print(\"Translating articles to English...\")\n",
    "    print(f\"Total articles: {len(df_translated)}\")\n",
    "    print(\"Note: This may take a long time depending on API speed.\\n\")\n",
    "\n",
    "    translated_texts = []\n",
    "    for idx, text in enumerate(df_translated['content_text_csv']):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Progress: {idx}/{len(df_translated)} articles translated...\")\n",
    "\n",
    "        translated = translate_text(text, dest='en')\n",
    "        translated_texts.append(translated)\n",
    "\n",
    "        # prevent rate limiting\n",
    "        if idx % 10 == 0 and idx > 0:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    df_translated['content_text_en'] = translated_texts\n",
    "\n",
    "    print(f\"\\n‚úÖ Translation finished. Total translated: {len(df_translated)}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # 3) Save results as Parquet\n",
    "    # ======================================================\n",
    "    print(f\"üíæ Saving translated dataset to: {PARQUET_PATH}\")\n",
    "    df_translated.to_parquet(PARQUET_PATH, index=False)\n",
    "    print(\"‚úÖ Parquet saved.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠ rerun_translations = False ‚Üí Skipping translation step.\")\n",
    "    print(f\"üìÇ Loading existing Parquet file from: {PARQUET_PATH}\\n\")\n",
    "\n",
    "    if not os.path.exists(PARQUET_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Parquet file '{PARQUET_PATH}' not found.\\n\"\n",
    "            \"Set rerun_translations = True to generate it first.\"\n",
    "        )\n",
    "\n",
    "    df_translated = pd.read_parquet(PARQUET_PATH)\n",
    "    print(f\"‚úÖ Loaded Parquet file. Rows: {len(df_translated)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96aff5b5-b55a-43ad-a4d5-10d9da2387ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Preview of Translated Articles by Publisher\n",
    "\n",
    "This cell displays a sample of translated articles in a presentation-friendly table. For each publisher, one original article and its English translation are shown side by side. The table is styled for readability, with wrapped text and adjusted column widths to facilitate comparison between the original and translated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a3b6c9-32be-462b-a4e5-31623e0e511c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preview the result in a notebook-friendly, presentation format\n",
    "df_preview = df_translated.groupby('publisher').apply(lambda x: x.head(1))[\n",
    "    ['content_text_csv', 'content_text_en']\n",
    "]\n",
    "df_preview.columns = ['Original Article', 'Translated Article (English)']\n",
    "\n",
    "display(\n",
    "    df_preview.style.set_properties(\n",
    "        subset=['Original Article', 'Translated Article (English)'],\n",
    "        **{'white-space': 'pre-wrap', 'max-width': '600px', 'font-size': '13px'}\n",
    "    ).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('font-size', '15px'), ('background-color', '#f0f0f0'), ('font-weight', 'bold')]},\n",
    "        {'selector': 'td', 'props': [('vertical-align', 'top')]}\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22f952d-2504-478f-a21b-1f447a29fb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Findings: Translation Pipeline\n",
    "\n",
    "- Successfully translated articles to English.\n",
    "- Enables multilingual content availability.\n",
    "- Translation quality was acceptable for internal use, but AI translation alone cannot be published on a news website due to internal editorial guidelines‚Äîthis is only a tool, not a product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f61b9db-c565-4db1-973d-7aea0f492103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Semantic Search\n",
    "\n",
    "**Use Case:** Quickly search all existing articles without needing Google.\n",
    "\n",
    "**Goal:** Enable journalists to verify if a story has already been written by colleagues in different branches.\n",
    "\n",
    "**Approach:**\n",
    "- Use text embeddings (Sentence Transformers) to represent article content\n",
    "- Enable similarity-based semantic search\n",
    "- Return most relevant articles for any query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7da8dd2-b9ec-42f0-af5d-cf2fc5a4f51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.1 Semantic Search: Embedding Articles and Querying\n",
    "\n",
    "This block prepares the dataset for semantic search by performing the following steps:\n",
    "- Selects the article text and ID columns, filling missing text values.\n",
    "- Loads a pre-trained Sentence Transformer model for generating text embeddings.\n",
    "- Encodes all article texts into dense vector embeddings, enabling efficient similarity search.\n",
    "- Stores the article IDs, texts, and their embeddings for later retrieval.\n",
    "- Defines a `semantic_search` function that, given a user query, encodes the query, computes cosine similarities with all article embeddings, and returns the top-k most similar articles as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b464b9-2989-4a11-be75-b32398c7aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TEXT_COL = \"content_text_en\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "RERUN_EMBEDDINGS = False  # <-- set True if you want to recompute\n",
    "\n",
    "# Filenames directly in the current working directory\n",
    "EMB_MATRIX_PATH = \"emb_matrix_minilm_en.npy\"\n",
    "META_PATH = \"semantic_search_meta.parquet\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Prepare data\n",
    "# -----------------------------------------------------------------------------\n",
    "df = df_translated.copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Load embedding model\n",
    "# -----------------------------------------------------------------------------\n",
    "_model = None\n",
    "def get_embedder():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return _model\n",
    "\n",
    "model = get_embedder()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Compute or load embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "if RERUN_EMBEDDINGS or not os.path.exists(EMB_MATRIX_PATH):\n",
    "    print(\"Recomputing embeddings...\")\n",
    "\n",
    "    emb_matrix = model.encode(\n",
    "        df[TEXT_COL].tolist(),\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "    np.save(EMB_MATRIX_PATH, emb_matrix)\n",
    "\n",
    "    meta = pd.DataFrame({\n",
    "        ID_COL: df[ID_COL].tolist(),\n",
    "        TEXT_COL: df[TEXT_COL].tolist(),\n",
    "    })\n",
    "    meta.to_parquet(META_PATH, index=False)\n",
    "\n",
    "    print(f\"‚úì Saved embeddings to {EMB_MATRIX_PATH}\")\n",
    "    print(f\"‚úì Saved metadata to {META_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading existing embeddings...\")\n",
    "\n",
    "    emb_matrix = np.load(EMB_MATRIX_PATH)\n",
    "    meta = pd.read_parquet(META_PATH)\n",
    "\n",
    "    print(f\"‚úì Loaded embeddings: {emb_matrix.shape}\")\n",
    "    print(f\"‚úì Loaded metadata rows: {len(meta)}\")\n",
    "\n",
    "# Make lookup lists for semantic search\n",
    "ids = meta[ID_COL].tolist()\n",
    "texts = meta[TEXT_COL].tolist()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Semantic search\n",
    "# -----------------------------------------------------------------------------\n",
    "def semantic_search(query: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Search for semantically similar articles.\"\"\"\n",
    "    q = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = emb_matrix @ q\n",
    "\n",
    "    k = min(top_k, len(sims) - 1)\n",
    "    top_idx = np.argpartition(-sims, kth=k)[:top_k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"id\": [ids[i] for i in top_idx],\n",
    "        \"content_text_csv\": [texts[i] for i in top_idx],  # <- wieder so wie vorher\n",
    "        \"similarity\": [float(sims[i]) for i in top_idx],\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e5f2a5-c102-47fc-9747-62aaa1366e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.2 Semantic Search: Embedding Articles and Querying\n",
    "\n",
    "This cell prepares the dataset for semantic search by:\n",
    "- Selecting the article text and ID columns, filling missing text values.\n",
    "- Loading a pre-trained Sentence Transformer model for generating text embeddings.\n",
    "- Encoding all article texts into dense vector embeddings, enabling efficient similarity search.\n",
    "- Storing the article IDs, texts, and their embeddings for later retrieval.\n",
    "- Defining a `semantic_search` function that, given a user query, encodes the query, computes cosine similarities with all article embeddings, and returns the top-k most similar articles as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c676802-a290-4139-a8f9-3fc225dd7d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example search\n",
    "# Define serach query\n",
    "query = \"climate change\"\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Example: Searching for '{query}' articles\")\n",
    "print(\"=\"*80)\n",
    "results = semantic_search(query, top_k=10)\n",
    "results = results.merge(df[[ID_COL, \"resources_locator_urls_csv\"]], left_on=\"id\", right_on=ID_COL, how=\"left\")\n",
    "results = results.drop_duplicates(subset=\"id\")  # Remove duplicate IDs\n",
    "for idx, row in results.iterrows():\n",
    "    text = row['content_text_csv']\n",
    "    truncated_text = text[:500] + \"...\" if len(text) > 500 else text\n",
    "    print(f\"ID: {row['id']}\\nURL: {row['resources_locator_urls_csv']}\\nSimilarity: {row['similarity']:.4f}\\nText: {truncated_text}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27935939-410c-42e7-a363-24d66cf9f136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.3 Key Findings: Semantic Search\n",
    "\n",
    "- Efficient semantic search implemented using Sentence Transformers.\n",
    "- Enables journalists to quickly find related articles.\n",
    "- Reduces duplicate story creation across branches.\n",
    "- Simple and effective for this exercise.\n",
    "- Note: Generating embeddings is already slow on this sample; scaling to the full dataset may cause significant performance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9f50e5-a895-41cf-b431-65f5dc7091c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Topic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664bc5b3-d76a-48bb-8b3f-608ac66a0fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.1 Method A: K-means (and umap projection)\n",
    "\n",
    "This section implements topic clustering using K-means with:\n",
    "\n",
    "- **Embeddings**: Sentence Transformers (MiniLM-L6-v2) to represent article content\n",
    "- **K-means**: Centroid-based clustering with a fixed number of clusters (k=10)\n",
    "- **UMAP**: Dimensionality reduction for 2D visualization of clusters\n",
    "- **Keyword extraction**: Top keywords per cluster for human-readable topic labels\n",
    "- **Visualizations**: UMAP scatter plots and cluster distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07ff1d55-4365-41ef-b422-7a1a67e57961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.1.1 K-means Clustering and Keyword Extraction\n",
    "\n",
    "K-means clustering is used to group multilingual news articles by content similarity, leveraging their dense vector embeddings. The process involves:\n",
    "\n",
    "- **Clustering**: Articles are clustered based on semantic similarity in their embeddings.\n",
    "- **Keyword Extraction**: For each cluster, the most frequent and meaningful words (excluding stopwords) are identified as representative topic keywords.\n",
    "- **Interpretability**: Each article is assigned to a cluster, and clusters are labeled using these keywords, enabling clear and interpretable topic groupings within the dataset.\n",
    "\n",
    "This approach helps uncover the main themes present in the news corpus and supports efficient topic-based exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca636a56-22f1-4ebc-82d6-ce0c5d109b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG ‚Äî compute only when True\n",
    "# -----------------------------------------------------------------------------\n",
    "RERUN_CLUSTERING = False   # <-- set True to recompute everything\n",
    "\n",
    "# File paths in current working directory\n",
    "EMB_PATH = \"emb_matrix_en.npy\"\n",
    "LABELS_PATH = \"cluster_labels_en.npy\"\n",
    "DF_CLUSTERS_PATH = \"df_clusters_en.parquet\"\n",
    "TOPIC_LABELS_PATH = \"topic_labels_en.json\"\n",
    "\n",
    "n_clusters_en = 10\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# -----------------------------------------------------------------------------\n",
    "df_en = pd.DataFrame(df_translated)\n",
    "df_en[\"content_text_en\"] = df_en[\"content_text_en\"].fillna(\"\").astype(str)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RUN OR LOAD EMBEDDINGS + CLUSTERING\n",
    "# -----------------------------------------------------------------------------\n",
    "should_recompute = (\n",
    "    RERUN_CLUSTERING or\n",
    "    not os.path.exists(EMB_PATH) or\n",
    "    not os.path.exists(LABELS_PATH) or\n",
    "    not os.path.exists(DF_CLUSTERS_PATH) or\n",
    "    not os.path.exists(TOPIC_LABELS_PATH)\n",
    ")\n",
    "\n",
    "if should_recompute:\n",
    "    print(\"Recomputing embeddings + clustering...\")\n",
    "\n",
    "    # ---- Embeddings ----\n",
    "    model_en = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    emb_matrix_en = model_en.encode(\n",
    "        df_en[\"content_text_en\"].tolist(),\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "    np.save(EMB_PATH, emb_matrix_en)\n",
    "\n",
    "    # ---- K-means clustering ----\n",
    "    kmeans_en = KMeans(n_clusters=n_clusters_en, random_state=42, n_init=\"auto\")\n",
    "    labels_en = kmeans_en.fit_predict(emb_matrix_en)\n",
    "    np.save(LABELS_PATH, labels_en)\n",
    "\n",
    "    df_clusters_en = pd.DataFrame({\n",
    "        \"id\": df_en['id'].tolist(),\n",
    "        \"original_text\": df_en['content_text_csv'].tolist(),\n",
    "        \"translated_text_en\": df_en['content_text_en'].tolist(),\n",
    "        \"cluster\": labels_en\n",
    "    })\n",
    "    df_clusters_en.to_parquet(DF_CLUSTERS_PATH, index=False)\n",
    "\n",
    "    # ---- Keyword extraction ----\n",
    "    def get_topic_keywords_en(cluster_id, df_clusters, top_n=3):\n",
    "        cluster_texts = df_clusters[df_clusters[\"cluster\"] == cluster_id][\"translated_text_en\"].tolist()\n",
    "        combined_text = \" \".join(cluster_texts).lower()\n",
    "\n",
    "        words = re.findall(r'\\b[a-z]{4,}\\b', combined_text)\n",
    "\n",
    "        stopwords = {\n",
    "            'this','that','with','from','have','been','were','their','what','which','when','where',\n",
    "            'there','will','would','could','should','about','after','also','many','more','most','other',\n",
    "            'some','such','than','them','then','these','they','very','into','just','like','only','over',\n",
    "            'said','same','says','does','make','made','well','much','even','back','through','year',\n",
    "            'years','being','people','according','since','during'\n",
    "        }\n",
    "\n",
    "        words = [w for w in words if w not in stopwords and len(w) > 3]\n",
    "        word_counts = Counter(words)\n",
    "        top_words = [w for w, c in word_counts.most_common(top_n)]\n",
    "        return \", \".join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "    topic_labels_en = {}\n",
    "    for cluster_id in range(n_clusters_en):\n",
    "        keywords = get_topic_keywords_en(cluster_id, df_clusters_en, top_n=10)\n",
    "        topic_labels_en[cluster_id] = keywords\n",
    "\n",
    "    with open(TOPIC_LABELS_PATH, \"w\") as f:\n",
    "        json.dump(topic_labels_en, f)\n",
    "\n",
    "    print(\"‚úì Saved embeddings, clusters, metadata, and topic labels.\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading existing embeddings + clustering results...\")\n",
    "\n",
    "    emb_matrix_en = np.load(EMB_PATH)\n",
    "    labels_en = np.load(LABELS_PATH)\n",
    "    df_clusters_en = pd.read_parquet(DF_CLUSTERS_PATH)\n",
    "\n",
    "    with open(TOPIC_LABELS_PATH, \"r\") as f:\n",
    "        topic_labels_en = json.load(f)\n",
    "\n",
    "    print(\"‚úì Loaded data successfully.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ADD LABELS + PRINT SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "df_clusters_en[\"cluster_topic\"] = df_clusters_en[\"cluster\"].map(topic_labels_en)\n",
    "\n",
    "print(\"\\nCluster Topics (based on translated English text):\")\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    count = (df_clusters_en[\"cluster\"] == cluster_id).sum()\n",
    "    print(f\"Cluster {cluster_id}: {topic_labels_en[str(cluster_id)]} ({count} articles)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "766fe98e-4b3d-415e-aae3-261bf3258903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.1.2 Cluster-to-Category Mapping and Label Assignment\n",
    "\n",
    "This section assigns interpretable topic categories to each previously computed cluster by combining manual labels with a generic keyword-based fallback system. The process includes:\n",
    "\n",
    "- **Keyword-Based Category Matching**: A predefined dictionary of thematic categories (e.g., Politics, Sports, Economy) is matched against each cluster‚Äôs representative keywords. If a cluster contains multiple relevant terms, the category with the highest match score is selected as a fallback.\n",
    "\n",
    "- **Manual ‚ÄúPretty‚Äù Labels**: For improved clarity and readability, specific cluster IDs receive manually curated descriptive labels (e.g., Swiss politics & institutions, Conflict & war ‚Äì Middle East). These override the fallback system whenever available.\n",
    "\n",
    "- **High-Level Grouping**: In addition to detailed labels, each cluster is also assigned a broad, high-level category (e.g., Politics, Conflict & War, Economy & Business) to support more abstract analysis and simplified reporting.\n",
    "\n",
    "- **Final Mapping & Distribution**: The enhanced and high-level category labels are merged back into the main DataFrame and the distribution of articles across categories is printed, providing an overview of topic dominance within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936ca732-ee9a-4428-b432-a27f96af6925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# --- 1. Load topic_labels_en (z.B. aus BERTopic oder eigener Logik) ---\n",
    "with open(TOPIC_LABELS_PATH, \"r\") as f:\n",
    "    topic_labels_en = json.load(f)\n",
    "\n",
    "# Keys von str -> int casten\n",
    "topic_labels_en = {int(k): v for k, v in topic_labels_en.items()}\n",
    "\n",
    "\n",
    "def make_keyword_label(raw_value, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Erzeugt ein Label aus den wichtigsten top_k Keywords.\n",
    "    Funktioniert sowohl, wenn raw_value eine Liste oder ein String ist.\n",
    "    \"\"\"\n",
    "    # Fall 1: Liste von Keywords\n",
    "    if isinstance(raw_value, (list, tuple)):\n",
    "        keywords = [str(x).strip() for x in raw_value if str(x).strip()]\n",
    "        keywords = keywords[:top_k]\n",
    "\n",
    "    # Fall 2: String (z.B. \"switzerland, swiss, federal, climate, vote\")\n",
    "    elif isinstance(raw_value, str):\n",
    "        # zuerst nach Kommas splitten\n",
    "        parts = re.split(r\"[,\\n]\", raw_value)\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "        # falls das sehr kurz ist, zus√§tzlich nach Leerzeichen splitten\n",
    "        if len(parts) < top_k:\n",
    "            extra = re.split(r\"\\s+\", raw_value)\n",
    "            extra = [e.strip() for e in extra if e.strip()]\n",
    "            # bestehende behalten, neue erg√§nzen, Duplikate vermeiden\n",
    "            for token in extra:\n",
    "                if token not in parts:\n",
    "                    parts.append(token)\n",
    "        keywords = parts[:top_k]\n",
    "\n",
    "    # Fallback\n",
    "    else:\n",
    "        keywords = [str(raw_value)]\n",
    "\n",
    "    return \", \".join(keywords)\n",
    "\n",
    "\n",
    "# --- 2. Mapping Cluster -> Keyword-Label ---\n",
    "\n",
    "cluster_keyword_labels = {}\n",
    "\n",
    "print(\"\\nCluster keyword labels:\\n\")\n",
    "print(f\"{'Cluster':<10} {'Raw Topic Labels':<60} {'Top-5 Keyword Label':<60}\")\n",
    "print(\"=\" * 140)\n",
    "\n",
    "for cluster_id in range(n_clusters_en):\n",
    "    raw_topic_value = topic_labels_en.get(cluster_id, \"\")\n",
    "    keyword_label = make_keyword_label(raw_topic_value, top_k=5)\n",
    "\n",
    "    cluster_keyword_labels[cluster_id] = keyword_label\n",
    "\n",
    "    print(f\"{cluster_id:<10} {str(raw_topic_value)[:58]:<60} {keyword_label:<60}\")\n",
    "\n",
    "# --- 3. In den DataFrame schreiben ---\n",
    "\n",
    "df_clusters_en[\"cluster_label_keywords\"] = df_clusters_en[\"cluster\"].map(cluster_keyword_labels)\n",
    "\n",
    "# Optional: Verteilung der Cluster mit Keyword-Labels anzeigen\n",
    "print(\"\\nArticle distribution by keyword-based cluster labels:\")\n",
    "print(\"=\" * 70)\n",
    "label_counts = df_clusters_en[\"cluster_label_keywords\"].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(df_clusters_en)) * 100\n",
    "    print(f\"{label:<45} {count:>5} articles ({percentage:>5.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef74b9a-f8a7-48f2-b6f1-0d2d9ed0bfd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.1.3 UMAP Visualization of Topic Clusters\n",
    "\n",
    "This section visualizes the semantic structure of the clustered articles using a 2-dimensional UMAP projection. The goal is to provide an intuitive, visual understanding of how clusters relate to one another in embedding space. The procedure includes:\n",
    "\n",
    "- **Dimensionality Reduction (UMAP)**:  \n",
    "  High-dimensional sentence embeddings are reduced to two dimensions using UMAP, preserving local and global semantic relationships. This allows clusters to be visualized spatially in a scatter plot.\n",
    "\n",
    "- **Color-Coding by Category**:  \n",
    "  Each enhanced topic category receives a unique color, enabling quick identification of cluster boundaries and thematic groupings within the projected space.\n",
    "\n",
    "- **Scatter Plot of Article Clusters**:  \n",
    "  Articles are plotted according to their UMAP coordinates, colored by their assigned topic category. This reveals  \n",
    "  - how well-separated clusters are,  \n",
    "  - where thematic overlap exists,  \n",
    "  - and how articles distribute within and across categories.\n",
    "\n",
    "- **Pie Chart of Category Distribution**:  \n",
    "  A complementary pie chart summarises the overall share of articles per enhanced topic category, offering a high-level view of dominant and less frequent themes in the corpus.\n",
    "\n",
    "Together, these visualisations provide both **spatial** and **quantitative** insights into the structure of the multilingual news dataset, supporting interpretation, quality checks, and communication of clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24b44bb-207c-462e-ac53-9fbe046744b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from umap import UMAP  # nur falls noch nicht importiert\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create UMAP projection for translated text\n",
    "reducer_en = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d_en = reducer_en.fit_transform(emb_matrix_en)\n",
    "\n",
    "# Color map for keyword-based cluster labels\n",
    "unique_labels = sorted(df_clusters_en[\"cluster_label_keywords\"].unique())\n",
    "category_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "category_color_map = {lab: color for lab, color in zip(unique_labels, category_colors)}\n",
    "\n",
    "# Create visualization\n",
    "fig = plt.figure(figsize=(22, 10))\n",
    "\n",
    "# -----------------------------------\n",
    "# Left: Scatter plot by label\n",
    "# -----------------------------------\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "for label in unique_labels:\n",
    "    mask = df_clusters_en[\"cluster_label_keywords\"] == label\n",
    "    indices = df_clusters_en[mask].index\n",
    "\n",
    "    ax1.scatter(\n",
    "        embedding_2d_en[indices, 0],\n",
    "        embedding_2d_en[indices, 1],\n",
    "        c=[category_color_map[label]],\n",
    "        label=label,\n",
    "        alpha=0.7,\n",
    "        s=60,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "\n",
    "ax1.set_title(\n",
    "    \"Article Clusters with Keyword-based Labels\\n(UMAP 2D Projection)\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "ax1.set_xlabel(\"UMAP Dimension 1\", fontsize=12)\n",
    "ax1.set_ylabel(\"UMAP Dimension 2\", fontsize=12)\n",
    "ax1.legend(loc=\"best\", fontsize=9, framealpha=0.9, edgecolor=\"black\")\n",
    "ax1.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Right: Pie chart by label\n",
    "# -----------------------------------\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "label_counts = df_clusters_en[\"cluster_label_keywords\"].value_counts()\n",
    "colors = [category_color_map[lab] for lab in label_counts.index]\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(\n",
    "    label_counts.values,\n",
    "    labels=label_counts.index,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors,\n",
    "    textprops={\"fontsize\": 10, \"weight\": \"bold\"},\n",
    "    wedgeprops={\"edgecolor\": \"black\", \"linewidth\": 1.5},\n",
    ")\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color(\"black\")\n",
    "    autotext.set_fontsize(9)\n",
    "\n",
    "ax2.set_title(\n",
    "    \"Distribution of Articles by Keyword-based Cluster Labels\\n\"\n",
    "    + f\"(Total: {len(df_clusters_en)} articles)\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bacd8cb-e199-4b60-b753-f9db9fc294b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.2 Method B: BERTopic (Custom Embeddings, UMAP & HDBSCAN)\n",
    "\n",
    "This section implements a more advanced, fully unsupervised topic-modeling workflow using **BERTopic**, combining state-of-the-art embeddings with density-based clustering and automated topic extraction. Compared to K-means, this method adapts dynamically to the structure of the corpus. This BERTopic pipeline delivers a flexible and scalable method for identifying themes in large-scale news corpora, allowing for more adaptive and data-driven topic discovery than fixed-cluster K-means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7cb46b6-57c0-4446-aba2-5f6fa872c876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.1 BERTopic Pipeline with Custom Embeddings, UMAP Reduction, and HDBSCAN Clustering\n",
    "\n",
    "This section builds a complete BERTopic pipeline using a **custom embedding model**, UMAP dimensionality reduction, and HDBSCAN density-based clustering to extract robust and interpretable topics from the translated text corpus. The main components are:\n",
    "\n",
    "- **Custom Embedding Class**:  \n",
    "  A lightweight wrapper is implemented to generate sentence embeddings using the `nvidia/llama-embed-nemotron-8b` model.  \n",
    "  - Texts are tokenized, padded, and truncated to 512 tokens.  \n",
    "  - The model computes contextual embeddings, and the mean-pooled token representations form the final sentence vectors.  \n",
    "  This ensures compatibility with BERTopic and gives full control over embedding generation.\n",
    "\n",
    "- **Model & Data Loading**:  \n",
    "  The NVIDIA model is loaded with automatic device placement for efficient GPU execution. All translated article texts are retrieved and prepared for processing.\n",
    "\n",
    "- **UMAP Configuration**:  \n",
    "  UMAP reduces the high-dimensional embedding space into 5 components using cosine distance.  \n",
    "  - `n_neighbors` controls the balance between local and global structure,  \n",
    "  - `min_dist` adjusts clustering tightness,  \n",
    "  - `random_state` ensures reproducibility.\n",
    "\n",
    "- **HDBSCAN Clustering Setup**:  \n",
    "  HDBSCAN groups articles into dense semantic regions without requiring a predefined number of clusters.  \n",
    "  - `min_cluster_size` determines the minimum topic size,  \n",
    "  - `min_samples` controls how conservative clustering is,  \n",
    "  - `cluster_selection_method='eom'` identifies stable topic regions.\n",
    "\n",
    "- **BERTopic Assembly**:  \n",
    "  BERTopic is created using:  \n",
    "  - the custom embedder,  \n",
    "  - the configured UMAP and HDBSCAN models,  \n",
    "  - a CountVectorizer with customised stopwords.  \n",
    "  This pipeline enables nuanced topic discovery across a large multilingual corpus.\n",
    "\n",
    "- **Topic Modeling Execution**:  \n",
    "  The model is trained on all documents, returning both topic assignments and cluster probabilities.  \n",
    "  Topics are added back to the cleaned DataFrame for further analysis.\n",
    "\n",
    "- **Results & Inspection**:  \n",
    "  A summary of discovered topics is printed, including:  \n",
    "  - number of topics,  \n",
    "  - topic metadata table,  \n",
    "  - sample articles with their assigned topics.  \n",
    "  This allows rapid evaluation of topic quality and interpretability.\n",
    "\n",
    "This workflow provides a scalable and highly configurable approach to topic modeling that leverages state-of-the-art embeddings and modern clustering techniques to uncover meaningful thematic structures in the news corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13e20e7-f329-42c5-8161-065ea164bffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "RERUN_BERTOPIC = False  # <-- auf True setzen, wenn du NEU fitten willst\n",
    "\n",
    "TOPIC_MODEL_DIR = \"bertopic_nemotron_model\"\n",
    "DF_TOPICS_PATH = \"articles_with_topics.parquet\"\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EMBEDDER\n",
    "# =============================================================================\n",
    "class CustomEmbedder:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encode(self, texts, **kwargs):\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOGIC: ENTWEDER NEU BERECHNEN ODER LADEN\n",
    "# =============================================================================\n",
    "def build_or_load_bertopic_model(df_translated):\n",
    "    \"\"\"Build BERTopic model if necessary, otherwise load from disk.\"\"\"\n",
    "    should_recompute = (\n",
    "        RERUN_BERTOPIC\n",
    "        or not os.path.exists(TOPIC_MODEL_DIR)\n",
    "        or not os.path.exists(DF_TOPICS_PATH)\n",
    "    )\n",
    "    \n",
    "    if should_recompute:\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 1) Modell + Daten laden\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Recomputing BERTopic model...\")\n",
    "\n",
    "        print(\"Loading embedding model...\")\n",
    "        MODEL_NAME = \"nvidia/llama-embed-nemotron-8b\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        print(f\"‚úì Loaded {MODEL_NAME}\")\n",
    "\n",
    "        print(\"\\nLoading data...\")\n",
    "        df = df_translated.copy()\n",
    "        df = df.dropna(subset=[\"content_text_en\"]).reset_index(drop=True)\n",
    "        docs = df[\"content_text_en\"].tolist()\n",
    "        print(f\"‚úì Loaded {len(docs)} documents\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 2) UMAP + HDBSCAN konfigurieren\n",
    "        # ---------------------------------------------------------------------\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=20,\n",
    "            n_components=5,\n",
    "            min_dist=0.0,\n",
    "            metric=\"cosine\",\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=220,\n",
    "            min_samples=15,\n",
    "            metric=\"euclidean\",\n",
    "            cluster_selection_method=\"eom\",\n",
    "            prediction_data=True,\n",
    "        )\n",
    "\n",
    "        print(\"\\nSetting up BERTopic...\")\n",
    "        embedding_model = CustomEmbedder(model, tokenizer)\n",
    "\n",
    "        custom_stopwords = list(ENGLISH_STOP_WORDS) + [\"said\", \"efe\"]\n",
    "        vectorizer_model = CountVectorizer(stop_words=custom_stopwords)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            verbose=True,\n",
    "        )\n",
    "        print(\"‚úì BERTopic configured\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3) Fit + Transform\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"\\nRunning topic modeling...\")\n",
    "        topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "        df_clean = df.copy()\n",
    "        df_clean[\"topic\"] = topics\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 4) Speichern\n",
    "        # ---------------------------------------------------------------------\n",
    "        df_clean.to_parquet(DF_TOPICS_PATH, index=False)\n",
    "        topic_model.save(TOPIC_MODEL_DIR)\n",
    "\n",
    "        print(f\"\\n‚úì Saved articles with topics to: {DF_TOPICS_PATH}\")\n",
    "        print(f\"‚úì Saved BERTopic model to: {TOPIC_MODEL_DIR}\")\n",
    "\n",
    "    else:\n",
    "        # ---------------------------------------------------------------------\n",
    "        # NUR LADEN\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"Loading existing BERTopic model and topics from disk...\")\n",
    "        topic_model = BERTopic.load(TOPIC_MODEL_DIR)\n",
    "        df_clean = pd.read_parquet(DF_TOPICS_PATH)\n",
    "        print(\"‚úì Loaded existing model and topic assignments\")\n",
    "\n",
    "    return topic_model, df_clean\n",
    "\n",
    "# =============================================================================\n",
    "# AUSF√úHREN\n",
    "# =============================================================================\n",
    "topic_model, df_clean = build_or_load_bertopic_model(df_translated)\n",
    "\n",
    "# =============================================================================\n",
    "# AUSGABEN / DIAGNOSTIK\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTS: Found {len(set(df_clean['topic']))} topics\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "display(topic_info)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAMPLE ARTICLES WITH TOPICS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "display(df_clean[[\"content_text_en\", \"topic\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83913eff-817d-460e-99d3-179e08f2418e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for topic_id in topic_info['Topic']:\n",
    "    if topic_id == -1:\n",
    "        continue\n",
    "    keywords = topic_model.get_topic(topic_id)\n",
    "    if keywords:\n",
    "        keywords_str = ', '.join([word for word, _ in keywords])\n",
    "        print(f\"Topic {topic_id}: {keywords_str}\")\n",
    "    else:\n",
    "        print(f\"Topic {topic_id}: (no keywords)\")\n",
    "if -1 in topic_info['Topic'].values:\n",
    "    keywords = topic_model.get_topic(-1)\n",
    "    if keywords:\n",
    "        keywords_str = ', '.join([word for word, _ in keywords])\n",
    "        print(f\"Topic -1: {keywords_str}\")\n",
    "    else:\n",
    "        print(\"Topic -1: (no keywords)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01ceccd8-fb2e-4349-bb13-4a0a5646d754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.2 Exporting Topic-Enhanced Articles to CSV\n",
    "\n",
    "In this final step, the clustered articles are prepared for export by attaching human-readable topic labels and writing the enriched dataset to a CSV file. The process includes:\n",
    "\n",
    "- **Custom Topic Label Definition**:  \n",
    "  A dictionary maps each numerical topic ID (including the outlier cluster `-1`) to a descriptive thematic label.  \n",
    "  These labels provide clearer interpretations, such as *Swiss Domestic Affairs & Policy* or *Middle East Conflict & Humanitarian Issues*.\n",
    "\n",
    "- **Label Assignment**:  \n",
    "  Each article's assigned topic ID is matched with its corresponding descriptive label and written into a new column (`topic_label`).  \n",
    "  This enhances readability and makes the dataset suitable for reporting, dashboards, or editorial review.\n",
    "\n",
    "- **Export to CSV**:  \n",
    "  The full dataset‚Äîcontaining article text, topic IDs, and descriptive topic labels‚Äîis exported as `articles_with_topics.csv`.  \n",
    "  UTF-8 encoding ensures compatibility across analytical tools and platforms.\n",
    "\n",
    "This export step provides a clean, labelled dataset ready for downstream analysis, sharing with stakeholders, or integration into BI tools such as Power BI or Databricks SQL dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b9b1d4-4d44-40a1-a33f-f1019d815895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS TO CSV\n",
    "# ============================================================================\n",
    "print(\"\\nPreparing data for export...\")\n",
    "\n",
    "# Helper: build a label from the top N BERTopic keywords\n",
    "def make_topic_label(topic_id, topic_model, top_k: int = 5) -> str:\n",
    "    \"\"\"Create a short label from the top_k words of a BERTopic topic.\"\"\"\n",
    "    if pd.isna(topic_id):\n",
    "        return \"Unknown\"\n",
    "    topic_id_int = int(topic_id)\n",
    "\n",
    "    if topic_id_int == -1:\n",
    "        return \"Outliers / Miscellaneous\"\n",
    "\n",
    "    words = topic_model.get_topic(topic_id_int)\n",
    "    if not words:\n",
    "        return f\"Topic {topic_id_int}\"\n",
    "\n",
    "    top_words = [w for w, _ in words[:top_k]]\n",
    "    return \", \".join(top_words)\n",
    "\n",
    "# Create topic_label column directly from BERTopic model\n",
    "df_clean[\"topic_label\"] = df_clean[\"topic\"].apply(\n",
    "    lambda t: make_topic_label(t, topic_model, top_k=5)\n",
    ")\n",
    "\n",
    "# Save all articles with topics\n",
    "output_file = \"articles_with_topics.csv\"\n",
    "df_clean.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "print(f\"‚úì Saved all articles to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4f63238-d50a-40ea-9848-8440d3838af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.3 Visualizing Topic Distribution Across the Corpus\n",
    "\n",
    "This step generates a clean and interpretable visualization of how frequently each topic appears within the full article dataset. The process includes:\n",
    "\n",
    "- **Topic Share Calculation**:  \n",
    "  Each article‚Äôs descriptive topic label is counted, and the relative frequency (in percent) of each topic is computed.  \n",
    "  Sorting the percentages from smallest to largest ensures that less common topics remain visible in the final chart.\n",
    "\n",
    "- **Horizontal Bar Chart Construction**:  \n",
    "  A horizontal bar plot is created to display the percentage share of each topic category.  \n",
    "  - A smooth **viridis** color palette is applied for a visually appealing gradient.  \n",
    "  - Each bar represents one topic, and the length corresponds to its share of the corpus.\n",
    "\n",
    "- **Bar Labeling & Styling**:  \n",
    "  Percentage labels are added directly next to each bar for easy interpretation.  \n",
    "  The chart uses minimalist styling‚Äîhidden top/right spines and subtle gridlines‚Äîto keep attention on the distribution itself.\n",
    "\n",
    "- **Exporting the Chart**:  \n",
    "  The final visualization is saved as a high-resolution PNG file (`topic_distribution.png`), making it suitable for presentations, reports, or dashboards.\n",
    "\n",
    "This visual summary provides a quick overview of dominant and niche themes in the corpus, helping stakeholders understand topic prevalence at a glance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6262004-0e26-49eb-9ae8-c5b50f424929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate topic percentages\n",
    "topic_counts = df_clean['topic_label'].value_counts()\n",
    "topic_percentages = (topic_counts / len(df_clean) * 100).sort_values(ascending=True)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Use a nice color palette (viridis, but you can try: 'plasma', 'cividis', 'coolwarm')\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(topic_percentages)))\n",
    "\n",
    "bars = ax.barh(topic_percentages.index, topic_percentages.values, color=colors)\n",
    "\n",
    "# Add bar labels\n",
    "for i, (bar, value) in enumerate(zip(bars, topic_percentages.values)):\n",
    "    ax.text(value + 0.3, i, f'{value:.1f}%', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Minimal styling\n",
    "ax.set_xlabel('Percentage of Corpus (%)', fontsize=11)\n",
    "ax.set_ylabel('')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Saved chart to: topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "757a8b6f-af60-4bd9-8ee7-68c77302e42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2.4 Topic Trends Over Time ‚Äì Small Multiples\n",
    "\n",
    "This section analyses how topic prevalence evolves over time and visualises daily article counts per topic using a **compact small-multiples layout**. The goal is to detect temporal patterns, spikes, and event-driven dynamics across topics. The workflow consists of:\n",
    "\n",
    "- **Data Loading and Date Parsing**:  \n",
    "  The enriched dataset `articles_with_topics.csv` is loaded, and publication timestamps (`releaseDate`) are converted into proper datetime objects.  \n",
    "  A simplified `date` column (YYYY-MM-DD) is created to aggregate articles at daily resolution.\n",
    "\n",
    "- **Topic Ordering and Outlier Handling**:  \n",
    "  Total article counts per `topic_label` are computed and used to sort topics by overall importance.  \n",
    "  Any outlier category (e.g. *Outliers / Unassigned*) is detected and placed at the end of the visualisation to keep focus on meaningful topics.\n",
    "\n",
    "- **Daily Topic Aggregation**:  \n",
    "  The number of articles per topic and per day is calculated, resulting in a time series for each topic across the selected date range (here: 27 October 2025 to 5 November 2025).\n",
    "\n",
    "- **Small Multiples Line Charts**:  \n",
    "  For each topic, a separate small line chart is drawn in a grid layout:  \n",
    "  - Daily counts are plotted as a line with markers,  \n",
    "  - A shared y-axis scale is derived from the global maximum to allow visual comparison across panels,  \n",
    "  - Each subplot includes a compact title and a legend indicating the **total** number of articles for that topic.\n",
    "\n",
    "- **Axis Formatting and Export**:  \n",
    "  Dates on the x-axis are displayed in a short `MM-DD` format with a two-day interval for readability.  \n",
    "  The final multi-panel figure is saved as `topics_over_time_small_multiples.png`, ready for use in reports or presentations.\n",
    "\n",
    "These compact small multiples make it easy to compare temporal dynamics across topics, highlight news-driven peaks, and identify which themes are persistent versus event-specific in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4accab91-f069-4de3-b5eb-a0722ece742a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TOPIC COUNTS OVER TIME - SMALL MULTIPLES (COMPACT)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"articles_with_topics.csv\")\n",
    "print(f\"‚úì Loaded {len(df)} articles\")\n",
    "\n",
    "# Parse dates\n",
    "df['releaseDate'] = pd.to_datetime(df['releaseDate'])\n",
    "df['date'] = df['releaseDate'].dt.date\n",
    "print(f\"Date range: {df['releaseDate'].min()} to {df['releaseDate'].max()}\")\n",
    "\n",
    "# Set date limits for x-axis\n",
    "date_min = datetime(2025, 10, 27)\n",
    "date_max = datetime(2025, 11, 5)\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE DATA\n",
    "# ============================================================================\n",
    "# Calculate total counts per topic\n",
    "topic_totals = df.groupby('topic_label').size().reset_index(name='total')\n",
    "topic_totals = topic_totals.sort_values('total', ascending=False)\n",
    "\n",
    "# Separate outliers from other topics\n",
    "outliers_mask = topic_totals['topic_label'] == 'Outliers / Unassigned'\n",
    "outliers_topic = topic_totals[outliers_mask]['topic_label'].tolist()\n",
    "other_topics = topic_totals[~outliers_mask]['topic_label'].tolist()\n",
    "\n",
    "# Combine: sorted topics + outliers at the end\n",
    "topics = other_topics + outliers_topic\n",
    "\n",
    "daily_counts = df.groupby(['topic_label', 'date']).size().reset_index(name='count')\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SMALL MULTIPLES (COMPACT)\n",
    "# ============================================================================\n",
    "print(\"\\nCreating compact small multiples chart...\")\n",
    "n_topics = len(topics)\n",
    "n_cols = 4  # More columns = more compact\n",
    "n_rows = (n_topics + n_cols - 1) // n_cols\n",
    "\n",
    "# Reduced figure size\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 2.5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else axes\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    topic_data = daily_counts[daily_counts['topic_label'] == topic].copy()\n",
    "    topic_data = topic_data.sort_values('date')\n",
    "    topic_data['date'] = pd.to_datetime(topic_data['date'])\n",
    "    \n",
    "    # Calculate total\n",
    "    total = topic_data['count'].sum()\n",
    "    \n",
    "    # Plot with label for legend\n",
    "    ax.plot(topic_data['date'], topic_data['count'], \n",
    "            color='steelblue', linewidth=1.5, marker='o', markersize=3, \n",
    "            markerfacecolor='steelblue', markeredgecolor='white', markeredgewidth=0.5,\n",
    "            label=f'Total: {total:,}')\n",
    "    \n",
    "    # Smaller title with less padding\n",
    "    ax.set_title(topic, fontsize=9, fontweight='bold', pad=5)\n",
    "    \n",
    "    # Set a flexible, shared y-limit for all plots based on the global max\n",
    "    global_max = daily_counts['count'].max()\n",
    "    y_max = int((global_max + 24) // 25 * 25)  # round up to next 25\n",
    "    ax.set_ylim(0, y_max)\n",
    "    \n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax.set_xlim(date_min, date_max)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))  # Shorter date format\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), fontsize=7)\n",
    "    \n",
    "    # Add legend (text only, no marker or line)\n",
    "    ax.legend(loc='upper right', fontsize=7, framealpha=0.9, handlelength=0, handletextpad=0, markerscale=0)\n",
    "\n",
    "for idx in range(n_topics, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout(h_pad=1.5, w_pad=1.5)\n",
    "plt.savefig('topics_over_time_small_multiples.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Saved: topics_over_time_small_multiples.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33737eaa-649d-43db-968e-fe0c86c341cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5.3 Method Comparison: K-means vs. BERTopic\n",
    "\n",
    "This section compares the two topic modeling approaches applied to the same article corpus:\n",
    "\n",
    "- **Method A ‚Äì K-means**  \n",
    "  - Fixed number of clusters (e.g. *k = 10*)  \n",
    "  - Centroid-based partitioning of MiniLM embeddings  \n",
    "  - Topics inferred via keyword extraction and manual labeling  \n",
    "  - Simpler, faster, but less flexible regarding cluster shape and number\n",
    "\n",
    "- **Method B ‚Äì BERTopic (UMAP + HDBSCAN)**  \n",
    "  - Uses custom LLaMA-based embeddings  \n",
    "  - Reduces dimensionality with UMAP  \n",
    "  - Finds dense regions with HDBSCAN (no fixed k)  \n",
    "  - Automatically extracts topic terms and allows custom labeling  \n",
    "  - More adaptive to corpus structure and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "075c10d2-ebd6-4ff0-a3f7-bc8677cf1405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5.3.1 Comparing the Output (Clusters / Topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcd9eec-e309-4fb9-a584-4d6bba382669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge K-means cluster info and BERTopic info into original df_translated\n",
    "\n",
    "# Ensure 'id' is present in all DataFrames\n",
    "df_kmeans = df_clusters_en[['id', 'cluster', 'cluster_label_keywords']]\n",
    "df_kmeans = df_kmeans.rename(columns={\n",
    "    'cluster': 'kmeans_cluster',\n",
    "    'cluster_label_keywords': 'kmeans_cluster_topic',\n",
    "    'topic_category_enhanced': 'kmeans_topic_category_enhanced',\n",
    "    'topic_category_highlevel': 'kmeans_topic_category_highlevel'\n",
    "})\n",
    "\n",
    "df_bertopic = df_clean[['id', 'topic', 'topic_label']]\n",
    "df_bertopic = df_bertopic.rename(columns={\n",
    "    'topic': 'bertopic_topic',\n",
    "    'topic_label': 'bertopic_topic_label'\n",
    "})\n",
    "\n",
    "# Merge K-means clusters\n",
    "df_merged = pd.DataFrame(df_translated).merge(df_kmeans, on='id', how='left')\n",
    "\n",
    "# Merge BERTopic topics\n",
    "df_merged = df_merged.merge(df_bertopic, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bffc670-b936-4833-aef3-fd5ca306500e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clusters_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a27cf2-1a81-411d-99bf-94fe0ab1c15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A quick look at the data \n",
    "for _, row in df_merged.head(10).iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Text: {str(row.get('content_text_en', ''))[:500]}\")\n",
    "    print(f\"K-means Category: {row.get('kmeans_cluster_topic', '')} | BERTopic Label: {row.get('bertopic_topic_label', '')}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2d41c4-5045-4829-93e5-d637e7622e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Helper: Extract top keywords for a BERTopic topic\n",
    "# -------------------------------------------------------------------\n",
    "def get_bertopic_keywords(topic_id, topic_model, top_n=10):\n",
    "    \"\"\"\n",
    "    Return comma-separated keywords for a BERTopic topic.\n",
    "    Handles missing topics, outliers (-1), and empty categories.\n",
    "    \"\"\"\n",
    "    if topic_id is None or pd.isna(topic_id):\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        topic_id = int(topic_id)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "    # Outlier topic\n",
    "    if topic_id == -1:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        topic_words = topic_model.get_topic(topic_id)\n",
    "    except KeyError:\n",
    "        # Topic does not exist in model (can happen after caching)\n",
    "        return \"\"\n",
    "\n",
    "    if topic_words is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract words only\n",
    "    keywords = [word for word, _ in topic_words[:top_n]]\n",
    "    return \", \".join(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4895dc-08cc-4534-b786-200ddd07f707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 0) Ensure helper objects exist\n",
    "# ============================================================\n",
    "\n",
    "# topic_labels_en: mapping K-means cluster_id -> keywords (list or string)\n",
    "try:\n",
    "    topic_labels_en\n",
    "except NameError:\n",
    "    with open(TOPIC_LABELS_PATH, \"r\") as f:\n",
    "        topic_labels_en = json.load(f)\n",
    "\n",
    "# ensure int keys\n",
    "topic_labels_en = {int(k): v for k, v in topic_labels_en.items()}\n",
    "\n",
    "\n",
    "# BERTopic: pretty labels (top-5 keywords) and keyword helper\n",
    "try:\n",
    "    bertopic_pretty_labels\n",
    "except NameError:\n",
    "    bertopic_pretty_labels = {}\n",
    "    for topic_id in topic_model.get_topics().keys():\n",
    "        if topic_id == -1:\n",
    "            bertopic_prety_labels[topic_id] = \"Outliers / Miscellaneous\"\n",
    "        else:\n",
    "            words = topic_model.get_topic(topic_id)\n",
    "            if not words:\n",
    "                bertopic_pretty_labels[topic_id] = f\"Topic {topic_id}\"\n",
    "            else:\n",
    "                top_words = [w for w, _ in words[:5]]\n",
    "                bertopic_pretty_labels[topic_id] = \", \".join(top_words)\n",
    "\n",
    "\n",
    "def get_bertopic_keywords(topic_id, topic_model, top_n: int = 10) -> str:\n",
    "    \"\"\"Longer keyword list for a given BERTopic topic_id.\"\"\"\n",
    "    if topic_id == -1 or pd.isna(topic_id):\n",
    "        return \"\"\n",
    "    words = topic_model.get_topic(int(topic_id))\n",
    "    if not words:\n",
    "        return \"\"\n",
    "    return \", \".join([w for w, _ in words[:top_n]])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helper: auto-label K-means clusters with top keywords\n",
    "# ============================================================\n",
    "\n",
    "def make_kmeans_keyword_label(cluster_id, topic_labels_en, top_k: int = 5) -> str:\n",
    "    if pd.isna(cluster_id):\n",
    "        return \"\"\n",
    "    cid = int(cluster_id)\n",
    "\n",
    "    raw = topic_labels_en.get(cid, \"\")\n",
    "\n",
    "    if isinstance(raw, (list, tuple)):\n",
    "        keywords = [str(w) for w in raw][:top_k]\n",
    "    elif isinstance(raw, str):\n",
    "        parts = [p.strip() for p in raw.split(\",\") if p.strip()]\n",
    "        keywords = parts[:top_k]\n",
    "    else:\n",
    "        keywords = [str(raw)]\n",
    "\n",
    "    return \", \".join(keywords)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) K-means ‚Äì compact overview\n",
    "# ============================================================\n",
    "\n",
    "# 1) Size per cluster\n",
    "cluster_sizes = (\n",
    "    df_merged\n",
    "    .groupby(\"kmeans_cluster\")\n",
    "    .size()\n",
    "    .rename(\"num_articles\")\n",
    ")\n",
    "\n",
    "# 2) Dominant BERTopic topic per cluster\n",
    "dominant_bertopic = (\n",
    "    df_merged\n",
    "    .groupby(\"kmeans_cluster\")[\"bertopic_topic\"]\n",
    "    .agg(lambda s: s.value_counts().idxmax() if s.notna().any() else np.nan)\n",
    "    .rename(\"dominant_bertopic_topic\")\n",
    ")\n",
    "\n",
    "df_kmeans_overview = (\n",
    "    pd.concat([cluster_sizes, dominant_bertopic], axis=1)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"kmeans_cluster\": \"cluster_id\"})\n",
    ")\n",
    "\n",
    "# 3) Auto-label K-means clusters with top keywords\n",
    "df_kmeans_overview[\"kmeans_label\"] = df_kmeans_overview[\"cluster_id\"].apply(\n",
    "    lambda cid: make_kmeans_keyword_label(cid, topic_labels_en)\n",
    ")\n",
    "\n",
    "# 4) Dominant BERTopic label (top-5 keyword label)\n",
    "df_kmeans_overview[\"dominant_bertopic_label\"] = df_kmeans_overview[\"dominant_bertopic_topic\"].map(\n",
    "    bertopic_pretty_labels\n",
    ")\n",
    "\n",
    "df_kmeans_final = df_kmeans_overview[\n",
    "    [\"kmeans_label\", \"num_articles\", \"dominant_bertopic_label\"]\n",
    "].sort_values(\"num_articles\", ascending=False)\n",
    "\n",
    "print(\"=== K-means ‚Äì Final Compact Overview ===\")\n",
    "display(df_kmeans_final)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) BERTopic ‚Äì compact overview\n",
    "# ============================================================\n",
    "\n",
    "# 1) Size per BERTopic topic\n",
    "topic_sizes = (\n",
    "    df_merged\n",
    "    .groupby(\"bertopic_topic\")\n",
    "    .size()\n",
    "    .rename(\"num_articles\")\n",
    ")\n",
    "\n",
    "# 2) Dominant K-means per topic\n",
    "dominant_kmeans = (\n",
    "    df_merged\n",
    "    .groupby(\"bertopic_topic\")[\"kmeans_cluster\"]\n",
    "    .agg(lambda s: s.value_counts().idxmax() if s.notna().any() else np.nan)\n",
    "    .rename(\"dominant_kmeans_cluster\")\n",
    ")\n",
    "\n",
    "df_bertopic_overview = (\n",
    "    pd.concat([topic_sizes, dominant_kmeans], axis=1)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"bertopic_topic\": \"topic_id\"})\n",
    ")\n",
    "\n",
    "# 3) BERTopic labels + keywords\n",
    "df_bertopic_overview[\"bertopic_label\"] = df_bertopic_overview[\"topic_id\"].map(\n",
    "    bertopic_pretty_labels\n",
    ")\n",
    "\n",
    "df_bertopic_overview[\"bertopic_keywords\"] = df_bertopic_overview[\"topic_id\"].apply(\n",
    "    lambda t: get_bertopic_keywords(t, topic_model, top_n=10)\n",
    ")\n",
    "\n",
    "# 4) Dominant K-means label (keyword-based)\n",
    "df_bertopic_overview[\"dominant_kmeans_label\"] = df_bertopic_overview[\"dominant_kmeans_cluster\"].apply(\n",
    "    lambda cid: make_kmeans_keyword_label(cid, topic_labels_en)\n",
    ")\n",
    "\n",
    "df_bertopic_final = df_bertopic_overview[\n",
    "    [\"bertopic_label\", \"num_articles\", \"dominant_kmeans_label\"]\n",
    "].sort_values(\"num_articles\", ascending=False)\n",
    "\n",
    "print(\"=== BERTopic ‚Äì Final Compact Overview ===\")\n",
    "display(df_bertopic_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e1a7a7d-e166-4513-8026-499e738306a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Key Findings\n",
    "\n",
    "---\n",
    "\n",
    "**K-means Cluster Labels (10 Clusters):**  \n",
    "*K-means produces broader, more general thematic groupings based on centroid-based clustering.*\n",
    "\n",
    "- **Cluster 0:** Swiss Domestic Politics, Society & Climate Policy  \n",
    "  *Keywords: switzerland, federal, climate, council*\n",
    "- **Cluster 1:** Middle East Conflict (Gaza & Israel)  \n",
    "  *Keywords: gaza, israeli, hamas, ceasefire*\n",
    "- **Cluster 2:** US‚ÄìChina Geopolitics & International Diplomacy  \n",
    "  *Keywords: trump, states, china, meeting*\n",
    "- **Cluster 3:** Global Culture, Lifestyle & Human Interest Stories  \n",
    "  *Keywords: film, world, life, music*\n",
    "- **Cluster 4:** Natural Disasters ‚Äì Hurricanes & Caribbean Impact  \n",
    "  *Keywords: hurricane, melissa, jamaica*\n",
    "- **Cluster 5:** Russia‚ÄìUkraine Conflict & International Military Affairs  \n",
    "  *Keywords: russian, forces, city, government*\n",
    "- **Cluster 6:** Latin American Politics, Security & Government Affairs  \n",
    "  *Keywords: venezuela, police, drug, president*\n",
    "- **Cluster 7:** International Sports News & Competitions  \n",
    "  *Keywords: team, league, match*\n",
    "- **Cluster 8:** Crime, Justice & Law Enforcement  \n",
    "  *Keywords: police, court, arrested*\n",
    "- **Cluster 9:** Global Economy, Markets & Corporate Finance  \n",
    "  *Keywords: company, market, billion*\n",
    "\n",
    "---\n",
    "\n",
    "**BERTopic Labels (9 Topics + Outliers):**  \n",
    "*BERTopic yields more fine-grained, semantically nuanced topic structures.*\n",
    "\n",
    "- **Topic 0:** US‚ÄìChina Diplomacy & International Politics  \n",
    "  *Keywords: president, trump, china, government*\n",
    "- **Topic 1:** Swiss Domestic Politics, Initiatives & Fiscal Policy  \n",
    "  *Keywords: swiss, tax, federal, canton*\n",
    "- **Topic 2:** Gaza War & Middle East Humanitarian Crisis  \n",
    "  *Keywords: gaza, hamas, ceasefire*\n",
    "- **Topic 3:** AI, Film, Arts & Tech-Driven Culture  \n",
    "  *Keywords: ai, film, music, artificial*\n",
    "- **Topic 4:** Sports ‚Äì Football, Matches, Leagues  \n",
    "  *Keywords: team, league, goal*\n",
    "- **Topic 5:** Russia‚ÄìUkraine War & Military Technology  \n",
    "  *Keywords: russian, ukraine, nato, military*\n",
    "- **Topic 6:** Crime, Courts & Law Enforcement  \n",
    "  *Keywords: police, court, arrested*\n",
    "- **Topic 7:** Hurricanes & Severe Weather in the Caribbean  \n",
    "  *Keywords: hurricane, melissa, cuba*\n",
    "- **Topic 8:** US Domestic Politics ‚Äì GOP, Democrats, Shutdown  \n",
    "  *Keywords: republicans, democratic, government*\n",
    "- **Topic -1:** Mixed Political & Security News (Unassigned)  \n",
    "  *Keywords: mixed cross-domain content without stable topic cohesion*\n",
    "\n",
    "  **Mapping Table: K-means vs. BERTopic**\n",
    "\n",
    "| Harmonised Category                         | K-means Cluster Label                                   | BERTopic Topic Label                                           |\n",
    "|---------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| Swiss Domestic Politics & Society           | Swiss Domestic Politics, Society & Climate Policy        | Swiss Domestic Politics, Initiatives & Fiscal Policy           |\n",
    "| Middle East Conflict (Gaza/Israel)          | Middle East Conflict (Gaza & Israel)                     | Gaza War & Middle East Humanitarian Crisis                     |\n",
    "| US‚ÄìChina / Global Geopolitics               | US‚ÄìChina Geopolitics & International Diplomacy           | US‚ÄìChina Diplomacy & International Politics                    |\n",
    "| Global Culture & Arts / AI Culture          | Global Culture, Lifestyle & Human Interest Stories        | AI, Film, Arts & Tech-Driven Culture                           |\n",
    "| Natural Disasters ‚Äì Hurricanes              | Natural Disasters ‚Äì Hurricanes & Caribbean Impact         | Hurricanes & Severe Weather in the Caribbean                   |\n",
    "| Russia‚ÄìUkraine & Military Affairs           | Russia‚ÄìUkraine Conflict & International Military Affairs  | Russia‚ÄìUkraine War & Military Technology                       |\n",
    "| Latin American Politics & Security          | Latin American Politics, Security & Government Affairs    | Mixed Political & Security News (Unassigned)                   |\n",
    "| Sports ‚Äì Football & Competitions            | International Sports News & Competitions                  | Sports ‚Äì Football, Matches, Leagues                            |\n",
    "| Crime, Courts & Law Enforcement             | Crime, Justice & Law Enforcement                          | Crime, Courts & Law Enforcement                                |\n",
    "| Global Economy & Markets                    | Global Economy, Markets & Corporate Finance               | US Domestic Politics ‚Äì GOP, Democrats, Shutdown (partially)    |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6991eb13-c62e-41b8-bf99-12bfacdb38fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3.2 Comparing how similar the results actually are for single articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77057bcc-a39a-4665-8060-9f6ef3d5a8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: falls noch nicht gemacht, sicherstellen, dass topic_labels_en int-Keys hat\n",
    "topic_labels_en = {int(k): v for k, v in topic_labels_en.items()}\n",
    "\n",
    "def make_kmeans_keyword_label(cluster_id, topic_labels_en, top_k=5):\n",
    "    raw = topic_labels_en.get(cluster_id, \"\")\n",
    "    \n",
    "    # raw kann Liste oder String sein\n",
    "    if isinstance(raw, (list, tuple)):\n",
    "        keywords = [str(w) for w in raw][:top_k]\n",
    "    elif isinstance(raw, str):\n",
    "        parts = [p.strip() for p in raw.split(\",\") if p.strip()]\n",
    "        keywords = parts[:top_k]\n",
    "    else:\n",
    "        keywords = [str(raw)]\n",
    "        \n",
    "    return \", \".join(keywords)\n",
    "\n",
    "\n",
    "# Crosstab K-means x BERTopic\n",
    "crosstab = pd.crosstab(\n",
    "    df_merged['kmeans_cluster'],\n",
    "    df_merged['bertopic_topic']\n",
    ")\n",
    "\n",
    "# K-means labels: \"0 ‚Äì swiss, politics, climate, ...\" (Top-5 Keywords)\n",
    "kmeans_id_to_label = {\n",
    "    cid: f\"{cid} ‚Äì {make_kmeans_keyword_label(cid, topic_labels_en)}\"\n",
    "    for cid in sorted(df_merged['kmeans_cluster'].dropna().unique())\n",
    "}\n",
    "\n",
    "# BERTopic labels: \"3 ‚Äì Gaza, Israel, war, ...\" (aus bertopic_pretty_labels)\n",
    "bertopic_id_to_label = {\n",
    "    tid: f\"{tid} ‚Äì {bertopic_pretty_labels.get(tid, 'Unknown')}\"\n",
    "    for tid in sorted(df_merged['bertopic_topic'].dropna().unique())\n",
    "}\n",
    "\n",
    "# Normalisieren (Zeilenweise)\n",
    "crosstab_norm = crosstab.div(crosstab.sum(axis=1), axis=0)\n",
    "\n",
    "# Achsen mit Labels umbenennen\n",
    "crosstab_norm_labeled = crosstab_norm.rename(\n",
    "    index=kmeans_id_to_label,\n",
    "    columns=bertopic_id_to_label\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(\n",
    "    crosstab_norm_labeled,\n",
    "    cmap=\"Blues\",\n",
    "    annot=False,\n",
    "    cbar_kws={\"label\": \"Share of articles per K-means cluster\"}\n",
    ")\n",
    "plt.title(\"K-means vs BERTopic ‚Äì Relative Overlap (Row-normalized)\")\n",
    "plt.xlabel(\"BERTopic Topic (ID ‚Äì Label)\")\n",
    "plt.ylabel(\"K-means Cluster (ID ‚Äì Label)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc143d98-11df-4355-8d5e-3ddbe95e278f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The heatmap shows the relative overlap between K-means clusters and BERTopic topics. Overall, the results indicate that both models agree well on highly cohesive themes but differ more strongly on broad or multi-dimensional topics.\n",
    "\n",
    "1. Strong matches: Several K-means clusters map almost perfectly to a single BERTopic topic. This includes:\n",
    "- Middle East conflict (K-means 1 ‚Üí BERTopic 2)\n",
    "- Russia‚ÄìUkraine war (K-means 5 ‚Üí BERTopic 5)\n",
    "- Sports (K-means 7 ‚Üí BERTopic 4)\n",
    "- Crime and justice (K-means 8 ‚Üí BERTopic 6)\n",
    "- Hurricanes (K-means 4 ‚Üí BERTopic 7)\n",
    "\n",
    "These topics are very coherent, with clear vocabulary, which makes them easy for both algorithms to detect consistently.\n",
    "\n",
    "2. Partial alignment: Some clusters show moderate overlap across multiple BERTopic topics. Examples:\n",
    "- Swiss domestic politics (K-means 0), which maps mostly to BERTopic 1 but with noticeable spillover.\n",
    "- US‚ÄìChina geopolitics (K-means 2), which spans BERTopic 0 (international diplomacy) and BERTopic 8 (US domestic politics). K-means merges these, while BERTopic separates them.\n",
    "\n",
    "This indicates that these domains have internal substructure that BERTopic picks up more clearly.\n",
    "\n",
    "3. Divergence on heterogeneous categories:\n",
    "- Global culture and human-interest stories (K-means 3) are distributed across several BERTopic topics, suggesting diverse vocabulary and weak semantic cohesion.\n",
    "- Latin American politics (K-means 6) maps largely to BERTopic topic -1 (outliers), meaning BERTopic does not find a stable cluster for this region.\n",
    "- Global economy (K-means 9) maps partially to political topics in BERTopic, implying economic reporting is often embedded in political narratives rather than forming its own semantic group.\n",
    "\n",
    "4. Overall conclusion: Both models detect a similar core of strong topics (conflicts, crime, sports, disasters), but BERTopic reveals more fine-grained distinctions within political themes, while K-means produces broader, more general categories. BERTopic is more sensitive to contextual and linguistic subtleties, whereas K-means is more stable for high-level thematic groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356dff87-bfc0-4ba3-bb69-4686f409351d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3.3 A more sofisiticated approach using overlap metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aec30b0-5be2-48af-8c8e-43524715d75a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Drop outliers (if BERTopic labels -1 exist)\n",
    "df_eval = df_merged.dropna(subset=['kmeans_cluster', 'bertopic_topic'])\n",
    "\n",
    "ari = adjusted_rand_score(df_eval['kmeans_cluster'], df_eval['bertopic_topic'])\n",
    "nmi = normalized_mutual_info_score(df_eval['kmeans_cluster'], df_eval['bertopic_topic'])\n",
    "\n",
    "print(\"=== Agreement Metrics ===\\n\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f48024e5-88cc-4fe3-b147-aa1226119cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Key Findings \n",
    "The agreement between K-means and BERTopic can be quantified using two standard clustering similarity metrics:\n",
    "\n",
    "- **Adjusted Rand Index (ARI): 0.31**  \n",
    "  This value indicates *moderate agreement*. An ARI close to 0 means almost random similarity, while 1.0 represents identical clusterings.  \n",
    "  An ARI of 0.31 shows that both models partially agree on the structure of the data, but also differ substantially‚Äîconsistent with the fact that K-means produces broader clusters while BERTopic identifies more fine-grained subtopics.\n",
    "\n",
    "- **Normalized Mutual Information (NMI): 0.50**  \n",
    "  This reflects *medium information overlap* between the two methods.  \n",
    "  An NMI of 0.50 suggests that roughly half of the topical structure found by one method is shared by the other. This is typical when comparing a coarse clustering method (K-means) with a more granular one (BERTopic).\n",
    "\n",
    "**Overall:**  \n",
    "The numbers confirm that the two models detect a similar general thematic landscape, but they diverge in how they draw boundaries‚ÄîK-means merges broader themes, whereas BERTopic splits them into more specific, semantically coherent topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50adf863-252c-4d62-b232-f3b2ffda6ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Appendix\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "### Models Used\n",
    "- **Semantic Search & K-means**: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- **BERTopic**: `nvidia/llama-embed-nemotron-8b`\n",
    "- **Translation**: Google Translate API\n",
    "\n",
    "### Libraries\n",
    "- pandas, numpy: Data manipulation\n",
    "- sentence-transformers: Text embeddings\n",
    "- scikit-learn: K-means clustering\n",
    "- umap-learn: Dimensionality reduction\n",
    "- BERTopic: Advanced topic modeling\n",
    "- hdbscan: Density-based clustering\n",
    "- matplotlib, seaborn: Visualization\n",
    "- transformers: HuggingFace models\n",
    "\n",
    "### Parameters\n",
    "\n",
    "**UMAP:**\n",
    "- n_neighbors: 20\n",
    "- n_components: 5\n",
    "- min_dist: 0.0\n",
    "- metric: cosine\n",
    "\n",
    "**HDBSCAN:**\n",
    "- min_cluster_size: 220\n",
    "- min_samples: 15\n",
    "- metric: euclidean\n",
    "\n",
    "**K-means:**\n",
    "- n_clusters: 10\n",
    "- random_state: 42\n",
    "\n",
    "## Data Access\n",
    "\n",
    "**Public Sample:**\n",
    "- URL: https://github.com/Tao-Pi/CAS-Applied-Data-Science/raw/main/Module-3/01_Module%20Final%20Assignment/export_articles_v2_sample25mb.parquet\n",
    "- Size: <25MB\n",
    "- Articles: 1,000 (sampled)\n",
    "\n",
    "**Full Dataset (Confidential):**\n",
    "- Location: Databricks Delta table\n",
    "- Access: Requires SRG SSR credentials\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [RenkuLab](https://renkulab.io) - Cloud-based Jupyter environment\n",
    "- [BERTopic Documentation](https://maartengr.github.io/BERTopic/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "## Contact\n",
    "\n",
    "For questions about this analysis or data access, contact the SRG SSR data team.\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Date:** November 2025  \n",
    "**Notebook Version:** 1.0 (Comprehensive Merged)  \n",
    "**Authors:** CAS ADS Module 3 Project Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "957c5e72-c7bf-42e9-9964-1c847dc3dbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abandoned: Kmeans, without translations)\n",
    "\n",
    "**Use Case:** Discover what topics SRG writes about to improve navigation and content organization.\n",
    "\n",
    "**Approach:**\n",
    "- Use K-means clustering on article embeddings\n",
    "- Extract representative keywords for each cluster\n",
    "- Visualize topic distribution in 2D space using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ea32b9-1b2b-4295-b5a4-c118a5aa09d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import re\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 10\n",
    "print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "\n",
    "# --- suppress ugly threadpoolctl exceptions printed to stderr ---\n",
    "stderr_buffer = io.StringIO()\n",
    "with contextlib.redirect_stderr(stderr_buffer):\n",
    "    labels = kmeans.fit_predict(emb_matrix.astype(float))\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "df_clusters = srgssr_article_corpus.DataFrame({\n",
    "    \"id\": ids,\n",
    "    \"content_text_csv\": texts,\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "def get_topic_keywords(cluster_id, df_clusters, top_n=3):\n",
    "    \"\"\"Extract most common meaningful words from articles in a cluster\"\"\"\n",
    "    cluster_texts = df_clusters[df_clusters['cluster'] == cluster_id]['content_text_csv'].tolist()\n",
    "    combined_text = ' '.join(cluster_texts).lower()\n",
    "    words = re.findall(r'\\b[a-z√§√∂√º√†√©√®√™√´√Ø√¥√π√ª]{4,}\\b', combined_text)\n",
    "    \n",
    "    stopwords = {\n",
    "        'dass', 'sind', 'wird', 'wurden', 'wurde', 'haben', 'sein',\n",
    "        'eine', 'einem', 'einen', 'einer', 'dies', 'diese', 'dieser',\n",
    "        'auch', 'mehr', 'beim', '√ºber', 'nach', 'sich', 'oder', 'kann',\n",
    "        'k√∂nnen', 'm√ºssen', 'soll', 'sollen', 'noch', 'bereits', 'aber',\n",
    "        'wenn', 'weil', 'denn', 'dann', 'sowie', 'dass', 'damit', 'with',\n",
    "        'from', 'have', 'this', 'that', 'will', 'been', 'were', 'their',\n",
    "        'what', 'which', 'when', 'where', 'there', 'pour', 'dans', 'avec',\n",
    "        'sont', '√™tre', 'cette', 'mais', 'plus', 'comme', 'fait'\n",
    "    }\n",
    "    \n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    word_counts = Counter(words)\n",
    "    top_words = [word for word, count in word_counts.most_common(top_n)]\n",
    "    return ', '.join(top_words) if top_words else f\"Topic {cluster_id}\"\n",
    "\n",
    "# Generate topic labels\n",
    "topic_labels = {}\n",
    "for cluster_id in range(n_clusters):\n",
    "    topic_labels[cluster_id] = get_topic_keywords(cluster_id, df_clusters, top_n=3)\n",
    "\n",
    "df_clusters['cluster_topic'] = df_clusters['cluster'].map(topic_labels)\n",
    "\n",
    "print(\"Finished clustering & topic extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb09dcd-c431-4a35-a44c-60cbae705cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nCluster Topics (K-means):\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id in range(n_clusters):\n",
    "    count = len(df_clusters[df_clusters['cluster'] == cluster_id])\n",
    "    print(f\"Cluster {cluster_id}: {topic_labels[cluster_id]} ({count} articles)\")\n",
    "\n",
    "print(\"\\nSome examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "examples = df_clusters.groupby(\"cluster\").head(1)  # 1 example per cluster\n",
    "\n",
    "for _, row in examples.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Cluster: {row['cluster']} | Topic: {row['cluster_topic']}\")\n",
    "    print(f\"URL: https://www.swissinfo.ch/s/{row['id'].split(':')[-1]}\")\n",
    "    print(f\"Text: {row['content_text_csv'][:400]}...\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747653f9-ff75-4b0d-b1fc-8a1d45bbf546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize K-means Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508fdf4f-230f-4329-9769-10e425881d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Creating UMAP projection for visualization...\")\n",
    "reducer = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "embedding_2d = reducer.fit_transform(emb_matrix)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "scatter = plt.scatter(\n",
    "    embedding_2d[:, 0], \n",
    "    embedding_2d[:, 1], \n",
    "    c=labels, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6, \n",
    "    s=50\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('K-means Topic Clusters Visualization (UMAP Projection)', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1', fontsize=12)\n",
    "plt.ylabel('UMAP Dimension 2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add cluster centers with labels\n",
    "kmeans_centers_2d = reducer.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(\n",
    "    kmeans_centers_2d[:, 0], \n",
    "    kmeans_centers_2d[:, 1], \n",
    "    c='red', \n",
    "    marker='X', \n",
    "    s=200, \n",
    "    edgecolors='black', \n",
    "    linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "for cluster_id in range(n_clusters):\n",
    "    x, y = kmeans_centers_2d[cluster_id]\n",
    "    label_text = f\"C{cluster_id}: {topic_labels[cluster_id]}\"\n",
    "    plt.annotate(\n",
    "        label_text,\n",
    "        xy=(x, y),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black', lw=1)\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"\\nK-means Cluster Distribution:\")\n",
    "cluster_counts = df_clusters['cluster'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster_id} ({topic_labels[cluster_id]}): {count} articles\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) CAS_Module3_Presentation_2024-12-04_David-Petra-Viktor",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "crashpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
