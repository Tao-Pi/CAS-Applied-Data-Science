{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeaf82aa-e92c-4fa0-804c-bde56025175e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "n_5oRe0SXilM"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neworldemancer/DSF5/blob/master/Course_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7dbc591-3d6b-49c1-82a6-a07a0e8cbd22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to machine learning & Data Analysis\n",
    "\n",
    "Basic introduction on how to perform typical machine learning tasks with Python.\n",
    "\n",
    "Prepared by Mykhailo Vladymyrov & Aris Marcolongo,\n",
    "Data Science Lab, University Of Bern, 2023\n",
    "\n",
    "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b5d7b5-fb5e-430d-a47e-9c9675c1fb99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NVSRftm8X1m1"
   },
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1cf679-f246-4dcb-bf1a-4081b0c9b807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run code below, just once \n",
    "# %pip install --upgrade \"numpy<2.0.0\"\n",
    "# dbutils.library.restartPython()  # restart interpreter\n",
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7338ad1c-2f01-4e62-b1ca-8f71812b4b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hVJn0ilgOS8F",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free\n",
    "# software machine learning library for the Python programming language.\n",
    "# It features various classification, regression and clustering algorithms,\n",
    "# and is designed to interoperate with the Python numerical and scientific\n",
    "# libraries NumPy and SciPy. (from wiki)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# common visualization module\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# numeric library\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time as timer\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8213c0-9871-4f1d-8aa9-8f2d1757cd2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8Y7aMevU3Ug8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import os\n",
    "from packaging import version\n",
    "import shutil\n",
    "import sys \n",
    "\n",
    "def download_and_extract_data(\n",
    "    url=\"https://github.com/neworldemancer/DSF5/raw/master/colab_material.tgz\",\n",
    "    target_dir=\"data\",\n",
    "    fname=\"colab_material.tgz\",\n",
    "    update_folder=False\n",
    "):\n",
    "    \"\"\"Download and extract a tar.gz dataset into target_dir.\"\"\"\n",
    "    \n",
    "    if update_folder and os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        cache_dir = os.path.abspath(\".\")\n",
    "\n",
    "        if version.parse(tf.__version__) >= version.parse(\"2.13.0\"):\n",
    "            # new behavior: fname must be only a filename\n",
    "            path = tf.keras.utils.get_file(\n",
    "                fname=fname,\n",
    "                origin=url,\n",
    "                cache_dir=cache_dir\n",
    "            )\n",
    "        else:\n",
    "            # old behavior: can pass full path\n",
    "            path = tf.keras.utils.get_file(\n",
    "                fname=os.path.join(cache_dir, fname),\n",
    "                origin=url\n",
    "            )\n",
    "        # extract tar into target_dir\n",
    "        with tarfile.open(path, \"r:gz\") as tar:\n",
    "            tar.extractall(target_dir)\n",
    "    else:\n",
    "        print('Data already present. Use update_folder = True to overwrite/update if desired.')\n",
    "    return os.path.abspath(target_dir)\n",
    "\n",
    "data_path = download_and_extract_data(update_folder=False)\n",
    "sys.path.append(data_path)\n",
    "print(\"Data available at:\", data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b124c7-49fc-427d-9355-2700fd8ec413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.routines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47662adf-a27a-4c1d-8176-ed156fbeedf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "pclZR6uFklf_"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f37454-e889-4db5-86aa-1d2b381ad570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "s_wxOrdWko8W"
   },
   "source": [
    "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f367b44-5fa7-496b-93c1-ac0659762757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8UQgU5I-lEll"
   },
   "source": [
    "## 1. Synthetic linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e848b969-a868-4798-8e58-e03ffea9276a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jGfWOWRjlWPa"
   },
   "outputs": [],
   "source": [
    "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
    "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
    "\n",
    "  w = w or np.random.uniform(0.1, 10, n_d)\n",
    "  b = b or np.random.uniform(-10, 10)\n",
    "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
    "\n",
    "  print('true slopes: w =', w, ';  b =', b)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfe8b41-be9a-4fe3-8861-6304f15f2755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "5RLYxGy_nBZG",
    "outputId": "4836570c-c789-4c16-bda7-37aa222f66fc"
   },
   "outputs": [],
   "source": [
    "x, y = get_linear(n_d=1, sigma=1)\n",
    "plt.plot(x[:, 0], y, '*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7278456-5cd6-4ae1-8e2a-5b7b5dfb1f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "10ODDOp4nX4S",
    "outputId": "5a70b4e8-5e8b-4cd6-e890-c5eb1d5261ca"
   },
   "outputs": [],
   "source": [
    "n_d = 2\n",
    "x, y = get_linear(n_d=n_d, n_points=100)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=10)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f33d31-1470-401c-a05b-df4a6d92c053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FJ5rjq7fIe8Q"
   },
   "source": [
    "## 2. House prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2481ec-1ccf-46f6-89eb-442f2c311f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "A-45usskInlD"
   },
   "source": [
    "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf90330-6828-4745-b44e-601e0aef2568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dVv2ID96IyN0"
   },
   "outputs": [],
   "source": [
    "def house_prices_dataset(return_df=False, return_df_xy=False, price_max=400000, area_max=40000):\n",
    "  path = 'data/AmesHousing.csv'\n",
    "\n",
    "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False,  )\n",
    "\n",
    "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
    "  df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "  useful_fields = ['LotArea',\n",
    "                  'Utilities', 'OverallQual', 'OverallCond',\n",
    "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
    "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
    "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
    "                  'FullBath', 'HalfBath',\n",
    "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
    "                  'Functional','PoolArea',\n",
    "                  'YrSold', 'MoSold'\n",
    "                  ]\n",
    "  target_field = 'SalePrice'\n",
    "\n",
    "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
    "\n",
    "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
    "                  'LotFrontage': {'NA':0},\n",
    "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
    "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
    "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
    "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
    "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
    "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'CentralAir':  {'N':0, 'Y': 1},\n",
    "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
    "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
    "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
    "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
    "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
    "                  }\n",
    "\n",
    "  df_X = df[useful_fields].copy()\n",
    "  df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical\n",
    "  df_Y = df[target_field].copy()\n",
    "\n",
    "  x = df_X.to_numpy().astype(np.float32)\n",
    "  y = df_Y.to_numpy().astype(np.float32)\n",
    "\n",
    "  if price_max>0:\n",
    "    idxs = y<price_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  if area_max>0:\n",
    "    idxs = x[:,0]<area_max\n",
    "    x = x[idxs]\n",
    "    y = y[idxs]\n",
    "\n",
    "  return (x, y, df) if return_df else ((x, y, (df_X, df_Y)) if return_df_xy else (x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8453cda-4f5f-42fa-be8a-5e5824e36e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "YqWU0eHts1RM",
    "outputId": "b61e8e82-7d5a-4191-b70a-80fed57c8d2e"
   },
   "outputs": [],
   "source": [
    "x, y, df = house_prices_dataset(return_df=True)\n",
    "print(x.shape, y.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61bbe95-4490-44c1-8d02-622cf5e511e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "YDtzVS-1Mxxe",
    "outputId": "77dbfa01-62a5-49fa-fcb8-cc2a147d96d4"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9765fe48-e5e6-4e24-ab86-bd14252bf254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "91nj7znzMEpA",
    "outputId": "99410485-f9f2-4e59-b9ef-0f07cfb5b1e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(x[:, 0], y, '.')\n",
    "plt.xlabel('area, sq.ft')\n",
    "plt.ylabel('price, $');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8090d182-46e2-417d-b963-d4ac95886430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "q7CNxkPdNB4L"
   },
   "source": [
    "## 3. Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb6d83e-6d30-48f1-9f10-d16c5d002521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "j8wXhleONKgZ",
    "outputId": "7e0e974c-17e2-4cf9-bdc8-7a69f0537c5a"
   },
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
    "colors = \"ygr\"\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y == i\n",
    "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2641b618-fb33-49f9-be72-df3fb1ea915b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "NKcmdcZf0VO8",
    "outputId": "55f512ea-94f5-4a33-c295-d05b83dbd770"
   },
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
    "\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]  # affine transformation matrix\n",
    "x = np.dot(x, transformation)               # applied to point coordinated to make blobs less separable\n",
    "\n",
    "colors = \"ygr\"\n",
    "for i, color in enumerate(colors):\n",
    "    idx = y == i\n",
    "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4a13bf-4039-41de-95e8-f7e031b694fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8S1jwU4cXQX4"
   },
   "source": [
    "## 4. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a74095fb-ec3e-4b7a-958f-a177e7c0353d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "e2u82UQ5XQX4"
   },
   "source": [
    "The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting (taken from http://yann.lecun.com/exdb/mnist/). Each example is a 28x28 grayscale image and the dataset can be readily downloaded from Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc704d55-b245-45ea-8d45-22f0c5ada5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JaNaGGOkXQX5"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b691b6-d2d0-4869-aedb-7d3f98b002fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dlUY5gl8XQX7"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49fdd07-b0b7-489e-9507-80f8b4ed526d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "qtYtGEDdXQX8",
    "outputId": "7e7c988f-8e55-480b-dcee-99860e00a947"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32a1eb8-676b-412c-8296-ee4045a57f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ITfbaOgfYNsq"
   },
   "source": [
    "## 5. Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b8f0d5-9dcc-440f-ae7d-186580fbba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jgzzOS7YYTru"
   },
   "source": [
    "`Fashion-MNIST` is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19e1d2f-11aa-4396-9348-658605291eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RcV2gzmuYljJ"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb2d442-74c7-4ea4-be97-d8b8db56595d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SPw6-GoPbT6U"
   },
   "source": [
    "Let's check few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29762f4-8118-42c7-8c68-822d1859f517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "tHFd0sFHY4Li",
    "outputId": "19fc320d-bf19-4f5b-b36d-dd6a9f74ebe6"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
    "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
    "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
    "  im = train_images[im_idx]\n",
    "  im_class = train_labels[im_idx]\n",
    "  axi.imshow(im, cmap='gray')\n",
    "  axi.text(1, 4, f'{im_class}', color='r', size=16)\n",
    "  axi.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243dc2e6-f0fe-4473-9e9a-a5e934cc414c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "b2LkoWfZEi4g"
   },
   "outputs": [],
   "source": [
    "fmnist_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c98a397-4d49-45e0-a183-73c2d80e8f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iHEA0tCLagoV"
   },
   "source": [
    "Each of the training and test examples is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8087d2a9-2c78-4d48-8d7b-f548ee6a45af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7-CGSS2OZKHD"
   },
   "source": [
    "# 2. Trees & Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aed4ec2-9e09-4328-bfe7-d14e27b2a774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Bxtv48o-F1Ku"
   },
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03a3fb1-35e5-4005-8bbf-a1fdb028bfdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "l582Sr0_WGXj"
   },
   "source": [
    "Decision Trees are a supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning **simple** decision rules inferred from the data features.\n",
    "\n",
    "They are fast to train, easily interpretable, capture non-linear dependencies, and require small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc44ef4f-2835-4eb5-8109-12332064bc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oF3CCNq26EN2"
   },
   "source": [
    "We will see that each tree creates a partition of the feature space $X$ into subregions $R_i,i=1..N_r$, this partition being described by a tree structure. Predictions will be made with the following procedure. Given a new test point $x$:\n",
    "\n",
    "1. Assign $x$ to the region it belongs, e.g. $R_k$\n",
    "2. For classification, make a majiority vote using the training points belonging to $R_k$. For regression, evaluate the mean of the target over all training points belonging to $R_k$.\n",
    "\n",
    "Let's see how the trees generate a partition of the domain into distinct regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09323f40-0aa2-41cc-bd38-4c296ef65eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fz5raIG_WQfg",
    "outputId": "ec2f5c8e-2210-4353-d6d7-226277054c4a"
   },
   "outputs": [],
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "X = np.dot(X, transformation)\n",
    "dtcs = []\n",
    "for depth in (1, 2, 3, 4):\n",
    "    # do fit\n",
    "    dtc = tree.DecisionTreeClassifier(max_depth=depth, criterion='gini', min_samples_leaf=3)  # 'entropy'\n",
    "    dtcs.append(dtc)\n",
    "    dtc.fit(X, y)\n",
    "\n",
    "    # print the training scores\n",
    "    print(\"training score : %.3f (depth=%d)\" % (dtc.score(X, y), depth))\n",
    "\n",
    "    # get range for visualization\n",
    "    x_0 = X[:, 0]\n",
    "    x_1 = X[:, 1]\n",
    "    x_min = x_0.min() - 1\n",
    "    x_max = x_0.max() + 1\n",
    "    y_min = x_1.min() - 1\n",
    "    y_max = x_1.max() + 1\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2,  figsize=(14,7), dpi=300)\n",
    "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=dtc, ax=ax[0])\n",
    "\n",
    "    ax[0].set_title(\"Decision surface of DTC (%d)\" % depth)\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = \"rbg\"\n",
    "    for i, color in zip(dtc.classes_, colors):\n",
    "        idx = np.where(y == i)\n",
    "        ax[0].scatter(x_0[idx], x_1[idx], c=color,\n",
    "                    edgecolor='black', s=20, linewidth=0.2)\n",
    "\n",
    "    with plt.style.context('classic'):\n",
    "      tree.plot_tree(dtc, ax=ax[1]);\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d91b28-ff0d-440f-a2a8-a2f496037085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0IgYhWghyUhL"
   },
   "source": [
    "## How is the tree constructed from a labelled training set?\n",
    "\n",
    "We distinguish two cases: **classification** and **regression**.\n",
    "\n",
    "---\n",
    "### Classification\n",
    "Given a sample $S$ of $N$ points in feature space ($x_i$, $i=1...N$), each assigned to one of $C$ classes ($c_i \\in \\{1,..,C\\}$), we compute class proportions:\n",
    "$$p_c = \\frac{N_c}{N}$$\n",
    "where $N_c$ is the number of points in class $c$.\n",
    "\n",
    "**Impurity measures:**\n",
    "- **Gini index:**\n",
    "  $$G(S) = 1 - \\sum_c p_c^2$$\n",
    "- **Entropy:**\n",
    "  $$E(S) = -\\sum_c p_c \\log(p_c)$$\n",
    "\n",
    "These quantify how mixed the classes are in a node.\n",
    "\n",
    "**Splitting:**\n",
    "- At each step, choose a feature and cutoff $a$.\n",
    "- Points with $x_{i,k} < a$ go left; $x_{i,k} \\ge a$ go right.\n",
    "- Define $S_{<}$, $S_{>}$ and their sizes $N_{<}$, $N_{>}$. \n",
    "- **Information gain:**\n",
    "  $$ IG = E(S) - \\frac{N_{<}}{N} E(S_{<}) - \\frac{N_{>}}{N} E(S_{>}) $$\n",
    "- (Same formula for Gini index.)\n",
    "- The split with highest information gain is chosen, favoring relevant features at the top and helping reduce overfitting by pruning.\n",
    "\n",
    "---\n",
    "### Regression\n",
    "Similar setup, but targets are real values $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "**Splitting criterion:**\n",
    "- Use sample variance:\n",
    "  $$V(S) = \\frac{1}{N} \\sum_i (y_i - \\bar{y})^2$$\n",
    "- After splitting, compute means $y_{<}$ and $y_{>}$ for left/right nodes.\n",
    "- **Reduction in variance:**\n",
    "  $$ \\frac{N_{<}}{N} V(S_{<}) + \\frac{N_{>}}{N} V(S_{>}) = \\frac{1}{N}\\left(\\sum_{i \\in S_{<}} (y_i-y_{<})^2 + \\sum_{i \\in S_{>}} (y_i-y_{>})^2\\right) $$\n",
    "- The best split maximizes reduction in variance.\n",
    "\n",
    "---\n",
    "These are known as the **CART** (Classification and Regression Trees) splitting criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae30a75-91c4-42c0-9ae9-e994913e77c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EHZ-hHGuY5aG"
   },
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c95540-4c6a-4f07-afb4-b009e9332344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zETTKyFmwTae"
   },
   "source": [
    "The `sklearn.ensemble` provides several ensemble algorithms. RandomForest is an averaging algorithm based on randomized decision trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction.\n",
    "\n",
    "The prediction of the ensemble is given as the averaged prediction of the individual classifiers (regression) or by majority voting (classification). E.g. for regression:\n",
    "\n",
    "$$ RF(x) = \\frac{1}{N_\\text{trees}}\\sum_{i=1}^{N_\\text{trees}} Tree_i(x)$$\n",
    "\n",
    "Individual decision trees typically exhibit high variance and tend to overfit.\n",
    "In random forests:\n",
    "* each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
    "* when splitting each node during the construction of a tree, the best split is found from a random subset of features, according to `max_features` parameter.\n",
    "\n",
    "The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant, hence yielding an overall better model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c0aed1-f385-4bd6-a7b4-d8751c51a4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s4hfPUqSZCbH",
    "outputId": "213cf885-33d1-447d-db17-91e5f3bb3ec0"
   },
   "outputs": [],
   "source": [
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "for n_est in (1, 4, 50):\n",
    "    # do fit\n",
    "    rfc = ensemble.RandomForestClassifier(max_depth=4, n_estimators=n_est,)\n",
    "    rfc.fit(X, y)\n",
    "\n",
    "    # print the training scores\n",
    "    print(\"training score : %.3f (n_est=%d)\" % (rfc.score(X, y), n_est))\n",
    "\n",
    "    # get range for visualization\n",
    "    x_0 = X[:, 0]\n",
    "    x_1 = X[:, 1]\n",
    "    x_min = x_0.min() - 1\n",
    "    x_max = x_0.max() + 1\n",
    "    y_min = x_1.min() - 1\n",
    "    y_max = x_1.max() + 1\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=rfc)\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = 'rbg'\n",
    "    for i, color in enumerate(colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
    "                    edgecolor='black', s=20, linewidth=0.2)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e6f53b-5204-4d87-b77c-70d3a8d3c70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yY6EVEestysr",
    "outputId": "a3b3386e-696c-4e88-967c-8df690db3994"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=300)\n",
    "with plt.style.context('classic'):\n",
    "  tree.plot_tree(rfc.estimators_[20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecac2edf-2e1f-4c37-9e49-7395eaaee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "puQNgKN0wS7H"
   },
   "source": [
    "## 3. Boosted Decision Trees (XGboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214fc610-d283-4801-8c6c-1d321ce6b8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TkL6_R05wez3"
   },
   "source": [
    "Another approach to the ensemble tree modeling is Boosted Decision Trees. In a boosting framework, the trees are created sequentially. This way each next tree reduces error of the ensemble, by adding corrections to previous predictions.\n",
    "\n",
    "Corrections to the predictions of boosted decision trees are sequentially added, in contrast with random forest predictions:\n",
    "\n",
    "$$ Boost(x) = Tree_0(x)+\\lambda \\sum_{i=1}^{N_\\text{trees}} Tree_i(x)$$\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The trees used at each step are very shallow trees (with high biased). Boosting is a way to combine `weak` classifiers (high bias, low variance) and decrease bias and increase variance gradually, in a controlled way.\n",
    "\n",
    "- $\\lambda$ is a parameters, called shrinkage or learning rate, that describes how fast we are changing the predictions at each step. A smaller learning rate is useful to control overfitting.\n",
    "\n",
    "- Predictions and classifications are treated often on the same footing, by making the ensemble output logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e7d7ef-6c4a-4a98-b11e-9d9d884f2670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aJpef_8WmkSz"
   },
   "source": [
    "One of the most popular implementations of boosting is XGBoost:\n",
    "\n",
    "https://arxiv.org/abs/1603.02754\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77609da0-c3a0-4fdf-9df4-c3e172d8bde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "id": "mfP-oSCWmkS0",
    "outputId": "5cd9c65c-a7ed-425c-eefe-ab3848da0760"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "params = {\n",
    "    'num_class': 3,               # Number of classes in the target variable\n",
    "    'max_depth': 3,               # Maximum depth of trees\n",
    "    'learning_rate': 0.1,         # Learning rate\n",
    "    'n_estimators': 100\n",
    "}\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(X, y, verbose=True)\n",
    "\n",
    "# print the training scores\n",
    "#print(\"training score : %.3f (n_est=%d)\" % (dtc.score(X, y), n_est))\n",
    "x_0 = X[:, 0]\n",
    "x_1 = X[:, 1]\n",
    "x_min = x_0.min() - 1\n",
    "x_max = x_0.max() + 1\n",
    "y_min = x_1.min() - 1\n",
    "y_max = x_1.max() + 1\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_prediction_2d(x_min, x_max, y_min, y_max, classifier = model)\n",
    "\n",
    "plt.title(f'Decision surface of Boosted model after 100 iterations')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Plot also the training points\n",
    "colors = 'rbg'\n",
    "for i, color in enumerate(colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(x_0[idx], x_1[idx], c=color,\n",
    "                edgecolor='black', s=20, linewidth=0.2)\n",
    "\n",
    "# Plot the three one-against-all classifiers\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f0f5b4-cd95-4749-b500-63d26990fb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = house_prices_dataset()\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "evals_result = {}\n",
    "# Create an XGBoost regression model\n",
    "model = XGBRegressor(eval_metric='rmse' ,\n",
    "                     max_depth = 5, \n",
    "                     early_stopping_rounds = 10, \n",
    "                     learning_rate = 0.3, \n",
    "                     n_estimators = 1000 )\n",
    "\n",
    "# Train the XGBoost regression model\n",
    "model.fit(x_train, y_train, \n",
    "          eval_set=[(x_train, y_train),(x_test, y_test)], \n",
    "          verbose=True)\n",
    "\n",
    "y_p_train = model.predict(x_train)\n",
    "y_p_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f2ee80-fb7e-4fe1-bc7b-1993b1b84394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(model.evals_result_['validation_0']['rmse'], label='train')\n",
    "plt.plot(model.evals_result_['validation_1']['rmse'], label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcde522-3165-4ecf-919a-e0a0bc183b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# mse\n",
    "print('train mse =', np.std(y_train - y_p_train))\n",
    "print('test mse =', np.std(y_test - y_p_test))\n",
    "\n",
    "residuals=y_train - y_p_train\n",
    "print('train mse =', np.sqrt( np.mean(residuals**2)) )\n",
    "residuals=y_test - y_p_test\n",
    "print('train mse =', np.sqrt( np.mean(residuals**2)) )\n",
    "\n",
    "# mse\n",
    "print('train mae =', np.mean(np.abs(y_train - y_p_train)))\n",
    "print('test mae =', np.mean(np.abs(y_test - y_p_test)))\n",
    "# R2\n",
    "print('train R2 =', r2_score(y_train, y_p_train))\n",
    "print('test R2 =', r2_score(y_test, y_p_test))\n",
    "\n",
    "\n",
    "# 4. plot y vs predicted y for test and train parts\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(y_train, y_p_train, 'b.', label='train')\n",
    "plt.plot(y_test, y_p_test, 'r.', label='test')\n",
    "\n",
    "plt.plot(y_train, y_train,'-')\n",
    "\n",
    "plt.plot([0], [0], 'w.')  # dummy to have origin\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7045d0-ec0a-4a52-bdf2-5a3b5edb2a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NA3ywyy_mkS0"
   },
   "source": [
    "## EXERCISE 3 : Random forest for FMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942aafdc-28a0-434f-909a-54b720efc7e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "96H8PmVHmkS0"
   },
   "source": [
    "Classify fashion MNIST images with Random Forest classifier. This is similar to Ex 2 (Logistic Regression for FMNIST) but you have to play with a different classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6691ca-bb6b-4a2a-8097-608982fa464b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584ea5b2-33d6-41ac-87d5-97dcd77b4658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qKIwUbcomkS0"
   },
   "outputs": [],
   "source": [
    "# Repeat the steps of Ex. 2, but in step 3 when fitting the model, you can play with r.f. parameters.\n",
    "\n",
    "# TIP:  You can use verbose = 2 to see the training progress (in case it is too slow) and n_jobs=-1 to use all available corse: ensemble.RandomForestClassifier(..., verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d5ed04-13d9-43fe-96a7-f0e424eb5e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once you are done, call clf.get_params() and update the google doc here with your findindgs!\n",
    "\n",
    "https://docs.google.com/document/d/1f5sDOS_cq1_ZaUkwlSnog5I8jpYiK_QAF1hk3vcBFL8/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31167734-121a-4cfe-b46b-311f09faddd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Interpretable Machine Learning: Feature Importance\n",
    "\n",
    "Understanding how a model makes decisions is a key aspect of **Interpretable Machine Learning**. This is crucial for:\n",
    "\n",
    "* debugging\n",
    "* model development \n",
    "* building trust in predictions\n",
    "* generalizability\n",
    "\n",
    "Many methods produce a feature importance plot. The simplest approach is **Permutation Feature Importance** ([scikit-learn documentation](https://scikit-learn.org/stable/modules/permutation_importance.html)):\n",
    "\n",
    "---\n",
    "\n",
    "### Permutation Feature Importance: Methodology\n",
    "\n",
    "1. **Train Model**\n",
    "    - Fit the model on training data and compute the `baseline_score`.\n",
    "2. **For each feature `f`:**\n",
    "    - Shuffle `f` in the validation data.\n",
    "    - Compute the `shuffled_score` using the trained model.\n",
    "    - Calculate `importances[f] = baseline_score - shuffled_score`.\n",
    "3. **(Optional) Normalization**\n",
    "    - Normalize the `importances` vector so all values are positive and sum to one.\n",
    "\n",
    "---\n",
    "\n",
    "For **Random Forest** models, another type of feature importance is available as a model attribute. This method is much faster to plot than permutation feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "928bb61a-a5bc-4e72-85cd-867dacfe7897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Importance in Practice\n",
    "\n",
    "Below is a code snippet demonstrating how to compute and compare permutation feature importance and Random Forest feature importance for Exercise 3 (see Solutions for full code):\n",
    "\n",
    "```python\n",
    "# Permutation Feature Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "p_importances = permutation_importance(\n",
    "    clf, test_features, test_labels, n_repeats=10, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_importances = clf.feature_importances_\n",
    "```\n",
    "\n",
    "- `p_importances`: Measures the decrease in model score when a feature is randomly shuffled, indicating its impact on predictions.\n",
    "- `rf_importances`: Directly provided by the Random Forest model, reflecting the average decrease in impurity across all trees for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999ee066-d292-4e02-83fa-c1ae3cab91c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/FMNIST_feature_importance.png\" width=\"80%\"/>\n",
    "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/FMNIST_feature_importance_2.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a993bf54-25d0-46c3-ba5d-46d77b4b48fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "LIMITATIONS:\n",
    "\n",
    "1.  The samples generated may be out-of-distribution, therefore uninteresting or belonging to a region where the model returns random values\n",
    "2.  In the presence of correlated features a model can rely on one or the other feature for prediction. This may lead to misleading interpetations.\n",
    "\n",
    "PARTIAL SOLUTION: perform a study of the correlation between the variables to interpret the validation plot.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1576a8a7-65ee-4922-a4c7-0bae8e8ccd64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FURTHER READINGS (advanced):\n",
    "\n",
    "1. \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier, https://arxiv.org/abs/1602.04938\n",
    "2. \"Explaining individual predictions when features are dependent: More accurate approximations to Shapley values\", https://arxiv.org/abs/1903.10464\n",
    "3. https://shap.readthedocs.io/en/latest/overviews.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16348fa-4fe6-4415-adf9-39aadf910b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0dG6U6s3T95t"
   },
   "source": [
    "## EXERCISE 4: Random forest for AMES with Feature Imporance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c55c0c-aaea-492a-8c8e-37fe5ec913d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DlypA1MaT95u"
   },
   "outputs": [],
   "source": [
    "X, y, (df_x, df_y) = house_prices_dataset(return_df_xy=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f5975c-8d61-4a12-be97-8f232586d31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FYTwkoSpT95u"
   },
   "source": [
    "Predict the house prices using either a RandomForest model or boosted tree (choose one). Suggestion:\n",
    "\n",
    "```python\n",
    "model = ensemble.RandomForestRegressor(max_depth=... n_estimators= ...)\n",
    "```\n",
    "\n",
    "Use permutation importance for the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1103d3fa-a550-4a86-a66b-07036bf4d84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "V4wobzB_T95u"
   },
   "outputs": [],
   "source": [
    "# 1. Review the examples showcasing the usage of the random forest model or xgboost.\n",
    "\n",
    "# 2. Fit the model\n",
    "\n",
    "# 3. Inspect training and test accuracy\n",
    "\n",
    "# 4. Try to improve performance by adjusting hyperparameters.\n",
    "# How does it compare to linear model? Can you make a plot of y_pred vs y_exact as you did for a linear model and compare visually ?\n",
    "\n",
    "# 5. Study the feature importance\n",
    " \n",
    "\n",
    "# Helper code for plotting the permutation feature importances in this dataset:\n",
    "def plot_importances(feature_names, permutation_importances):\n",
    "        \"\"\"\n",
    "        Plots horizontal bar chart of permutation feature importances.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_names : list or array-like\n",
    "                Names of the features corresponding to the importances.\n",
    "        permutation_importances : \n",
    "                Output from sklearn.inspection.permutation_importance, containing\n",
    "                importances_mean and importances_std for each feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "                Displays the plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 10))\n",
    "        plt.barh(feature_names,\n",
    "                        permutation_importances.importances_mean,\n",
    "                        yerr=permutation_importances.importances_std, )\n",
    "        plt.ylabel('feature')\n",
    "        plt.xlabel('importance')\n",
    "        plt.xlim(0, 1)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70961136-9ef7-410e-8a1b-83c40377bd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Prepare data\n",
    "X, y, (df_x, df_y) = house_prices_dataset(return_df_xy=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 2. Fit the Random Forest model\n",
    "models = {}\n",
    "for i in range(5, 15):\n",
    "    rf = RandomForestRegressor(max_depth=i, n_estimators=100, random_state=42, n_jobs=-1, verbose=0)\n",
    "    rf.fit(x_train, y_train)\n",
    "    models[i] = rf\n",
    "\n",
    "# 3. Inspect training and test accuracy for each model\n",
    "best_rmse = float('inf')\n",
    "best_model = None\n",
    "best_depth = None\n",
    "for i, rf in models.items():\n",
    "    y_pred_train_rf = rf.predict(x_train)\n",
    "    y_pred_test_rf = rf.predict(x_test)\n",
    "    rmse_test = mean_squared_error(y_test, y_pred_test_rf, squared=False)\n",
    "    print(f\"Random Forest (max_depth={i}) train RMSE:\", mean_squared_error(y_train, y_pred_train_rf, squared=False))\n",
    "    print(f\"Random Forest (max_depth={i}) test RMSE:\", rmse_test)\n",
    "    print(f\"Random Forest (max_depth={i}) train R2:\", r2_score(y_train, y_pred_train_rf))\n",
    "    print(f\"Random Forest (max_depth={i}) test R2:\", r2_score(y_test, y_pred_test_rf))\n",
    "    print('-' * 50)\n",
    "    if rmse_test < best_rmse:\n",
    "        best_rmse = rmse_test\n",
    "        best_model = rf\n",
    "        best_depth = i\n",
    "\n",
    "print(f\"Best model: max_depth={best_depth}, test RMSE={best_rmse}\")\n",
    "\n",
    "# 4. Linear model for comparison\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred_train_lr = lr.predict(x_train)\n",
    "y_pred_test_lr = lr.predict(x_test)\n",
    "print(\"Linear Regression train RMSE:\", mean_squared_error(y_train, y_pred_train_lr, squared=False))\n",
    "print(\"Linear Regression test RMSE:\", mean_squared_error(y_test, y_pred_test_lr, squared=False))\n",
    "print(\"Linear Regression train R2:\", r2_score(y_train, y_pred_train_lr))\n",
    "print(\"Linear Regression test R2:\", r2_score(y_test, y_pred_test_lr))\n",
    "\n",
    "# Plot y_pred vs y_exact for both models\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "y_pred_test_best_rf = best_model.predict(x_test)\n",
    "plt.scatter(y_test, y_pred_test_best_rf, alpha=0.5, label='Random Forest')\n",
    "plt.plot(y_test, y_test, 'k--')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Random Forest')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_test, y_pred_test_lr, alpha=0.5, label='Linear Regression', color='orange')\n",
    "plt.plot(y_test, y_test, 'k--')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Linear Regression')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Study the feature importance (permutation importance)\n",
    "perm_importances = permutation_importance(best_model, x_test, y_test, n_repeats=10, n_jobs=-1, random_state=42)\n",
    "plot_importances(df_x.columns, perm_importances)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Course_1B",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "n_5oRe0SXilM",
    "9SxiIczg1s1k",
    "ll5e8N9SVwVa",
    "5PbjhPxLmsI4",
    "ROFPolZpm21t",
    "qBXGs0xRERuv",
    "AD6zwuTHiYKA",
    "NVSRftm8X1m1",
    "pclZR6uFklf_",
    "8UQgU5I-lEll",
    "FJ5rjq7fIe8Q",
    "q7CNxkPdNB4L",
    "8S1jwU4cXQX4",
    "ITfbaOgfYNsq",
    "x2NWxK0BFwyw",
    "RHRXds9U9134",
    "K4qgOdz7Yyeb",
    "Vlf6_berQ1vq",
    "zI6s2Amob48j",
    "zZX9MQlORLfY",
    "AQ69XKdbZcA3",
    "7-CGSS2OZKHD",
    "Bxtv48o-F1Ku",
    "EHZ-hHGuY5aG",
    "puQNgKN0wS7H",
    "vhhycm2S6wbz",
    "0dG6U6s3T95t"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
