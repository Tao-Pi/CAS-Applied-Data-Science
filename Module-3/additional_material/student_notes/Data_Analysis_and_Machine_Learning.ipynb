{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d5ef5a-831d-4a0b-a59e-c7c922fb8a7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VbfWQ30hUCfD"
   },
   "source": [
    "# UNIBE CAS ADS Module 3 - Data Analysis and Machine Learning\n",
    "\n",
    "This document is a summary of Module 3, which presents and summarizes the different algorithms covered in class in a simple way, without delving into mathematical details. The goal is for non-specialists to be able to understand these examples and easily apply them to their own data.\n",
    "\n",
    "Graduates will:\n",
    "- **know** the **basic concepts** of machine learning: training,\n",
    "testing, over/ and underfitting, performance measures;\n",
    "- **can apply** software to perform linear regression, decision trees and random forest, neural networks (ML software is accessed via Python.)\n",
    "\n",
    "Learning objectives:\n",
    "- Machine learning overview, classification and regression\n",
    "- Linear model and logistic regression\n",
    "- Trees and forests\n",
    "- PCA and embeddings\n",
    "- Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c141308-c9d5-42e7-844e-1978bb1f4a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_XRrbkl4sNcO"
   },
   "source": [
    "# 1. Introduction\n",
    "The machine learning process generally follows a structured, step-by-step approach, which can be broken down into three main phases:\n",
    "\n",
    "\n",
    "1. Data Pre-processing:\n",
    "  - Import the data\n",
    "  - Clean the data\n",
    "  - Encoding data\n",
    "  - Split the data into training and test sets\n",
    "  - Feature scaling\n",
    "2. Modeling:\n",
    "  - Build the machine learning model\n",
    "  - Train the model on the training data\n",
    "  - Make predictions using the model\n",
    "3. Evaluation:\n",
    "  - Calculate performance metrics to assess how well the model performs\n",
    "  - Determine if the model is a good fit for the data and fulfills its intended purpose\n",
    "\n",
    "This process is key to building effective machine learning models, and throughout the examples, you'll gain practical experience applying these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f69790-3347-418c-a12b-aadf1a049c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dC8SspxixLmn"
   },
   "source": [
    "# 2. Data pre-processing\n",
    "In this chapter, we will explore in detail the **Pre-processing** process, which is common to all machine learning algorithms.\n",
    "It is important to master this pre-processing step before exploring the different algorithms.\n",
    "\n",
    "Data preprocessing is the crucial first step in any machine learning project because it transforms raw data into a clean, consistent format for the model:\n",
    "- Data Quality: Raw data often contains errors like missing values or duplicates. Preprocessing cleans this up to ensure the model learns from accurate data, leading to better predictions.\n",
    "- Consistency: Different formats across data sources can confuse the model. Preprocessing standardizes formats, ensuring consistent analysis.\n",
    "- Performance: Preprocessing highlights important features and reduces noise, enabling the model to find patterns more effectively and make more accurate predictions.\n",
    "- Efficiency: By simplifying data, preprocessing makes the model faster and more efficient, helping it focus on what matters most.\n",
    "\n",
    "Preprocessing ensures your data is clean, organized, and ready for the model, improving both accuracy and efficiency.\n",
    "\n",
    "## 2.1 Importing the libraries\n",
    "The first step in building machine learning models is to import the necessary libraries. In this case, we’ll work with three essential libraries:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36602292-3b18-491a-a11b-3e0b91787bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yf1QhkBg0gPX"
   },
   "outputs": [],
   "source": [
    "# numpy helps us work with arrays, which are commonly used as input for machine learning models.\n",
    "# It is typically imported with the shortcut np.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib creates charts and graphs.\n",
    "# It is usually imported with the shortcut plt.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas is used for data manipulation and preprocessing, such as importing datasets\n",
    "# and creating matrices of features and dependent variables.\n",
    "# It is commonly imported with the shortcut pd.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c814351-f160-4359-a508-eb2d29950ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XkKBP2Yg0fdL"
   },
   "source": [
    "## 2.2 Importing the dataset\n",
    "To begin data pre-processing in machine learning, the first step is to import a dataset. In this example, we are working with a CSV file, data.csv, which contains customer information for a retail company, including country, age, salary, and whether they purchased a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ac1277-465e-4c61-8211-7727be796978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fV8ACZJn1yzf",
    "outputId": "fc2b20a8-0abf-40ae-891d-d274f72e6a8c"
   },
   "outputs": [],
   "source": [
    "# We use the read_csv function from Pandas to import the dataset, which is typically a CSV file\n",
    "# In this case, the dataset is hosted on a server\n",
    "# This reads the file and stores it as a data frame, which holds all rows and columns of data from the CSV.\n",
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Data.csv')\n",
    "dataset.head() # display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd21750-8314-469f-a88b-8bdaff9b76c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9B_jKKKF3eNY"
   },
   "source": [
    "In any machine learning model, the dataset is split into two parts:\n",
    "- **Features** (Independent Variables): The information used to make predictions, typically the columns other than the last one.\n",
    "- **Dependent Variable**: The target outcome we want to predict, typically the last column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05154e4-50c7-4ccd-b3db-c8d3a22a05e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pupbt0Sm2WMp",
    "outputId": "7aa2afc4-c9d7-4af3-d58f-e1a7ea668cb0"
   },
   "outputs": [],
   "source": [
    "# the features \"X\" include country, age, and salary\n",
    "X = dataset.iloc[:, :-1].values # get all the rows and get all the column except the last one\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa97faf0-6beb-4fa2-89d3-a7a37b100cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBrkrblj2WBR",
    "outputId": "bc379df0-08df-4c52-b9a2-8f566c025d82"
   },
   "outputs": [],
   "source": [
    "# the dependent variable \"y\" is whether the customer purchased the product or not\n",
    "y = dataset.iloc[:, -1].values # get all the rows and get only the last column\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b67c8e-e61c-4be2-ad4d-d966029ef745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CY5rnszo6cIG"
   },
   "source": [
    "## 2.3 Taking care of missing data\n",
    "Missing data can cause errors when training machine learning models, so it’s important to handle it properly. There are two common approaches:\n",
    "\n",
    "1. Ignoring or Deleting Missing Data:\n",
    "This method works well when the amount of missing data is minimal (e.g., less than 1%). Deleting a small percentage of data may not significantly impact the model's performance.\n",
    "\n",
    "2. Replacing Missing Data:\n",
    "A more common and reliable approach is to replace missing values with a calculated statistic, such as the mean of the column. This ensures that no data is lost, especially when there are large amounts of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a0ffa9-abfb-49b3-add1-a5fb3aad08c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ6xrW-377Fs",
    "outputId": "f240d236-84dc-4ffe-e53e-d4e993066e47"
   },
   "outputs": [],
   "source": [
    "# As we can see here, we have missing age for Spain and missing salary for Germany\n",
    "# We want to replace this missing salary and age by the average\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a0389b-bc31-408e-9fe5-7dcb650b6691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNXV776f8I5z",
    "outputId": "6a3b5586-b6ef-4d2d-d41f-ab073c2ca8b2"
   },
   "outputs": [],
   "source": [
    "# Import the library that handle missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an instance of SimpleImputer specifying the replacement strategy.\n",
    "# In this case we replace all missing values \"missing_values=np.nan\" with the mean \"strategy='mean'\"\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# The fit() method connects the imputer to the data by analyzing the columns with\n",
    "# numerical values and calculating statistics like the mean or median.\n",
    "# We give our last two columns that contains age and salary to the imputer\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# The transform() method applies the transformation to the data by replacing missing values\n",
    "# with the calculated statistics. The result is an updated matrix of features with no missing values.\n",
    "# Then we update the original matrix X, replacing the missing values in the specified columns with the mean values.\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# By using both the fit() and transform() methods, you ensure that your matrix of features\n",
    "# is complete and ready for further processing, avoiding errors\n",
    "# in your machine learning model caused by missing data.\n",
    "# As we can see here, all the missing values were replaced by the mean of each column.\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b60176cf-5342-4435-82f5-a85df5e1864f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ruX4SAYvCNiz"
   },
   "source": [
    "## 2.4 Encoding Categorical Data\n",
    "In machine learning, we often need to convert categorical data (like country names) into numerical data so that models can process them. However, a simple numerical encoding (e.g., France = 0, Spain = 1, Germany = 2) can create unintended relationships between categories that don’t exist, as the model may incorrectly assume an order between these numbers.\n",
    "\n",
    "To avoid this issue, we use **one-hot encoding**, which transforms a single categorical column into multiple binary columns. For example, the \"country\" column with values like \"France,\" \"Spain,\" and \"Germany\" is converted into three binary columns:\n",
    "\n",
    "- France: **[1, 0, 0]**\n",
    "- Spain: **[0, 1, 0]**\n",
    "- Germany: **[0, 0, 1]**\n",
    "\n",
    "This method removes any implied numerical order between categories.\n",
    "\n",
    "Additionally, binary categorical data (such as \"yes\" or \"no\" values in the \"purchased\" column) can be safely encoded as 0 and 1 without negatively impacting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "140961c1-20be-4048-824c-8628b24470b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GdmHspDIC6ZD"
   },
   "outputs": [],
   "source": [
    "# To perform one-hot encoding in Python, we use the ColumnTransformer and OneHotEncoder\n",
    "# These tools automate the conversion of categorical columns into binary columns,\n",
    "# allowing machine learning models to interpret and process them correctly.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f485e4fb-b868-487a-ba3b-7552aaa5c277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDKKAtaFDY02",
    "outputId": "90adf9b9-bc68-4460-e1de-f6a3656ead70"
   },
   "outputs": [],
   "source": [
    "# Step 1: Let's now encode the features \"X\"\n",
    "\n",
    "\"\"\"\n",
    "We start by creating an object of the ColumnTransformer class.\n",
    "This object is responsible for applying transformations to specific columns.\n",
    "\n",
    "In the transformers argument, we specify three things:\n",
    "  - The type of transformation (e.g., \"encoder\" for one-hot encoding).\n",
    "  - The transformer to apply (e.g., OneHotEncoder()).\n",
    "  - The index of the column to transform (e.g., the \"country\" column, which has index 0).\n",
    "\n",
    "We also set the remainder argument to \"passthrough\" to ensure that columns not being transformed (e.g., age and salary) are retained.\n",
    "\"\"\"\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "\n",
    "# the fit_transform() method is used\n",
    "# to fit the ColumnTransformer to the data and apply the transformation in one step.\n",
    "# The result is an updated matrix of features with one-hot encoded columns.\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Since machine learning models expect the feature matrix (X) as a NumPy array,\n",
    "# we convert the result of fit_transform() to a NumPy array using np.array().\n",
    "X = np.array(X)\n",
    "\n",
    "# This process efficiently transforms categorical data into a suitable format\n",
    "# for machine learning models while ensuring that the rest of the feature matrix remains intact.\n",
    "# Let's check how our countries were transformed\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d238c1bd-fb10-4ed8-97f2-dc9b56009c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noWNRp4rFUu9",
    "outputId": "b959e36c-e82d-4136-9655-9f41edb4e09a"
   },
   "outputs": [],
   "source": [
    "# Step 2: Let's now encode the dependant variable \"y\"\n",
    "\n",
    "\"\"\"\n",
    "for binary categorical data in the dependent variable (e.g., \"yes\" and \"no\"),\n",
    "we use label encoding.\n",
    "Label encoding converts these text values into binary numerical values:\n",
    "\n",
    "  - \"No\"  becomes 0\n",
    "  - \"Yes\" becomes 1\n",
    "\"\"\"\n",
    "\n",
    "# This transformation is done using the LabelEncoder class from scikit-learn.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# The fit_transform() method quickly converts the text into corresponding\n",
    "# numerical values for the dependent variable vector, which is now ready for model training.\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# By label encoding for binary outcomes, you ensure that categorical data is\n",
    "# in a format that machine learning models can process effectively.\n",
    "# Let's check the result of the transformation\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66009b0e-3f32-4edd-beb5-4bc2e1cf66a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "IXxZlZ3_HmV5"
   },
   "source": [
    "## 2.5 Splitting the dataset into Training set and Test set\n",
    "When preparing data for machine learning models, there are two key steps: **splitting the dataset** into training and test sets, and applying **feature scaling**.\n",
    "\n",
    "1. Splitting the Dataset, the dataset is split into two parts:\n",
    "  - Training Set: Used to train the model on existing observations.\n",
    "  - Test Set: Used to evaluate the performance of the model on new, unseen data, simulating future predictions.\n",
    "2. Feature Scaling:\n",
    "  - Feature scaling ensures that all features are on the same scale to prevent one feature from dominating others. This step adjusts the variables so that they take values within the same range, usually by normalizing or standardizing the data.\n",
    "\n",
    "**Feature Scaling must be applied AFTER splitting the dataset**\n",
    "\n",
    "The reason for this is to avoid **information leakage**. The test set is meant to represent new, unseen data. If you apply feature scaling before the split, you would be calculating the mean and standard deviation using the entire dataset, including the test set. This would allow the model to \"see\" the test data during training, which defeats the purpose of having a separate test set and leads to an inaccurate evaluation of model performance.\n",
    "\n",
    "By applying feature scaling after the split, you ensure that the test set remains untouched during training, preserving its role as new data to validate the model’s true performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40d974af-2ebc-4d00-9c3a-c6124d3fd879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OXuUP18qJ3iI"
   },
   "outputs": [],
   "source": [
    "# To split the dataset, we use the train_test_split function from scikit-learn’s model_selection module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "The function train_test_split splits the dataset into four sets:\n",
    "  X_train: Matrix of features for the training set.\n",
    "  X_test:  Matrix of features for the test set.\n",
    "  y_train: Dependent variable for the training set.\n",
    "  y_test:  Dependent variable for the test set.\n",
    "\n",
    "The main parameters are:\n",
    "  X: Matrix of features.\n",
    "  y: Dependent variable vector.\n",
    "  test_size: Specifies the proportion of data to allocate to the test set (e.g., 20% or 0.2).\n",
    "  random_state: A fixed seed to ensure reproducibility of the split.\n",
    "\n",
    "The function returns four sets: X_train, X_test, y_train, and y_test.\n",
    "Typically, 80% of the data is assigned to the training set and 20% to the test set.\n",
    "This ratio ensures the model has enough data to learn from while still reserving a portion for evaluation.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a5ee295-6f2b-499e-924c-f7f072ccba76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP5c5s_6KrzD",
    "outputId": "91f911b5-c1cb-498f-aa3b-aa4edc3308b4"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98b8ac8-91e4-4072-94a7-ac8279f03238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQz7ejN3KrpL",
    "outputId": "0ecd871c-9a7b-4953-b3e3-8f704d136b0b"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38074fa1-eeca-406a-b118-350d0878ce2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqIaRgBoKrfk",
    "outputId": "4095dcfb-b920-4a05-9b70-551261ee97ce"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "891f0d17-e7d9-4807-a672-7125e76f422a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvUOajs-KrUk",
    "outputId": "c17238b6-942e-45ea-ae67-22c359c4f10d"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb287cc7-6ef8-4b40-a467-17c7be94c3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rK1ouVz6LUCE"
   },
   "source": [
    "## 2.6 Feature Scaling\n",
    "**Feature scaling** is the process of putting all features on the same scale to prevent certain features from dominating others due to differences in their magnitudes. However, feature scaling is not necessary for all machine learning models but is crucial for models sensitive to the scale of data, such as gradient-based algorithms (e.g., logistic regression, support vector machines).\n",
    "\n",
    "There are two Main Techniques:\n",
    "- Standardization:\n",
    "  - Formula: Subtract the mean of the feature and divide by the standard deviation.\n",
    "  - Result: Values are typically scaled between -3 and +3.\n",
    "  - Works well for all types of data, making it a reliable go-to method.\n",
    "- Normalization:\n",
    "  - Formula: Subtract the minimum value of the feature and divide by the range (maximum - minimum).\n",
    "  - Result: Values are scaled between 0 and 1.\n",
    "  - Best used when features follow a normal distribution.\n",
    "\n",
    "Feature scaling should always be applied **after splitting** the dataset into training and test sets. This is to prevent information leakage from the test set into the training process. Feature scaling is fitted on the training data (X_train), and then the same scaling transformation is applied to the test data (X_test), using the mean and standard deviation calculated from X_train.\n",
    "\n",
    "Practical Recommendation:\n",
    "- Standardization is generally preferred because it works well across various scenarios and always improves model performance by ensuring consistent feature scaling.\n",
    "- Normalization is ideal when you have normally distributed data.\n",
    "\n",
    "Standardization should only be applied to numerical features, not to dummy (one-hot encoded) variables.\n",
    "Dummy variables (e.g., binary 0s and 1s representing categorical data)\n",
    "are already on the same scale, and applying scaling to them would distort the interpretation of these values.\n",
    "\n",
    "Dummy variables already take values between 0 and 1, so scaling them is unnecessary\n",
    "and could result in the loss of interpretability.\n",
    "For instance, you may lose the ability to identify which binary values represent specific categories\n",
    "(e.g., countries). Moreover, applying scaling to dummy variables\n",
    "does not significantly improve model performance, and in most cases,\n",
    "it can even make the interpretation harder.\n",
    "\n",
    "Thus, feature scaling should be applied only to the numerical variables\n",
    "(e.g., age, salary) where the scale differs significantly.\n",
    "This ensures that your machine learning models are properly trained\n",
    "without distorting categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed502f22-138a-4607-96c7-4bdf08fd9113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "94Bc1vM5ODcW"
   },
   "outputs": [],
   "source": [
    "# To apply feature scaling, we use the StandardScaler class from the scikit-learn library,\n",
    "# which standardizes the features by subtracting the mean and dividing by the standard deviation.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initiate the scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "\"\"\"\n",
    "X_train[:, 3:] means that we take all the rows and only the age and salary columns\n",
    "\n",
    "The first step is to fit the scaler on the training set (X_train) to compute\n",
    "the mean and standard deviation for each numerical feature.\n",
    "This process ensures that the scaling is based solely on the training data.\n",
    "You only need to scale the numerical columns.\n",
    "\n",
    "In this case, the numerical columns are the age and salary columns,\n",
    "which have indexes 3 and 4, respectively.\n",
    "We exclude the dummy (one hot encoded) variables from scaling.\n",
    "\"\"\"\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "\"\"\"\n",
    "After applying feature scaling to the training set (X_train),\n",
    "the next step is to apply the same transformation to the test set (X_test).\n",
    "This is crucial to ensure that the test data is scaled using the same parameters\n",
    "(mean and standard deviation) computed from the training set.\n",
    "\n",
    "Since the test set simulates new, unseen data, you should only apply the transform()\n",
    "method (not fit_transform()), using the scaler that was fitted on the training set.\n",
    "This ensures consistency in scaling between the training and test sets,\n",
    "and prevents the model from learning any information from the test data (avoiding information leakage).\n",
    "\"\"\"\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0114d2ff-2ee8-4b6a-988d-34f4b4e6875e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3y2_oNn6T2LU"
   },
   "source": [
    "After applying the scaling to both X_train and X_test, you can print them to verify that the numerical columns (e.g., age and salary) are now scaled between a common range, typically between -3 and +3 or -2 and +2, while dummy variables remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ae7107a-fea1-4dae-ab0e-587030966f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSwa3VftT5IC",
    "outputId": "3df96bd5-042c-45bb-8582-6bbd64d9cd29"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75551a79-b268-4314-be51-33cc42325cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW3ETV7PUDmu",
    "outputId": "d71aeb48-63b1-42fa-a864-bde6035dc0e9"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d361b3dd-14f1-45fb-96bc-bc4295faccf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yHJgtX-TUPLc"
   },
   "source": [
    "By properly scaling both the training and test sets, you ensure that your machine learning models are trained and evaluated under consistent conditions, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6607d2e8-99da-4a45-8111-6d03352dee28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "whZATC5BUXN3"
   },
   "source": [
    "# 3. Machine Learning Algorithms\n",
    "In this chapter, we are going to cover all the machine learning algorithms we have seen during the module 3.\n",
    "\n",
    "For each algorithm, we will start with some theory, followed by a simple example of training a model, and finally, the evaluation of its performance. The details of pre-processing are not covered in this chapter.\n",
    "\n",
    "## 3.0 Supervised and Unsupervised learning\n",
    "There are two types of machine learning algorithms:\n",
    "- **Supervised learning** involves training a model using labeled data. This means the model receives input data along with the correct output (answers). For example, if you provide images of apples and bananas to the model, you also provide labels that say which image is an apple and which is a banana. The model learns from this data and can then make predictions on new, unseen images.\n",
    "- **Unsupervised learning**, on the other hand, works with unlabeled data. Here, the model does not have any answers or labels to guide it. It must find patterns or relationships in the data on its own. One key technique in unsupervised learning is clustering.\n",
    "\n",
    "**What is Clustering?**\n",
    "\n",
    "Clustering is the process of grouping similar data points together. In unsupervised learning, where we don't provide the model with any labels, clustering helps the model identify and create groups based purely on the patterns in the data.\n",
    "\n",
    "For instance, if we provide a dataset of customer information, such as their annual income and spending habits, but don’t specify any categories (e.g., high spenders vs. low spenders), the model will find natural clusters in the data. These clusters might represent different types of customers based on their income and spending behavior, even though we never explicitly told the model who belongs to which group.\n",
    "\n",
    "**In conclusion**\n",
    "\n",
    "- In supervised learning, the model is trained on data where the answers are known.\n",
    "- In unsupervised learning, like clustering, the model explores patterns in unlabeled data, creating groups without knowing the \"right\" answers.\n",
    "- Clustering is a powerful tool in unsupervised learning, allowing us to uncover hidden structures in data\n",
    "\n",
    "## 3.1 Performance measures\n",
    "### 3.1.1 MSE, MAE, R-squared\n",
    "\n",
    "To measure the performance the performance of a **regression** algorithm, we would use the **MSE**, **MAE** AND **R-squared** metrics:\n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "  - **MSE** gives you an idea of the average squared error in your predictions. Since it squares the errors, it makes large errors stand out more.\n",
    "  - How to interpret ?\n",
    "    - The lower the MSE, the better your model is at predicting the target variable.\n",
    "    - Because MSE squares the errors, the units of MSE are squared as well, which makes it harder to interpret directly.\n",
    "    - For example, if your target variable is salary , an MSE of 25'000$ means your average squared error is 25,000^2$. In real-world terms, you usually look for a smaller value because that means smaller errors.\n",
    "  - In summary\n",
    "    - Look for small values, but remember that it’s in squared units. Lower MSE means fewer large errors.\n",
    "2. Mean Absolute Error (MAE)\n",
    "  - **MAE** tells you the average error in your predictions, but in the same units as your target variable.\n",
    "  - How to interpret ?\n",
    "    - Let’s say your MAE is 1'000. This means, on average, your model's predictions are off by $1,000, either too high or too low.\n",
    "    - Lower MAE values indicate better performance. If MAE is 0, your model is making perfect predictions.\n",
    "    - MAE is easier to understand than MSE because it’s in the same units as your data. For example, if you’re predicting salaries, the MAE directly tells you how far off you are on average\n",
    "  - In summary\n",
    "    - This gives you an easy-to-understand average error in the same units as your target. The smaller, the better.\n",
    "3. R-squared (R²)\n",
    "  - **R-squared** tells you how well your model is explaining the variation in the data.\n",
    "  - How to interpret ?\n",
    "    - R² = 1: Your model is perfect; it explains all the variation in the data.\n",
    "    - R² = 0: Your model doesn’t explain any of the variation; it’s no better than just guessing the average of the target variable.\n",
    "    - Negative R²: This is rare but means your model is doing worse than just predicting the average.\n",
    "    - Example: If R² is 0.85, it means 85% of the variability in the target variable is explained by your model, which is usually considered good.\n",
    "  - In summary:\n",
    "    - The closer to 1, the better. It tells you how much of the variation in the target variable your model explains.\n",
    "\n",
    "\n",
    "**Practical Example**: Let’s say you’re predicting salaries based on years of experience:\n",
    "- MSE: If you get an MSE of 50,000, it indicates the average squared difference between predicted and actual salaries is quite large. You may want to improve your model to reduce this.\n",
    "- MAE: If your MAE is 1,500, it means, on average, your predictions are off by $1,500. This is a more intuitive way to see how well your model is doing.\n",
    "- R²: If your R² is 0.9, this means your model explains 90% of the variation in salaries based on years of experience, which suggests it’s a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e336a8bc-c196-4dba-9f56-cabbec615169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DNOhs9TpTTfX"
   },
   "source": [
    "### 3.1.2 Confusion matrix\n",
    "A **confusion matrix** is a tool used to evaluate the performance of a **classification** model. It provides a detailed breakdown of how well a model's predictions match the actual outcomes by displaying the counts of true and false predictions for each class.\n",
    "\n",
    "For a binary classification problem (where the possible outcomes are \"Positive\" and \"Negative\"), the confusion matrix is a 2x2 table:\n",
    "\n",
    "|  | Predicted Positive | Predicted Negative |\n",
    "| - | - | - |\n",
    "| Actual Positive |\tTrue Positive (TP) |\tFalse Negative (FN) |\n",
    "| Actual Negative |\tFalse Positive (FP) |\tTrue Negative (TN) |\n",
    "\n",
    "\n",
    "Each cell contains the count of predictions falling into that category:\n",
    "\n",
    "- True Positive (TP): The model predicted Positive, and the actual value is Positive.\n",
    "- True Negative (TN): The model predicted Negative, and the actual value is Negative.\n",
    "- False Positive (FP): The model predicted Positive, but the actual value is Negative (also called a \"Type I Error\").\n",
    "- False Negative (FN): The model predicted Negative, but the actual value is Positive (also called a \"Type II Error\").\n",
    "\n",
    "**Why Use a Confusion Matrix?**\n",
    "\n",
    "A confusion matrix gives a more detailed breakdown of a model's performance beyond just its accuracy (the percentage of correct predictions).\n",
    "\n",
    "It shows how well the model distinguishes between the different classes.\n",
    "Whether the model is biased toward predicting one class over another.\n",
    "Which types of errors the model is making (e.g., predicting more false positives or false negatives).\n",
    "\n",
    "**How to Use a Confusion Matrix?**\n",
    "\n",
    "After making predictions with a classification model, you can create a confusion matrix to compare the predicted results with the actual labels.\n",
    "\n",
    "By inspecting the number of false positives and false negatives, you can understand where your model is going wrong and whether it is making specific types of errors more frequently.\n",
    "\n",
    "The confusion matrix is a foundation for other performance metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**How to Interpret a Confusion Matrix?**\n",
    "\n",
    "You can derive several important metrics from the confusion matrix:\n",
    "\n",
    "1. **Accuracy**: The ratio of correctly predicted instances (both true positives and true negatives) to the total instances.\n",
    "\n",
    "   \n",
    "   $${Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "   \n",
    "\n",
    "2. **Precision**: The proportion of true positive predictions out of all positive predictions. This answers the question: \"Of all the predicted 'Yes' instances, how many were actually 'Yes'?\"\n",
    "\n",
    "   $${Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "   - High precision means few false positives.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**: The proportion of actual positive instances that were correctly identified. This answers the question: \"Of all the actual 'Yes' instances, how many did the model correctly predict?\"\n",
    "\n",
    "   $${Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "   - High recall means few false negatives.\n",
    "\n",
    "4. **Specificity (True Negative Rate)**: The proportion of actual negative instances that were correctly identified. This is the opposite of recall, measuring how well the model identifies the \"No\" class.\n",
    "\n",
    "   $${Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "5. **F1-Score**: The harmonic mean of precision and recall, giving a single metric that balances both. It is particularly useful when you want to account for both false positives and false negatives.\n",
    "\n",
    "   $${F1score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "\n",
    "**Example of Using a Confusion Matrix**\n",
    "\n",
    "Let’s say you have a model that predicts whether people buy a product. After testing it on 100 people, you get the following confusion matrix:\n",
    "\n",
    "| | Predicted: Buy |\tPredicted: Not Buy |\n",
    "| - | - | - |\n",
    "| Actual: Buy |\t40 (TP) |\t10 (FN) |\n",
    "| Actual: Not Buy |\t5 (FP) |\t45 (TN) |\n",
    "\n",
    "From this confusion matrix:\n",
    "\n",
    "- Accuracy = (40 + 45) / (40 + 45 + 5 + 10) = 85% (Overall, the model is correct 85% of the time).\n",
    "- Precision = 40 / (40 + 5) = 88.89% (When the model predicts someone will buy, it is correct 88.89% of the time).\n",
    "- Recall = 40 / (40 + 10) = 80% (The model correctly identifies 80% of actual buyers).\n",
    "- Specificity = 45 / (45 + 5) = 90% (The model correctly identifies 90% of non-buyers).\n",
    "\n",
    "**When to Use a Confusion Matrix?**\n",
    "\n",
    "- Binary Classification: It’s widely used in binary classification problems where you want to evaluate the model’s ability to correctly identify two classes (like \"Yes\" vs. \"No\").\n",
    "- Multi-Class Classification: You can extend the confusion matrix to multi-class classification, where each row and column will represent the number of instances of a specific class being classified as another class.\n",
    "- Imbalanced Datasets: If your dataset has a class imbalance (e.g., 95% of data points are in one class), accuracy alone can be misleading. The confusion matrix helps focus on more specific metrics like precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c02890-2692-4c25-8d6d-833b0cd490e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "39hNNOvWUp0A"
   },
   "source": [
    "## 3.2 Simple Linear regressions\n",
    "Linear regression is one of the simplest algorithms in machine learning, often used for predicting continuous values. The goal of linear regression is to draw the straightest line possible through those points to represent their general trend.\n",
    "\n",
    "In essence, linear regression tries to adjust *m* and *b* so that the line fits the data points as closely as possible. Once you have that line, you can make predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a731b6-485a-4cd0-8002-12988e5da05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "unHZ27DHUplt"
   },
   "source": [
    "Importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dab98cc-7a26-4f27-b116-07751dd90d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "j5IsVHuuZcT3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a308213b-9851-4ddc-b2b1-9c517a136acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GcmusBkUZ8wZ"
   },
   "source": [
    "Importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32750560-19dc-4389-b47a-ccf3d13d1acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZTpZ5EN-Z_wY",
    "outputId": "a1cf80d6-b879-4f96-ec7c-9ff7cc8d51d5"
   },
   "outputs": [],
   "source": [
    "# import the dataset, create the features and dependant variables\n",
    "dataset = pd.read_csv(\"https://static.grosjean.io/cas/module3/Salary_Data.csv\")\n",
    "X = dataset.iloc[:, :-1].values # X = [YearsExperience]\n",
    "y = dataset.iloc[:, -1].values # y = [Salary]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090b4ae7-2092-49a5-a705-23e35345a58b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gmm8AjbQb0Nb"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19d42f4-5994-4e5b-856e-5c94fdde7d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "wltPC0wzb5Sp",
    "outputId": "a95f0f6d-c037-4d94-ea16-2c41b9681e92"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Train the Linear Regression model on the training set\n",
    "# the regressor contains our trained linear regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78f736c-9960-4912-9505-6dc0265f0e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DcBpY8WAcaA8"
   },
   "source": [
    "Use our newly created model to make some prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05084514-acf2-4d1f-bc9d-ed717e74552d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaIwWVm8ZqXc",
    "outputId": "1a1625d2-2e0e-4fd3-fba4-08a84a704f88"
   },
   "outputs": [],
   "source": [
    "for age in range(1, 6):\n",
    "  prediction = regressor.predict([[age]])[0]\n",
    "  print(f\"At {age} years of experience you are expected to earn {prediction:.2f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "176b8a65-5a37-49e8-9fd3-d008d84ecfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hr5zb_YzkUiY"
   },
   "source": [
    "Compute the performance metrics MSE, MAE and R-squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "015c1910-aa38-46b8-bf44-18c8e6eb0b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7Rd-YjDkaf6",
    "outputId": "221f9069-2f83-40d8-cb54-51076760e4e2"
   },
   "outputs": [],
   "source": [
    "# Predict the test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5247b6e3-8f25-4248-abdd-703804fa30b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "kkZ9UleOcltW",
    "outputId": "e94c1494-8d17-4cce-a275-a830a2ba9717"
   },
   "outputs": [],
   "source": [
    "# Plot the Training set results\n",
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (Training set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# Plot the Test set results\n",
    "plt.scatter(X_test, y_test, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title('Salary vs Experience (Test set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40245e04-ad57-4a04-8b08-3661d02951ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KDbXQlMoUcph"
   },
   "source": [
    "## 3.3 Logistic regression\n",
    "**Logistic regression** is a statistical method used to predict a categorical outcome (like \"yes\" or \"no\") based on one or more independent variables. Unlike linear regression, which predicts a continuous outcome, logistic regression predicts probabilities that are then converted into binary outcomes (e.g., 0 for no, 1 for yes).\n",
    "\n",
    "For example, an insurance company might want to predict whether a person will buy health insurance based on their age. The result is a probability (e.g., a 42% chance a 35-year-old will buy it). If the probability is greater than 50%, the model predicts \"yes,\" otherwise it predicts \"no.\"\n",
    "\n",
    "The logistic regression model uses a sigmoid curve to map input variables (like age) to a probability between 0 and 1. You can also include multiple independent variables, like income or education, to make more complex predictions.\n",
    "\n",
    "\n",
    "**Maximum likelihood estimation (MLE)** is a method used to find the best-fitting curve for a model, like logistic regression. The idea is to determine which curve is most likely to predict the actual data we have. Here's how it works:\n",
    "\n",
    "For each data point, we calculate the probability that the model predicts the outcome (either yes or no). For example, the model might predict a 3% chance for one person to say yes to an offer, and for another, a 95% chance.\n",
    "\n",
    "To find the best-fitting curve, we calculate the likelihood by multiplying these probabilities for all data points. For the \"no\" outcomes, we subtract the probability from 1 (e.g., 1% chance of yes means a 99% chance of no).\n",
    "\n",
    "Then, we compare the likelihoods of different curves, adjusting the curve until we find the one that gives the highest likelihood—this is the curve that best fits our data.\n",
    "\n",
    "In short, MLE helps us pick the curve that most accurately predicts the observed outcomes by maximizing the likelihood of the predictions being correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea69f36d-119f-4571-906f-b23fcb04c815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PgsnFBnvPE_j"
   },
   "source": [
    "Importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5207c98d-20ec-4cf9-bb98-a1dd99467bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uq65DjMGgSV4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ef3abd-77b4-4886-b2f4-089f57d87571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eMbwvVCvgbC4"
   },
   "source": [
    "Importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f309f9b-6a10-4418-9f28-33c83ce4fd8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kHB9ma-EgeWI",
    "outputId": "6ace0a4b-d653-4589-d376-01c639ce9aab"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"https://static.grosjean.io/cas/module3/Social_Network_Ads.csv\")\n",
    "\n",
    "X = dataset.iloc[:, :-1].values # [Age, EstimatedSalary]\n",
    "y = dataset.iloc[:, -1].values # [Purchased]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc8b84e-3af4-4565-aa89-aa4431739f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NRDSUfl5hRH0"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "988e6afe-b9b5-48d5-a141-af8eb30a60b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "1BKu2PjDhM8q",
    "outputId": "edc35864-2c02-49b8-d9d0-1370580a26fa"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# train the classifier\n",
    "classifier = LogisticRegression(random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e5a6a2-c3b6-45b3-b498-336c9c9e0089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hi_EKxtzsHhN"
   },
   "source": [
    "Predict a new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d102c9-7363-4fa5-b298-8b66fcab42e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5n5engvsKpM",
    "outputId": "18d9773e-7c4a-42de-ccf6-0540d3b1b100"
   },
   "outputs": [],
   "source": [
    "# Based on the age and salary we want to know\n",
    "# if a person will click an ad and buy the product\n",
    "Age = 32\n",
    "Salary = 120000\n",
    "\n",
    "result = classifier.predict(sc.transform([[Age, Salary]]))[0]\n",
    "result = \"Yes\" if result == 0 else \"No\"\n",
    "\n",
    "print(f\"Will a {Age} years old that has a salary of {Salary} buy the ad ? {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe09c64-0486-41a8-a5c5-0c936db79061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3wmonC4Mtw5N"
   },
   "source": [
    "Compute the performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76647e8c-9cc9-491c-be6f-c27470694824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eOB_yrxtzfF",
    "outputId": "ce83f60e-2a05-4e39-dcd0-324201fbcd4f"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba8103f-9336-4738-9f70-4aae90cd568a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "INLUxXBmuYCr"
   },
   "source": [
    "Plot the Training set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515bd14a-e838-4e58-844a-046664c604e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "nTL2oHcKubU1",
    "outputId": "b41ee9cd-ccb7-442d-c761-f19179fdbcf7"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968758f6-1389-4e03-9f2c-9b528bb04260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "J7bzkUSDutDf"
   },
   "source": [
    "Plot the tests set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e87e6e3c-f8a9-4034-b66d-d033a6ae536b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "U1VaRGmvuwdt",
    "outputId": "9c81f6d3-8d53-4bff-ae88-2c745aa70630"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b350f074-6b9e-4061-923c-eb80f0d30d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "F74Fu2RfUfCH"
   },
   "source": [
    "## 3.4 Decision Tree classification\n",
    "A **decision tree** is a simple and intuitive machine learning algorithm that helps us make decisions based on conditions. It works by splitting data into smaller parts based on features, making it easier to classify or predict outcomes.\n",
    "\n",
    "The tree starts with a root node that represents the entire dataset. It then makes decisions at each split by asking a question based on the data’s features, like “Is age greater than 30?” Each branch represents a possible answer (yes/no), leading to further splits until the data is fully categorized or predicted.\n",
    "\n",
    "The final outcomes are represented by leaf nodes, which give the final prediction or classification.\n",
    "\n",
    "**Key Concepts**\n",
    "- **Splitting**: The process of dividing the data based on conditions.\n",
    "- **Leaf Node**: The end point of a branch that gives the final classification or value.\n",
    "- **Entropy and Gini Impurity**: These are metrics used to decide the best way to split the data. They help the algorithm determine which feature divides the data most effectively.\n",
    "\n",
    "**Should I use decision tree ?**\n",
    "\n",
    "Use decision trees when you need a simple, interpretable model for classification or regression, especially with non-linear relationships and small datasets. They're prone to overfitting but useful for understanding decision-making. Alternatives like **Random Forests** (for reducing overfitting) and **Gradient Boosting** (for better performance) build on decision trees for more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624d0497-5da8-4a0b-ad4c-b8bc6b7e64bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xh5vkQkev_9L"
   },
   "source": [
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21819689-f6b0-4619-a1f4-ff9f49b64e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H5S30By3v_NS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "061c31f4-25c9-4e1f-b756-9587be1964c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yJ6R3EcjwEFx"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffcf5eff-2184-4de1-b8d8-822ade725a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zIWbO81YwFq1",
    "outputId": "ae6f02eb-d696-4fea-ba6d-95a2a95fa6f8"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, :-1].values # [Age, Salary]\n",
    "y = dataset.iloc[:, -1].values # [Purchased]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eee953e0-be20-48f6-89c1-d3f8882684d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AxwOLQkbxawn"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f92947bf-7af6-458b-bab1-f4bc299cc914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "wG7O4mLgxekb",
    "outputId": "4768c4ef-87b1-4996-e036-ceeef7d6ab9b"
   },
   "outputs": [],
   "source": [
    "# Split the train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# features scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# train the classifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cfca135-eb60-4aba-b5e0-c06161ffa775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0ewoJ4Odx8JU"
   },
   "source": [
    "Predict a new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421607f2-ee4e-4768-ac2c-2db65476668d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l2Yco1ux9gm",
    "outputId": "30cf2c11-2e83-4142-8148-59be05d1f54b"
   },
   "outputs": [],
   "source": [
    "# Based on the age and salary we want to know\n",
    "# if a person will click an ad and buy the product\n",
    "Age = 32\n",
    "Salary = 120000\n",
    "\n",
    "result = classifier.predict(sc.transform([[Age, Salary]]))\n",
    "result = \"Yes\" if result == 0 else \"No\"\n",
    "\n",
    "print(f\"Does a {Age} years old that has a wage of {Salary} will buy the ad ? {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b721c582-70c4-483b-8ad9-d8b44ab36b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GRajI0wqyngi"
   },
   "source": [
    "Compute the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cccdd283-a12e-48c5-b555-44daff613515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nq6DjA-rypyp",
    "outputId": "5c54c5fa-de9e-4663-fa15-91307ca0b633"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75fc443-14a7-4cd7-a2bd-acc247476cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "86cpTIUxy2b-"
   },
   "source": [
    "Visualize the training set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82924ce9-c43f-4859-b2d8-49180dd16ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "iwX--cpDy2Nr",
    "outputId": "8cf2b646-5cfc-4524-b1ce-eb5464bdd2b7"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Decision Tree Classification (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89660aca-e322-43fe-b33d-383023823058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vRlftotQywzk"
   },
   "source": [
    "Visualize the Test set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcf912dd-a36d-4995-a6a3-fb0af2e725a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "GajrQkzEzHK0",
    "outputId": "c9d9d7fd-13f4-4842-caf8-64a3e0259e22"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Decision Tree Classification (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63150cf2-8e7f-4f46-9b2b-79ac9d727a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NHoFdaAlUjgT"
   },
   "source": [
    "## 3.5 Random Forests classification\n",
    "\n",
    "In random forests, we use **ensemble learning**, which combines multiple machine learning models to improve performance. Specifically, random forests use **multiple decision trees**. Instead of building one decision tree, we build several, each trained on a random subset of the data.\n",
    "\n",
    "Here’s how it works:\n",
    "- Select a random subset of data points from your training set.\n",
    "- Build a decision tree using that subset.\n",
    "- Repeat this process to create multiple trees.\n",
    "- When making a prediction for new data, each tree gives its result, and the final prediction is based on the majority vote.\n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fe5d8b-e9e8-4bec-ab1b-08caba3d300a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7EPpozKlzsmu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2662cdba-2845-48de-bff4-ba4a7ab506bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "D50Mct--zpSJ"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75818167-b566-4cae-8868-7383403c32e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aIvVoFeCzyeq",
    "outputId": "967b7220-89ad-4521-a53a-0d22c331f741"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, :-1].values # [Age, Salary]\n",
    "y = dataset.iloc[:, -1].values # [Purchased]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d533c61a-202e-4c0a-b87a-01f5e9bb149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "i67pYMCmz4Jt"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8dac40-e3e7-4e1c-9f61-bb8a3a34fa44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "r4fLtL4yUUMV",
    "outputId": "22b923ff-74c8-4dfe-8a05-8f4b6929db26"
   },
   "outputs": [],
   "source": [
    "# Split the train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# train the model\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28d2d7b5-ab91-4e57-95ed-19cbe7daa337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f4hlS4R80V1-"
   },
   "source": [
    "Predict a new result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7311269e-ed62-4519-b236-93d2028d4f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCzxtzhN0S2S",
    "outputId": "906c6632-a3fc-43a4-ec34-462872368a8e"
   },
   "outputs": [],
   "source": [
    "# Based on the age and salary we want to know\n",
    "# if a person will click an ad and buy the product\n",
    "Age = 32\n",
    "Salary = 120000\n",
    "\n",
    "result = classifier.predict(sc.transform([[Age, Salary]]))\n",
    "result = \"Yes\" if result == 0 else \"No\"\n",
    "\n",
    "print(f\"Does a {Age} years old that has a wage of {Salary} will buy the ad ? {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "022671a5-2f55-4dc6-9818-c30f5025796b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "j4egcRAO0fgc"
   },
   "source": [
    "Predict the test set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "357cba26-b365-4d8d-95cd-5bf9c4735721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAD4Cgkf0BaJ",
    "outputId": "c2691e36-8fde-444b-ee3c-7827a280d704"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b83e1e63-23dd-4520-be61-e05070460b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5Od4BKvW0qB3"
   },
   "source": [
    "Plot the training set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb79728-ae8b-4380-ab69-012ee7206193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "cWA79UzN0jAM",
    "outputId": "6d3c0fb3-afd2-4571-98c8-f80cc8452274"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Random Forest Classification (Training set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab7b133-4dde-40c9-93ba-e08a9d7a09f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w6QoR85o07II"
   },
   "source": [
    "Plot thetest set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e51ed1-ca30-4ad2-82a6-7496a1395988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "9VN-xACD0vM0",
    "outputId": "aac469b2-0173-432b-f7e1-e7f31fa21d8d"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('Random Forest Classification (Test set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "303642b0-c7a1-400a-942e-356f4bc5e617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "l35Ll0mVCr7Q"
   },
   "source": [
    "## 3.6 PCA\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a widely used **unsupervised learning algorithm** primarily for dimensionality reduction. The goal of PCA is to simplify complex datasets by reducing the number of variables (or dimensions) while retaining most of the original information. Here's a simplified breakdown of how PCA works and its key points:\n",
    "\n",
    "- **Purpose of PCA**: PCA is used to find correlations between variables in a dataset. If strong correlations exist, PCA helps reduce the dimensionality, which makes the data easier to work with and visualize, while retaining most of the key information. It's often used for tasks like visualization, feature extraction, noise filtering, and even in applications like stock market prediction or gene analysis.\n",
    "\n",
    "- **How PCA Works**:\n",
    "  - It identifies the directions of maximum variance in high-dimensional data.\n",
    "  - Then, it projects the data onto a smaller subspace, preserving as much information as possible.\n",
    "  - For example, if you have data with D dimensions, PCA reduces it to K dimensions (where K < D) by selecting the most informative features.\n",
    "\n",
    "- **Steps in PCA**:\n",
    "  - **Standardize** the data to ensure each feature contributes equally.\n",
    "  - **Calculate** the eigenvectors and eigenvalues from the data’s covariance matrix.\n",
    "  - **Sort the eigenvalues** in descending order, as the larger eigenvalues represent more variance.\n",
    "  - **Construct a projection matrix** using the top K eigenvectors.\n",
    "  - **Transform the data** using this projection matrix to reduce dimensions.\n",
    "\n",
    "- **PCA Visualization**: PCA can be visualized in both 2D and 3D. In 3D visualizations, it becomes clearer how PCA transforms high-dimensional data into lower dimensions by projecting it along the axes that capture the most variance. Interactive tools can help users drag and view how PCA adjusts the data points.\n",
    "\n",
    "- **PCA vs. Linear Regression**: PCA may seem similar to linear regression but serves a different purpose. While linear regression predicts values, PCA seeks to understand the relationship between variables by identifying principal axes in the data.\n",
    "\n",
    "- **Limitations**: One key weakness of PCA is its sensitivity to outliers—outliers can distort the results, so it’s important to clean data before applying PCA.\n",
    "\n",
    "In summary, PCA is a powerful tool for simplifying complex datasets, making it easier to visualize and analyze data while retaining key patterns.\n",
    "\n",
    "Importing the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd15e6df-e044-4d7a-a21b-9ae00273e2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XQJ1qWe309Qc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ccbc845-8cfd-4845-a5b9-6a181d4ec8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "M6KX8ZCpC_TF"
   },
   "source": [
    "Importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16318ce0-1471-4915-9a18-c7d34e389a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "a56-B1ygC60A",
    "outputId": "8c203322-df76-436b-fb19-ff9c51cfddaf"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Wine.csv')\n",
    "X = dataset.iloc[:, :-1].values # [Alcohol, Malic_Acid,\tAsh,\tAsh_Alcanity,\tMagnesium,\tTotal_Phenols,\tFlavanoids,\tNonflavanoid_Phenols,\tProanthocyanins,\tColor_Intensity,\tHue,\tOD280,\tProline]\n",
    "y = dataset.iloc[:, -1].values # [Customer_Segment]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1f23f0-cfb8-4335-93bb-a9d3bc9e1ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eFNmTBchD7Wm"
   },
   "source": [
    "Here we're going to reduce the wine dataset with PCA and then train a logistic regression model with the data reduced by PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef4c108-2219-4187-8d6d-2a6eeb2d9a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "BRobfvVWDGGb",
    "outputId": "ff614760-3112-4cc2-9aaa-7dbf049f456f"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into the training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Apply the PCA\n",
    "# The n_components parameter controls how many dimensions (principal components) to retain.\n",
    "# In this case, the data will be reduced to a 2-dimensional space, making it easier to visualize or train the model more efficiently.\n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model with the data reduced\n",
    "classifier = LogisticRegression(random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23bfd6e-d736-48b3-9990-2c484cf2aad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qE4HWXFvETCM"
   },
   "source": [
    "Predict the test set result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379eb2fb-a5dc-4106-b562-bbcf5b1712ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6URzyTGEDWf",
    "outputId": "c0a57307-e530-4c8e-b7de-813bcb257205"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Confusion matrix for this logistic regression:\")\n",
    "print(cm)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae8f0f1-be92-402d-8b00-9fa5b74eac3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_GLtKhgXE5m8"
   },
   "source": [
    "Visualise the training set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e032d6-c8cd-4195-a964-e711ba05ff2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "E4-FcD3fEbbw",
    "outputId": "89efd69d-1aa5-43a9-f41a-6bfdbe881b34"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = X_train, y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444fcd9d-4969-4a53-877e-445e61318b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zIbxHhXfFAf0"
   },
   "source": [
    "Visualise the test set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41a5f53-8cc3-4d1e-b259-2f3fa4dd49c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "qEIoOQi3E9t9",
    "outputId": "afb8a904-2aa0-4453-b471-f74b389a4614"
   },
   "outputs": [],
   "source": [
    "X_set, y_set = X_test, y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ece0f6f7-0f42-4344-a307-76881a1801ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RL5j-ywjFvfp"
   },
   "source": [
    "##  3.7 K-means\n",
    "In this section, we introduce K-Means Clustering, a popular and simple **unsupervised learning** algorithm. K-Means helps group unlabeled data into distinct clusters based on their similarities.\n",
    "\n",
    "K-Means works through an iterative process to create K clusters of data points.\n",
    "- **Decide the number of clusters (K)**: The first step is to choose how many clusters you want to create. The value of K (number of clusters) is something you can decide in advance.\n",
    "- **Place the centroids**: After deciding K, the algorithm places K centroids randomly on the scatter plot. A centroid is simply a point that represents the center of a cluster. These centroids can be placed anywhere and do not have to overlap with existing data points.\n",
    "- **Assign data points to the nearest centroid**: Next, the algorithm assigns each data point to the closest centroid. This step forms preliminary clusters. For example, an equidistant line can be drawn between the two centroids, and all data points on one side will belong to one cluster, while those on the other side will belong to the second cluster.\n",
    "- **Recalculate the centroid positions**: For each cluster, the algorithm calculates the center of mass (or center of gravity). This is done by averaging the X and Y coordinates of all the data points in a cluster. The centroids are then moved to these new calculated positions, which represent the center of their assigned clusters.\n",
    "- **Repeat the process**: Once the centroids have moved, the algorithm reassigns the data points to the nearest centroid based on the new positions. This process of recalculating the centroids and reassigning data points continues iteratively.\n",
    "- **Stop when centroids no longer move**: The process repeats until the centroids no longer change position, meaning the clusters have stabilized. At this point, the algorithm has successfully created final clusters, and each data point belongs to the cluster closest to its final centroid.\n",
    "\n",
    "**Why Use K-Means?**\n",
    "\n",
    "K-Means is simple yet highly effective at identifying clusters within data. After the algorithm completes, you can then analyze the resulting clusters to interpret them in a business context. For instance, different clusters of customers may represent distinct spending behaviors or income levels, allowing you to tailor strategies accordingly.\n",
    "\n",
    "This iterative approach makes K-Means a valuable tool for discovering patterns in unlabeled data, helping you understand the structure of your dataset without any prior labels or training.\n",
    "\n",
    "**The elbow method**\n",
    "\n",
    "The Elbow Method is a common technique for selecting the optimal number of clusters in K-Means Clustering.\n",
    "\n",
    "When applying K-Means, one of the first decisions you need to make is how many clusters (K) to create. While sometimes domain knowledge might give you an idea of how many clusters to expect, if you're unsure, the Elbow Method can help.\n",
    "\n",
    "The **Elbow Method** is a visual technique that helps you determine the optimal number of clusters by looking at the Within Cluster Sum of Squares (WCSS). **WCSS** measures the total squared distance between each point and its respective cluster centroid. The goal is to minimize this value as you increase the number of clusters.\n",
    "\n",
    "Here’s the step-by-step process:\n",
    "- **Run K-Means multiple times**: First, you need to run the K-Means algorithm for different numbers of clusters (e.g., 1, 2, 3, 4, etc.). For each K value, calculate the WCSS. The more clusters you have, the smaller the WCSS becomes because data points are grouped into more tightly fitting clusters.\n",
    "- **Plot the WCSS vs. Number of Clusters**: After calculating the WCSS for each possible number of clusters, you plot the WCSS on the y-axis and the number of clusters on the x-axis. As you increase the number of clusters, WCSS decreases, but at some point, the rate of decrease slows down.\n",
    "- **Identify the 'Elbow'**: The Elbow Method looks for the point where the WCSS stops decreasing rapidly and starts leveling off. This point forms a kink or \"elbow\" in the plot. The number of clusters corresponding to this elbow is considered the optimal number of clusters. It’s where adding more clusters doesn’t significantly improve the compactness of the data within clusters.\n",
    "\n",
    "Key Points to Remember:\n",
    "- **The Elbow Method is iterative**: You need to run K-Means several times with different K values before applying the method.\n",
    "- **It’s a visual tool**: The decision is based on visually identifying the elbow in the plot.\n",
    "- **Judgment call**: Sometimes the elbow may not be obvious, and it might be a judgment call as to which K is optimal.\n",
    "\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d5ae3c8-c4a8-49e8-b9bc-9b366f39eb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MaCRwNamFE0A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5034f1ed-692f-4c62-9bc4-20395406a2c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JvrPqUt2GEkl"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b7ff63-5a34-4f46-af62-d2fd641802a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3v79rmseGDkj",
    "outputId": "393f04c7-4c77-40e7-de26-255782d5e9bf"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Mall_Customers.csv')\n",
    "X = dataset.iloc[:, [3, 4]].values # [Annual Income (k$),\tSpending Score (1-100)]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d653ff18-223d-4957-9b58-3c6844c15730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3KJM327QGewH"
   },
   "source": [
    "Using the elbow method to find the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ab6786-6d2d-4e04-963e-cc5780b09982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "n5syr4t8GJ2r",
    "outputId": "37838632-8f6a-4c5a-b028-1146679c7d86"
   },
   "outputs": [],
   "source": [
    "# K-Means++ initialization for the elbow method:\n",
    "# Instead of randomly placing centroids, K-Means++ selects them strategically:\n",
    "# 1. The first centroid is chosen randomly from the data points.\n",
    "# 2. For each remaining centroid, the algorithm selects a point farthest from the existing centroids.\n",
    "#    This spreads out the centroids across the dataset.\n",
    "# 3. After initializing the centroids, K-Means proceeds with clustering as usual.\n",
    "# K-Means++ improves the clustering efficiency and helps get better results when using techniques\n",
    "# like the Elbow Method to find the optimal number of clusters.\n",
    "wcss = []\n",
    "for i in range(1, 11): # we'll do it for 10 clusters to find the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d6fb46-46a3-4f3e-a23a-2d36a6e91897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ayXJ07hLjPdn"
   },
   "source": [
    "The optimal number of clusters is **5**. We can see visualy here that 5 is where the descent of the wcss value starts to slowing down or reducing its descent.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28fef6e0-92e0-4b61-b31a-6917cdb01797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CJJDa2bUGd6P"
   },
   "source": [
    "Train the model with the right number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16a3e8e4-ac1b-4687-b3b9-f6026d4f47e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fo0-23VmGtrL"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabe0336-9d54-4dbf-b2f9-954fe4430a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "o6X3Hr3Im2PP"
   },
   "source": [
    "Let's take a look at the `y_kmeans` variable.\n",
    "\n",
    "The `y_kmeans` variable contains the classification in a cluster of the customer.\n",
    "\n",
    "For instance, the customer 1 is in cluster 4, the customer 2 is in cluster 2 and customer 3 is in cluster 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87e4244f-88b5-41aa-a465-9013b4740ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MebBFyIk0J-",
    "outputId": "6684d117-c8b4-41e1-85ed-9af2f4dbc5c7"
   },
   "outputs": [],
   "source": [
    "print(dataset.head())\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13715e53-8b74-4a82-9ca1-0e68653a84af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Y0AftnW6Gv2p"
   },
   "source": [
    "Visualize the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838c629a-f5c0-4429-8054-981115d0416f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bdD9ThPVGu1A",
    "outputId": "b0db2f74-0a4e-49ff-ae0e-449143df9580"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c8ca25-7fe4-417f-8481-967d73d2fec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AYoC6NmmmM7H"
   },
   "source": [
    "Interpret the Clusters:\n",
    "- Cluster 1: Low income, high spending.\n",
    "- Cluster 2: High income, high spending (potential top customers).\n",
    "- Cluster 3: Low income, low spending.\n",
    "- Cluster 4: High income, low spending (customers to target with better offers).\n",
    "- Cluster 5: Average income and average spending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706a3912-e679-47cb-b90f-74217cc49161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hNY-JrTFN8mq"
   },
   "source": [
    "## 3.8 Hierarchical clustering\n",
    "**Hierarchical Clustering** is another important technique in unsupervised learning. Hierarchical clustering is similar to K-Means but follows a different approach to grouping data points.\n",
    "\n",
    "There are two main types of hierarchical clustering:\n",
    "- Agglomerative (bottom-up): Starts with each data point as its own cluster and merges them step by step until only one cluster remains.\n",
    "- Divisive (top-down): Starts with all data points in one cluster and splits them into smaller clusters. In this course, we focus on the agglomerative approach.\n",
    "\n",
    "Agglomerative Hierarchical Clustering step by step:\n",
    "- Step 1: Start with each data point as a separate cluster.\n",
    "- Step 2: Find the two closest clusters and merge them into one. The result is fewer clusters.\n",
    "- Step 3: Repeat this process, merging the closest clusters, until only one cluster remains.\n",
    "\n",
    "The key here is how we define \"closeness\" between clusters, which can be measured in different ways:\n",
    "- Minimum distance (closest points between clusters).\n",
    "- Maximum distance (farthest points).\n",
    "- Average distance (average of all distances between points in the clusters).\n",
    "- Centroid distance (distance between cluster centroids).\n",
    "\n",
    "We'll use Euclidean distance, the straight-line distance between two points. It is calculated using basic geometry: the square root of the sum of squared differences between corresponding coordinates.\n",
    "\n",
    "The purpose of Hierarchical Clustering is to build a memory of how clusters were merged at each step. This memory is stored in a structure called a dendrogram, which visually represents the hierarchy of clusters. We'll discuss dendrograms as they help us understand the structure and relationships between clusters.\n",
    "\n",
    "While hierarchical clustering initially combines all data points into one large cluster, the true purpose is to break this large cluster down into meaningful smaller clusters. But how do we decide the right number of clusters? This is where dendrograms come in. Dendrograms help visualize the clustering process step-by-step, showing how data points or clusters are merged based on their proximity.\n",
    "\n",
    "Building a Dendrogram Step-by-Step:\n",
    "- **Start with Individual Clusters**: Each data point starts as its own cluster. In the dendrogram, this is represented by each point being at the bottom.\n",
    "- **Merge Closest Points**: The next step is to merge the two closest points (or clusters) based on their Euclidean distance. This is shown by connecting these points with a horizontal line on the dendrogram. The height of the line corresponds to the distance between the points, representing their dissimilarity.\n",
    "- **Repeat the Process**: Continue merging the next two closest clusters and repeat until all points are combined into one large cluster. The vertical axis of the dendrogram reflects the distances between merged points or clusters at each step.\n",
    "\n",
    "The **height** of the lines in the dendrogram indicates the **dissimilarity** between points or clusters. The further apart two points or clusters are, the higher the line connecting them will be. This helps to visualize how similar or different data points are.\n",
    "\n",
    "Once the dendrogram is built, we can cut it at a specific height to determine the optimal number of clusters. This cut reveals the most appropriate number of clusters by examining where the largest jumps in distance occur.\n",
    "\n",
    "Dendrograms are a powerful tool in hierarchical clustering, providing a visual summary of the merging process and helping determine how to segment data into meaningful clusters.\n",
    "\n",
    "**How to Use a Dendrogram?**\n",
    "To extract meaningful clusters from a dendrogram, follow these steps:\n",
    "- **Set a Threshold**: Choose a dissimilarity threshold by drawing a horizontal line across the dendrogram. This line represents the maximum allowable dissimilarity within clusters. Any clusters formed above this threshold will not be combined.\n",
    "- **Count** the Vertical Lines Crossed: The number of vertical lines the threshold crosses determines how many clusters you will have. Each line represents a cluster, and the points below that line belong to the same cluster.\n",
    "\n",
    "**Example**\n",
    "- Set a threshold at a certain height (e.g., 1.7). If the horizontal line crosses two vertical lines, you will have two clusters.\n",
    "- Lowering the threshold may create more clusters, while raising it will combine clusters into fewer groups.\n",
    "\n",
    "**How to find the optimal number of cluster?**\n",
    "\n",
    "One common approach is to find the longest vertical line in the dendrogram that does not cross any extended horizontal lines. This long vertical line represents the largest dissimilarity between clusters, and cutting the dendrogram at this height often provides the optimal number of clusters.\n",
    "- If the longest vertical line is crossed by a horizontal threshold, and it intersects three vertical lines, this indicates three clusters.\n",
    "\n",
    "By analyzing the dendrogram and using the largest vertical distance to set your threshold, you can effectively determine the optimal number of clusters for your data. This technique provides an intuitive and visual way to extract meaningful insights from hierarchical clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b724610-7a5c-49a6-b1a8-924a5ec21542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rNImK8ePOE4x"
   },
   "source": [
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a57222-9e24-4f64-8c48-ff19ff070711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HUdnV07JOGkl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e70b09c-23e4-4dac-8e0f-7ca9007d698b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "r0J8tYAqOJsC"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5908b5b-b863-4407-a44a-0badb8e1dee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eewYb2C3OThf",
    "outputId": "42ab05d4-b9fa-4ae7-b6d2-f0f7def76629"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Mall_Customers.csv')\n",
    "X = dataset.iloc[:, [3, 4]].values # [Annual Income (k$),\tSpending Score (1-100)]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25db225d-84ab-4625-bc9b-b2a113ee3cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xIPP90MPOngn"
   },
   "source": [
    "Using the dendrogram to find the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "089fc198-cc84-4fca-a6fb-83155e648068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "RiEKrraPOq4x",
    "outputId": "d5951c98-33e3-4b81-92f9-584594a4040b"
   },
   "outputs": [],
   "source": [
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39b62373-310b-45e5-af79-f84993e96e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TpoZYMHkuIzZ"
   },
   "source": [
    "We chose **5** clusters based on the largest vertical distance in the dendrogram, which indicated a clear separation at that point. Although both 3 and 5 clusters were possible, the vertical distance for 5 clusters was slightly larger, suggesting a more distinct grouping. This decision was further supported by prior results from the K-Means algorithm and the Elbow Method, both of which also pointed to 5 as the optimal number of clusters. By combining these insights and measuring the pixel distances between the horizontal bars in the dendrogram, we confidently selected 5 clusters for the analysis.\n",
    "\n",
    "Step-by-Step Process to find the optimal number of clusters:\n",
    "- **Start from the Top**: Begin by examining the horizontal bars in the dendrogram. These bars represent the merging of clusters. Move downwards to observe where clusters are combined, and note the vertical distances between the horizontal bars. These distances represent the dissimilarity between the merged clusters.\n",
    "- **Identify the Largest Vertical Move**: As you move down, focus on the largest vertical distance between two horizontal bars. This typically indicates a significant difference between clusters. The number of vertical bars (clusters) within this largest vertical move can guide you to the optimal number of clusters.\n",
    "  - For example, if the largest vertical move contains three vertical bars, then three clusters might be optimal.\n",
    "  - However, as you continue analyzing, if a larger vertical move appears later, it could suggest a different number of clusters.\n",
    "- **Compare Cluster Options**: In some cases, there might be multiple valid cluster numbers. For example, in this case, both three clusters and five clusters make sense. While the dendrogram suggests that three clusters is a good option, you may already know from other methods, like K-Means or the Elbow Method, that five clusters is optimal.\n",
    "- **Measure Precision**: If needed, you can even measure the pixel distances in the dendrogram to confirm which vertical distance is larger. This can help you decide between close options like three or five clusters.\n",
    "\n",
    "Sometimes, there can be more than one reasonable number of clusters. Using multiple methods like K-Means and hierarchical clustering together provides extra insights, allowing you to make more informed decisions. In this example, both three and five clusters are viable, but based on previous analysis, we settle on five clusters as the optimal choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73de42ff-56c2-4984-be63-86f1fb98f604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nU5OZ9TMrQMf"
   },
   "source": [
    "Train the model with the right number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6737da8-12e8-44cd-91b6-073d0a190db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AyirSAgdrTBI"
   },
   "outputs": [],
   "source": [
    "hc = AgglomerativeClustering(n_clusters = 5, linkage = 'ward')\n",
    "y_hc = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e93307-0920-4e3f-981d-2f2c1c2c3921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SKsq4kb1vfmR"
   },
   "source": [
    "Visualise the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df098d92-a585-4c2f-b317-94947e2da5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "GazLnDcgviUo",
    "outputId": "946c68f9-47f8-471b-e543-71694bfe4480"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3232f4-ce44-442f-9247-9ed9908b49a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0bI1_V9BOxci"
   },
   "source": [
    "## 3.9 XG-Boost\n",
    "**XGBoost** is a powerful machine learning model that can be used for both regression and classification tasks. XGBoost is known for its efficiency and performance, often yielding excellent results across various problems.\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc36d41-44d8-4b85-9965-c6fb321dbf70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FigNLkOz0aXC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbcbc894-69ae-4318-b1f1-4d473ad1bda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mESc_GY_0YnL"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a789390e-c97d-4240-a6c6-858afae155f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "c75P022f1B65",
    "outputId": "d147e52f-e4fe-416e-cac0-3b68961bdef2"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/XGBoostData.csv')\n",
    "X = dataset.iloc[:, :-1].values # [Sample, code, number,\tClump, Thickness,\tUniformity of Cell Size,\tUniformity of Cell Shape,\tMarginal Adhesion,\tSingle Epithelial Cell Size,\tBare Nuclei,\tBland Chromatin,\tNormal Nucleoli,\tMitoses]\n",
    "y = dataset.iloc[:, -1].values # [Class]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994375bc-546d-401d-abd9-13148d970c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRjT80AH1Q2n",
    "outputId": "03f48096-8a3f-4663-9259-a46486329651"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d891041b-933a-4ad2-9d8c-8f9ea35a6408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGYiA1wK1R4L",
    "outputId": "c089a3a8-f529-4b8d-8eb3-dd1a554f882b"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02d4134b-0949-4ae8-bf2e-b1a9c94bf11e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EJuQCH5L3vrF"
   },
   "source": [
    "Encode the dependant variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6dc287c-f6b0-441e-a42b-b63a7da38580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiUjSdDM3xqW",
    "outputId": "da32493d-7c86-437d-89ad-c167644ae96f"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3ba5b6e-fb63-4be8-aa34-1bc888b8e105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Mmg0AP5n1ifD"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef11181-ed9d-4ec6-8c13-5a7cffc280c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "jmWLAbCA1k6g",
    "outputId": "c2b7b9cf-5df4-4e51-869d-a4e85c5fc074"
   },
   "outputs": [],
   "source": [
    "# Split the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c220800-78bf-4b77-91e2-45244944f127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zcImuiHx37ZA"
   },
   "source": [
    "Making the confusion matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069ed2b4-0f76-447b-a53e-6c64963746f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ds6w-U6i4AV2",
    "outputId": "d00ccd5c-bf54-4365-80e9-af2a82109bef"
   },
   "outputs": [],
   "source": [
    "print(f\"Confusion matrix:\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9115b76-3aaf-45ce-b35e-1cc53a25ac42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0OORPnO24MfY"
   },
   "source": [
    "Dang soo good !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659b0aab-f47a-4177-b48c-a7362ecd16e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cs5-PYYCIMC3"
   },
   "source": [
    "## 3.10 Artificial Neural Network\n",
    "\n",
    "I won't cover this chapter in details. Sorry :(\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e4967a-5982-40b6-800b-3cc6415c884e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "E_9e3JFHGxwQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8243a6-7af6-4a90-aa7e-88b4c7a23731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tFnPEQb_IVaG"
   },
   "source": [
    "Check tensorflow version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19be914-7e4f-4d4f-9756-00163925c807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Hvy8MymUIT-0",
    "outputId": "b3243b3d-834a-4e85-8332-462b3d5d2fa1"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7cdbf3-d55c-497c-9c0e-095ee7de3f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Uvx-eX2vIb41"
   },
   "source": [
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e042a26-da7c-4bbd-be75-098fd2edab91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "RQJno19DIZI_",
    "outputId": "bf0a8c43-fa6a-4da1-d2bb-e1ee8a0be813"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://static.grosjean.io/cas/module3/Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values # [CreditScore,\tGeography,\tGender,\tAge,\tTenure,\tBalance,\tNumOfProducts,\tHasCrCard,\tIsActiveMember,\tEstimatedSalary]\n",
    "y = dataset.iloc[:, -1].values   # [Exited]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c397d106-2767-443d-8d53-e50a6eadaa2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxZuMES6LfhY",
    "outputId": "fa14fb9a-84a9-4043-8382-b3106957eb00"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3dce2c-88ab-460f-ac1c-22ceb814aeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpGZ9uAuLhR2",
    "outputId": "79fbe989-3fe7-42e0-b9a8-bb2d6ab183a6"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1390076-fee3-4bc8-b945-3c768d4a1078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WlE3iVcFLvJG"
   },
   "source": [
    "Label Encoding the \"Gender\" column:. Encoding categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "909246c7-4617-421e-a84a-5ddf2e057a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zUDrzoDcL1ix"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418fc2b4-15c5-4872-8512-e1acf68b7df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID3uGZUFMOPu",
    "outputId": "dfc386eb-b3b3-4f5e-fc68-d1fe53ac9b3a"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1720099-bebb-4a53-9331-4e4bb395c0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mWYuvTFzIhIG"
   },
   "source": [
    "One Hot Encoding the Geography column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d92f2be-ff3f-411c-a6ed-41f2a19627da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ojAoUuOoIgXt"
   },
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e3a031-6489-4d11-8751-6bd33e68094c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpdJIL5dMYCC",
    "outputId": "e94e62bf-72c4-45d4-a8ec-4371125c9e09"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2723a3d3-4f2c-4c81-a0d7-fe506a3dcc8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fw4u4J0IKTQY"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de160ce6-7cc3-40ac-b429-85f788eba9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJoYF2eUJ7as",
    "outputId": "8c88dad2-1095-46fb-e8d5-6f2ce6e10945"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Initialize the ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding the first input layer and the first hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Traning the ANN on the training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08d9a68e-54c2-47ed-92d1-499342b66acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wb-mhg9YM4sq"
   },
   "source": [
    "Predicting the result of a single observation.\n",
    "\n",
    "Use our ANN model to predict if the customer with the following informations will leave the bank:\n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: $ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card ? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: $ 50000\n",
    "\n",
    "So, should we say goodbye to that customer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f9c771-ab36-40b1-8a94-67c6978e46d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A9AnuceKX_W",
    "outputId": "ff384098-14e6-44ed-9a65-6aebf3399ab6"
   },
   "outputs": [],
   "source": [
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50884051-e50d-41de-a16d-e6c09a6d1114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dWBuLk5-NEFz"
   },
   "source": [
    "Therefore, our ANN model predicts that this customer stays in the bank!\n",
    "\n",
    "Important note 1: Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "Important note 2: Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82371fd3-904b-475d-a5bd-9c9c2086f269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fChRLOY7NGzO"
   },
   "source": [
    "Predicting the test set results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a8a995-9c25-48d4-b504-22907934043c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4COhrTL5NA9M",
    "outputId": "23d5ee80-388c-4afb-974c-ea0f0ed3a520"
   },
   "outputs": [],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c26345b-597e-4bf1-8a37-3ca03b9eb533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bYs4IVUINNK1"
   },
   "source": [
    "Making the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31a672c-0dbc-4194-b08c-788f78bd0113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3Wq3ByfNIo6",
    "outputId": "57f32187-0fb4-4fdf-c7b4-8d57ff8e7776"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fed127c-8722-4275-a9ef-4841cfec32e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kCGKmVQbNRvd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Data_Analysis_and_Machine_Learning",
   "widgets": {}
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
