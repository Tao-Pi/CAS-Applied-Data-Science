{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7905d5f0-176a-446a-9f1b-0cc5a516f490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSTALLATION\n",
    "# ============================================================================\n",
    "!pip install transformers accelerate torch bertopic sentence-transformers scikit-learn pandas pyarrow hf-transfer seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f682967e-5991-4240-9731-810af3af1483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from IPython.display import display\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM EMBEDDER\n",
    "# ============================================================================\n",
    "class CustomEmbedder:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encode(self, texts, **kwargs):\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ANALYSIS\n",
    "# ============================================================================\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "MODEL_NAME = \"nvidia/llama-embed-nemotron-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "print(f\"✓ Loaded {MODEL_NAME}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "df = pq.read_table(\"export_articles_translated.parquet\").to_pandas()\n",
    "docs = df[\"content_english\"].dropna().tolist()\n",
    "print(f\"✓ Loaded {len(docs)} documents\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURE UMAP AND HDBSCAN PARAMETERS\n",
    "# ============================================================================\n",
    "# UMAP parameters\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=20,        # Default: 15. Higher = more global structure. Range: 2-100\n",
    "    n_components=5,        # Default: 5. Number of dimensions to reduce to\n",
    "    min_dist=0.0,          # Default: 0.1. Minimum distance between points. Range: 0.0-0.99\n",
    "    metric='cosine',       # Distance metric: 'cosine', 'euclidean', etc.\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "\n",
    "# HDBSCAN parameters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=220,   # Default: 10. Minimum size of clusters. Increase for fewer, larger topics\n",
    "    min_samples=15,         # Default: None. Higher = more conservative clustering\n",
    "    metric='euclidean',    # Distance metric: 'euclidean', 'manhattan', etc.\n",
    "    cluster_selection_method='eom',  # 'eom' or 'leaf'\n",
    "    prediction_data=True   # Needed for transform\n",
    ")\n",
    "\n",
    "# Setup BERTopic\n",
    "print(\"\\nSetting up BERTopic...\")\n",
    "embedding_model = CustomEmbedder(model, tokenizer)\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "custom_stopwords = list(ENGLISH_STOP_WORDS) + ['said', 'efe']\n",
    "vectorizer_model = CountVectorizer(stop_words=custom_stopwords)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"✓ BERTopic configured\")\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\nRunning topic modeling...\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Add topics to dataframe\n",
    "df_clean = df.dropna(subset=[\"content_english\"]).copy()\n",
    "df_clean[\"topic\"] = topics\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTS: Found {len(set(topics))} topics\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Show topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "display(topic_info)\n",
    "\n",
    "# Show sample of articles with topics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAMPLE ARTICLES WITH TOPICS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "display(df_clean[[\"content_english\", \"topic\"]].head(10))\n",
    "\n",
    "# Optional: Save results (uncomment if needed)\n",
    "# df_clean.to_parquet(\"articles_with_topics.parquet\", index=False)\n",
    "# topic_model.save(\"bertopic_model\")\n",
    "# print(\"\\n✓ Saved to: articles_with_topics.parquet and bertopic_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcfb0b96-eb9c-4e90-87f8-48bfe4af0ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS TO CSV\n",
    "# ============================================================================\n",
    "# Add topic labels to the dataframe\n",
    "print(\"\\nPreparing data for export...\")\n",
    "\n",
    "# Define your custom topic labels\n",
    "custom_topic_labels = {\n",
    "    -1: \"Outliers / Unassigned\",\n",
    "    0: \"Latin American Politics\",\n",
    "    1: \"Swiss Domestic Affairs\",\n",
    "    2: \"Africa & Middle East\",\n",
    "    3: \"International Sports News\",\n",
    "    4: \"Law, Crime, Public Safety\",\n",
    "    5: \"Arts & Culture\",\n",
    "    6: \"Business & Economics\",\n",
    "    7: \"International Security & Military Affairs\",\n",
    "    8: \"International Trade & Geopolitics\",\n",
    "    9: \"Natural Disaster & Humanitarian Response\",\n",
    "    10: \"US Domestic Affairs\",\n",
    "    11: \"Climate Action & Policy\",\n",
    "    12: \"Business & Economics in Latin America\"\n",
    "}\n",
    "\n",
    "# Map topics to custom labels\n",
    "df_clean['topic_label'] = df_clean['topic'].map(custom_topic_labels)\n",
    "\n",
    "# Save all articles with topics\n",
    "output_file = \"articles_with_topics.csv\"\n",
    "df_clean.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"✓ Saved all articles to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8094238d-f875-496f-bf8f-9aff5a0fb95f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate topic percentages\n",
    "topic_counts = df_clean['topic_label'].value_counts()\n",
    "topic_percentages = (topic_counts / len(df_clean) * 100).sort_values(ascending=True)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Use a nice color palette (viridis, but you can try: 'plasma', 'cividis', 'coolwarm')\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(topic_percentages)))\n",
    "\n",
    "bars = ax.barh(topic_percentages.index, topic_percentages.values, color=colors)\n",
    "\n",
    "# Add bar labels\n",
    "for i, (bar, value) in enumerate(zip(bars, topic_percentages.values)):\n",
    "    ax.text(value + 0.3, i, f'{value:.1f}%', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Minimal styling\n",
    "ax.set_xlabel('Percentage of Corpus (%)', fontsize=11)\n",
    "ax.set_ylabel('')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('topic_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"✓ Saved chart to: topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86bd97a5-229b-4b32-8a5b-6a4ea68de656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TOPIC COUNTS OVER TIME - SMALL MULTIPLES (COMPACT)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"articles_with_topics.csv\")\n",
    "print(f\"✓ Loaded {len(df)} articles\")\n",
    "\n",
    "# Parse dates\n",
    "df['releaseDate'] = pd.to_datetime(df['releaseDate'])\n",
    "df['date'] = df['releaseDate'].dt.date\n",
    "print(f\"Date range: {df['releaseDate'].min()} to {df['releaseDate'].max()}\")\n",
    "\n",
    "# Set date limits for x-axis\n",
    "date_min = datetime(2025, 10, 27)\n",
    "date_max = datetime(2025, 11, 5)\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE DATA\n",
    "# ============================================================================\n",
    "# Calculate total counts per topic\n",
    "topic_totals = df.groupby('topic_label').size().reset_index(name='total')\n",
    "topic_totals = topic_totals.sort_values('total', ascending=False)\n",
    "\n",
    "# Separate outliers from other topics\n",
    "outliers_mask = topic_totals['topic_label'] == 'Outliers / Unassigned'\n",
    "outliers_topic = topic_totals[outliers_mask]['topic_label'].tolist()\n",
    "other_topics = topic_totals[~outliers_mask]['topic_label'].tolist()\n",
    "\n",
    "# Combine: sorted topics + outliers at the end\n",
    "topics = other_topics + outliers_topic\n",
    "\n",
    "daily_counts = df.groupby(['topic_label', 'date']).size().reset_index(name='count')\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE SMALL MULTIPLES (COMPACT)\n",
    "# ============================================================================\n",
    "print(\"\\nCreating compact small multiples chart...\")\n",
    "n_topics = len(topics)\n",
    "n_cols = 4  # More columns = more compact\n",
    "n_rows = (n_topics + n_cols - 1) // n_cols\n",
    "\n",
    "# Reduced figure size\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 2.5*n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else axes\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    topic_data = daily_counts[daily_counts['topic_label'] == topic].copy()\n",
    "    topic_data = topic_data.sort_values('date')\n",
    "    topic_data['date'] = pd.to_datetime(topic_data['date'])\n",
    "    \n",
    "    # Calculate total\n",
    "    total = topic_data['count'].sum()\n",
    "    \n",
    "    # Plot with label for legend\n",
    "    ax.plot(topic_data['date'], topic_data['count'], \n",
    "            color='steelblue', linewidth=1.5, marker='o', markersize=3, \n",
    "            markerfacecolor='steelblue', markeredgecolor='white', markeredgewidth=0.5,\n",
    "            label=f'Total: {total:,}')\n",
    "    \n",
    "    # Smaller title with less padding\n",
    "    ax.set_title(topic, fontsize=9, fontweight='bold', pad=5)\n",
    "    \n",
    "    if topic == 'Outliers / Unassigned':\n",
    "        ax.set_ylim(0, 385)\n",
    "    else:\n",
    "        ax.set_ylim(0, 190)\n",
    "    \n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax.set_xlim(date_min, date_max)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))  # Shorter date format\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), fontsize=7)\n",
    "    \n",
    "    # Add legend (text only, no marker or line)\n",
    "    ax.legend(loc='upper right', fontsize=7, framealpha=0.9, handlelength=0, handletextpad=0, markerscale=0)\n",
    "\n",
    "for idx in range(n_topics, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout(h_pad=1.5, w_pad=1.5)\n",
    "plt.savefig('topics_over_time_small_multiples.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: topics_over_time_small_multiples.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f04fb55-34a4-4927-a3a0-3d3a358c2645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "BERTopic Analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
